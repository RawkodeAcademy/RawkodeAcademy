WEBVTT

NOTE
Transcription provided by Deepgram
Request Id: 0b6ad27a-4ee7-41db-848d-aeb6807af854
Created: 2025-04-29T13:42:07.792Z
Duration: 3266.7058
Channels: 1

00:02:29.459 --> 00:02:32.500
<v Speaker 0>Hello, and welcome back to the Rawkode Academy.

00:02:32.659 --> 00:02:34.500
<v Speaker 0>Today is an episode of Rawkode Live where

00:02:34.500 --> 00:02:35.939
<v Speaker 0>we are taking a look at a new

00:02:35.939 --> 00:02:38.659
<v Speaker 0>open source tool from the team at Robusta

00:02:38.855 --> 00:02:42.775
<v Speaker 0>to help you improve your Kubernetes resource optimization.

00:02:43.334 --> 00:02:45.655
<v Speaker 0>To guide us through our demo today, I

00:02:45.655 --> 00:02:47.095
<v Speaker 0>am joined by Nathan.

00:02:47.334 --> 00:02:48.695
<v Speaker 0>Hello. How are you?

00:02:49.095 --> 00:02:51.254
<v Speaker 1>Good. It's a pleasure to be here. Yeah.

00:02:52.150 --> 00:02:54.470
<v Speaker 0>What is this? Stream number is three together,

00:02:54.470 --> 00:02:57.189
<v Speaker 0>I think. So, you know, five months ago,

00:02:57.189 --> 00:02:58.310
<v Speaker 0>we looked at

00:02:59.590 --> 00:03:01.670
<v Speaker 0>oh, what was it? Kubernetes monitoring.

00:03:02.069 --> 00:03:03.750
<v Speaker 0>And a year ago, took a look at

00:03:03.750 --> 00:03:05.750
<v Speaker 0>robusta. So pleasure to have you back.

00:03:06.765 --> 00:03:08.685
<v Speaker 1>Pleasure is mine. Yeah. I

00:03:09.325 --> 00:03:11.245
<v Speaker 1>it a lot has happened since then both

00:03:11.245 --> 00:03:13.004
<v Speaker 1>in terms of Robusta and also my personal

00:03:13.004 --> 00:03:14.205
<v Speaker 1>life. I

00:03:14.525 --> 00:03:16.364
<v Speaker 1>just have a newborn baby at home now,

00:03:16.364 --> 00:03:17.965
<v Speaker 1>so it just feels like it's been so

00:03:17.965 --> 00:03:18.525
<v Speaker 1>much longer.

00:03:20.080 --> 00:03:21.440
<v Speaker 0>Well, congratulations.

00:03:22.000 --> 00:03:23.760
<v Speaker 0>It's a wonderful thing.

00:03:23.920 --> 00:03:26.000
<v Speaker 0>A hard thing, but a wonderful thing.

00:03:26.880 --> 00:03:30.320
<v Speaker 0>Yes. Thank you. For anyone who is you

00:03:30.320 --> 00:03:31.920
<v Speaker 0>know, hasn't seen any of our previous episodes

00:03:31.920 --> 00:03:34.295
<v Speaker 0>as in familiar review and your work, which,

00:03:34.295 --> 00:03:35.575
<v Speaker 0>you know, shame on them. You're doing some

00:03:35.575 --> 00:03:37.255
<v Speaker 0>great stuff right now in the Kubernetes space,

00:03:37.255 --> 00:03:39.175
<v Speaker 0>and people should definitely be checking out your

00:03:39.175 --> 00:03:40.695
<v Speaker 0>blog and stuff. But do you wanna give

00:03:40.695 --> 00:03:42.695
<v Speaker 0>people the TLDR and share whatever you'd like

00:03:42.695 --> 00:03:43.255
<v Speaker 0>to share?

00:03:43.975 --> 00:03:46.615
<v Speaker 1>Yeah. So what we do at Robust or

00:03:46.615 --> 00:03:48.135
<v Speaker 1>rather, I'll start with the problem. The problem

00:03:48.135 --> 00:03:48.695
<v Speaker 1>is that

00:03:49.680 --> 00:03:51.680
<v Speaker 1>there's two problems. The first problem

00:03:52.000 --> 00:03:54.800
<v Speaker 1>is that people have all this observability data.

00:03:55.120 --> 00:03:56.240
<v Speaker 1>Mean, sometimes

00:03:56.560 --> 00:03:59.040
<v Speaker 1>terabytes of observability data that's sitting there. But

00:03:59.040 --> 00:04:00.959
<v Speaker 1>when there's an incident and when something's going

00:04:00.959 --> 00:04:03.200
<v Speaker 1>on in production, then finding the right thing

00:04:03.200 --> 00:04:04.799
<v Speaker 1>to look at finding the right thing to

00:04:04.799 --> 00:04:07.335
<v Speaker 1>look at fast is often very difficult.

00:04:07.975 --> 00:04:09.575
<v Speaker 1>So what we do at robust is to

00:04:09.575 --> 00:04:12.855
<v Speaker 1>try and help you take your observe observability

00:04:12.855 --> 00:04:15.655
<v Speaker 1>there and your existing observability data in Prometheus

00:04:15.655 --> 00:04:16.695
<v Speaker 1>and other places

00:04:16.855 --> 00:04:19.095
<v Speaker 1>and to extract from that meaningful,

00:04:19.495 --> 00:04:20.695
<v Speaker 1>and actionable insights.

00:04:21.370 --> 00:04:22.010
<v Speaker 1>And

00:04:22.810 --> 00:04:25.289
<v Speaker 1>there's several ways that, like, that comes about.

00:04:25.289 --> 00:04:27.050
<v Speaker 1>So we have an open source project,

00:04:27.610 --> 00:04:28.569
<v Speaker 1>that

00:04:28.569 --> 00:04:31.690
<v Speaker 1>takes your Prometheus alerts and then, adds on

00:04:31.690 --> 00:04:33.210
<v Speaker 1>extra data and ties it to logs and

00:04:33.210 --> 00:04:35.705
<v Speaker 1>so on. Then there's KRR, which we're gonna

00:04:35.705 --> 00:04:38.345
<v Speaker 1>look at today, which is about taking your

00:04:38.345 --> 00:04:41.145
<v Speaker 1>existing Prometheus data and using that to now

00:04:41.145 --> 00:04:43.785
<v Speaker 1>extract insights about efficiency and about CPU request

00:04:43.785 --> 00:04:45.385
<v Speaker 1>to memory. And, of course, we have a

00:04:45.385 --> 00:04:46.425
<v Speaker 1>commercial platform as well.

00:04:49.370 --> 00:04:50.250
<v Speaker 0>Awesome.

00:04:50.490 --> 00:04:53.450
<v Speaker 0>Well, I think we have Pavan and Elbe

00:04:53.450 --> 00:04:56.410
<v Speaker 0>at the chat. Elbe says congrats, and Pavan

00:04:56.410 --> 00:04:58.250
<v Speaker 0>says hello, who I believe is on the

00:04:58.250 --> 00:04:59.290
<v Speaker 0>robust dot team. Right?

00:05:01.015 --> 00:05:01.575
<v Speaker 0>Awesome.

00:05:02.695 --> 00:05:04.535
<v Speaker 0>So today, as you said, we're taking a

00:05:04.535 --> 00:05:07.015
<v Speaker 0>look at KRR, a tool for Kubernetes Resource

00:05:07.015 --> 00:05:07.895
<v Speaker 0>Optimization,

00:05:07.895 --> 00:05:10.775
<v Speaker 0>trying to help people with scaling workloads within

00:05:10.775 --> 00:05:13.015
<v Speaker 0>our cluster. But maybe before we dive into,

00:05:13.015 --> 00:05:15.040
<v Speaker 0>like, the demo and talk about what KRR

00:05:15.040 --> 00:05:17.199
<v Speaker 0>does, we can kind of understand what the

00:05:17.199 --> 00:05:20.080
<v Speaker 0>challenges are right now with workloads with Kubernetes

00:05:20.080 --> 00:05:23.199
<v Speaker 0>and, you know, resource requests and vertical pod

00:05:23.199 --> 00:05:25.360
<v Speaker 0>auto scaling. There's a whole bunch of stuff

00:05:25.919 --> 00:05:28.000
<v Speaker 0>that isn't really there by default. Right? It's

00:05:28.000 --> 00:05:29.520
<v Speaker 0>not like you just create your deployment, you

00:05:29.520 --> 00:05:31.445
<v Speaker 0>get your pods and and things magically work.

00:05:31.445 --> 00:05:32.405
<v Speaker 0>I mean, they could,

00:05:32.805 --> 00:05:34.725
<v Speaker 0>but as your cluster grows and more workloads

00:05:34.725 --> 00:05:36.565
<v Speaker 0>come on and the older your cluster is

00:05:36.565 --> 00:05:38.325
<v Speaker 0>and the more teams you onboard,

00:05:38.645 --> 00:05:41.285
<v Speaker 0>there's a plethora of challenges that are inevitably

00:05:41.285 --> 00:05:43.365
<v Speaker 0>gonna come your way. So, you know, what's

00:05:43.365 --> 00:05:46.910
<v Speaker 0>your experience of working with Kubernetes, onboarding new

00:05:46.910 --> 00:05:48.990
<v Speaker 0>workloads, and where do things start to click

00:05:48.990 --> 00:05:49.950
<v Speaker 0>and fall apart?

00:05:51.550 --> 00:05:53.710
<v Speaker 1>So there are a few stories here. The

00:05:53.710 --> 00:05:54.670
<v Speaker 1>first story is that

00:05:55.710 --> 00:05:56.270
<v Speaker 1>we

00:05:56.590 --> 00:05:57.470
<v Speaker 1>we had this.

00:05:58.195 --> 00:05:59.715
<v Speaker 1>You'd expect this as we move to the

00:05:59.715 --> 00:06:00.995
<v Speaker 1>cloud. Right? And we adapt all this great

00:06:00.995 --> 00:06:02.835
<v Speaker 1>technology, and we adapt Kubernetes, and we adapt

00:06:02.835 --> 00:06:03.715
<v Speaker 1>all these things.

00:06:04.515 --> 00:06:06.355
<v Speaker 1>Then you'd actually see that we're as, like,

00:06:06.355 --> 00:06:08.195
<v Speaker 1>an industry, we're becoming more efficient,

00:06:08.435 --> 00:06:09.955
<v Speaker 1>and we're becoming more reliable.

00:06:10.540 --> 00:06:12.380
<v Speaker 1>But in fact, if you look, like, at

00:06:12.380 --> 00:06:14.140
<v Speaker 1>the past year, then we've only seen the

00:06:14.140 --> 00:06:17.340
<v Speaker 1>opposite happen with incident after incident after incident.

00:06:17.340 --> 00:06:18.860
<v Speaker 1>Just last week, Instagram

00:06:19.180 --> 00:06:20.860
<v Speaker 1>just this week, I think Instagram was down

00:06:20.860 --> 00:06:21.980
<v Speaker 1>for a couple of hours.

00:06:22.300 --> 00:06:25.005
<v Speaker 1>And, like, what you see is that we're

00:06:25.005 --> 00:06:27.165
<v Speaker 1>not like, we'd expect that we're adopting all

00:06:27.165 --> 00:06:28.845
<v Speaker 1>this great technology. You're going to Kubernetes. You're

00:06:28.845 --> 00:06:30.525
<v Speaker 1>getting all these great things, and then your

00:06:30.525 --> 00:06:32.445
<v Speaker 1>system is becoming better. But instead,

00:06:32.685 --> 00:06:34.765
<v Speaker 1>there's all this added complexity.

00:06:35.245 --> 00:06:36.445
<v Speaker 1>And I think a lot of that complexity

00:06:36.445 --> 00:06:38.110
<v Speaker 1>is just ifying. It's good. You're getting these

00:06:38.110 --> 00:06:39.870
<v Speaker 1>benefits. You're getting, like, all those scaling. You're

00:06:39.870 --> 00:06:41.630
<v Speaker 1>getting all this stuff. But to set it

00:06:41.630 --> 00:06:43.470
<v Speaker 1>up and to configure it can sometimes be

00:06:43.470 --> 00:06:44.990
<v Speaker 1>a little bit bit challenging.

00:06:45.710 --> 00:06:47.150
<v Speaker 1>And then if you, like, double click on

00:06:47.150 --> 00:06:49.310
<v Speaker 1>that and you zoom really in, like, specifically

00:06:49.310 --> 00:06:51.275
<v Speaker 1>on Kubernetes and scaling, then there are two

00:06:51.275 --> 00:06:52.635
<v Speaker 1>things that you're always trying to balance. So

00:06:52.635 --> 00:06:54.475
<v Speaker 1>on the one hand, you wanna have something

00:06:54.555 --> 00:06:56.555
<v Speaker 1>that's really efficient, so you don't wanna allocate

00:06:56.555 --> 00:06:58.875
<v Speaker 1>all these resources that you don't need. On

00:06:58.875 --> 00:07:01.355
<v Speaker 1>the other hand, though, you wanna guarantee reliability,

00:07:01.514 --> 00:07:03.570
<v Speaker 1>and those always come hand in hand. And

00:07:03.570 --> 00:07:05.650
<v Speaker 1>there's, like, a trade off between them. And

00:07:05.650 --> 00:07:07.169
<v Speaker 1>what I mean by that is let's say

00:07:07.169 --> 00:07:08.530
<v Speaker 1>you have one application

00:07:08.610 --> 00:07:09.490
<v Speaker 1>that requires,

00:07:09.730 --> 00:07:11.810
<v Speaker 1>like, 10 c that requires one CPU, and

00:07:11.810 --> 00:07:14.690
<v Speaker 1>occasionally, it requires two CPU. Right? Then if

00:07:14.690 --> 00:07:16.210
<v Speaker 1>you give it two CPU,

00:07:16.290 --> 00:07:18.585
<v Speaker 1>then you're gonna satisfy the reliability. Right? And

00:07:18.585 --> 00:07:20.745
<v Speaker 1>maybe sometimes it speaks up it's can spike

00:07:20.745 --> 00:07:22.585
<v Speaker 1>to three CPU. So give it 10 CPU.

00:07:22.585 --> 00:07:24.025
<v Speaker 1>Give it a hundred CPU. Right? You're never

00:07:24.025 --> 00:07:25.705
<v Speaker 1>gonna have a reliability issue,

00:07:25.945 --> 00:07:27.465
<v Speaker 1>but you're gonna have a cost and efficiency

00:07:27.465 --> 00:07:29.305
<v Speaker 1>issue. On the other hand, if you take

00:07:29.305 --> 00:07:30.345
<v Speaker 1>that same workload

00:07:30.590 --> 00:07:32.510
<v Speaker 1>that requires one CPU most of the time,

00:07:32.510 --> 00:07:34.270
<v Speaker 1>you just give it one CPU.

00:07:34.430 --> 00:07:36.190
<v Speaker 1>Now you're gonna run into reliability issues when

00:07:36.190 --> 00:07:38.670
<v Speaker 1>it needs two. And all of the different

00:07:38.670 --> 00:07:41.390
<v Speaker 1>technologies involved in, like, an HPA and the

00:07:41.390 --> 00:07:43.870
<v Speaker 1>VPA and scaling and cluster other skin and

00:07:43.870 --> 00:07:45.630
<v Speaker 1>all that is about trying to

00:07:46.145 --> 00:07:48.625
<v Speaker 1>balance those constraints, and you balance it each

00:07:48.625 --> 00:07:50.305
<v Speaker 1>time in different ways. But there are always,

00:07:50.305 --> 00:07:52.705
<v Speaker 1>always trade offs, and I think that's important

00:07:52.705 --> 00:07:55.105
<v Speaker 1>to understand that that you will never have

00:07:55.105 --> 00:07:56.625
<v Speaker 1>a % efficiency

00:07:56.705 --> 00:07:58.865
<v Speaker 1>and a % reliability

00:07:58.865 --> 00:08:01.750
<v Speaker 1>because by definition, there's a conflict between the

00:08:01.750 --> 00:08:03.830
<v Speaker 1>two of those. Because with reliability,

00:08:03.830 --> 00:08:05.990
<v Speaker 1>you're looking at the 1% edge case. And

00:08:05.990 --> 00:08:08.870
<v Speaker 1>with efficiency, you're looking at the 99 case.

00:08:09.990 --> 00:08:12.070
<v Speaker 0>Yeah. I mean, I guess to try and

00:08:12.070 --> 00:08:14.230
<v Speaker 0>summarize that in, like, a single line. Right?

00:08:14.230 --> 00:08:16.635
<v Speaker 0>To understand that trade off is that you

00:08:16.635 --> 00:08:18.955
<v Speaker 0>you could get really good resiliency and redundancy

00:08:18.955 --> 00:08:20.795
<v Speaker 0>and all of that stuff on the happy

00:08:20.795 --> 00:08:23.115
<v Speaker 0>path, but it's the expensive path. Like, you

00:08:23.115 --> 00:08:25.835
<v Speaker 0>over provision 300, four hundred, six hundred percent,

00:08:25.835 --> 00:08:27.915
<v Speaker 0>whatever you want. If you've got the money

00:08:28.075 --> 00:08:29.355
<v Speaker 0>and you wanna throw it away or set

00:08:29.355 --> 00:08:30.715
<v Speaker 0>it on fire, sure. Go for it. Like,

00:08:30.715 --> 00:08:32.799
<v Speaker 0>yeah. You're you're gonna have a better time.

00:08:32.799 --> 00:08:34.559
<v Speaker 0>Maybe not an easy time, but a better

00:08:34.559 --> 00:08:37.040
<v Speaker 0>time than most. However, most organizations

00:08:37.120 --> 00:08:39.360
<v Speaker 0>can't just over provision to that kind of

00:08:39.360 --> 00:08:41.520
<v Speaker 0>level. They wanna reduce costs when they migrate

00:08:41.520 --> 00:08:43.520
<v Speaker 0>to the cloud. In fact, cost is usually

00:08:43.520 --> 00:08:43.840
<v Speaker 0>a

00:08:44.324 --> 00:08:46.084
<v Speaker 0>portion of why they're doing this. You know,

00:08:46.084 --> 00:08:48.964
<v Speaker 0>they're making the swap for operational

00:08:48.964 --> 00:08:52.084
<v Speaker 0>expenditure versus capital expenditure or whatever that term

00:08:52.084 --> 00:08:55.045
<v Speaker 0>is. I'm not a business person. So then

00:08:55.045 --> 00:08:56.805
<v Speaker 0>what they wanna do is, well, they set

00:08:56.805 --> 00:08:58.680
<v Speaker 0>all these resource requests so that they say,

00:08:58.680 --> 00:09:00.600
<v Speaker 0>okay. I expect these workloads to do this.

00:09:00.600 --> 00:09:02.920
<v Speaker 0>We have a buffer of 20%, thirty %,

00:09:02.920 --> 00:09:04.520
<v Speaker 0>whatever that is for your team. And then

00:09:04.520 --> 00:09:06.200
<v Speaker 0>you try to scale within that and scale

00:09:06.200 --> 00:09:08.120
<v Speaker 0>your cluster and your pods at the same

00:09:08.120 --> 00:09:10.360
<v Speaker 0>time. But that's the tricky path. That's the

00:09:10.360 --> 00:09:11.960
<v Speaker 0>hard path because it's

00:09:12.584 --> 00:09:15.625
<v Speaker 0>your workloads constantly move. They constantly change. They

00:09:15.625 --> 00:09:17.945
<v Speaker 0>constantly nodes or ephemerals are not always gonna

00:09:17.945 --> 00:09:19.865
<v Speaker 0>be there. All of these things really start

00:09:19.865 --> 00:09:22.265
<v Speaker 0>to compound and provide a pretty strong challenge

00:09:22.265 --> 00:09:24.505
<v Speaker 0>for your platform or SRE team to keep

00:09:24.505 --> 00:09:27.310
<v Speaker 0>the cluster healthy, the workload's healthy, and your

00:09:27.310 --> 00:09:28.350
<v Speaker 0>customers happy.

00:09:28.990 --> 00:09:29.630
<v Speaker 1>Yes.

00:09:29.950 --> 00:09:31.550
<v Speaker 1>So you can choose two out of three.

00:09:31.550 --> 00:09:32.910
<v Speaker 1>Right? It's always choose two out three. So

00:09:32.910 --> 00:09:34.030
<v Speaker 1>you have reliability,

00:09:34.190 --> 00:09:35.070
<v Speaker 1>efficiency,

00:09:35.310 --> 00:09:36.270
<v Speaker 1>and simplicity.

00:09:36.270 --> 00:09:37.470
<v Speaker 1>And I say you can choose two out

00:09:37.470 --> 00:09:39.390
<v Speaker 1>of three because you can get close with

00:09:39.390 --> 00:09:39.870
<v Speaker 1>efficiency

00:09:40.265 --> 00:09:42.265
<v Speaker 1>and with reliability and, like, kind of trying

00:09:42.265 --> 00:09:43.465
<v Speaker 1>to have both of them, but then you

00:09:43.465 --> 00:09:45.385
<v Speaker 1>go up in terms of complexity. Like, if

00:09:45.385 --> 00:09:46.585
<v Speaker 1>you think of a think of your cluster

00:09:46.585 --> 00:09:48.585
<v Speaker 1>as a pizza party, then either and you're

00:09:48.585 --> 00:09:49.625
<v Speaker 1>throwing a party and you don't know how

00:09:49.625 --> 00:09:51.625
<v Speaker 1>many people are gonna show up. So either

00:09:51.625 --> 00:09:53.305
<v Speaker 1>you order more pizzas and you end up

00:09:53.305 --> 00:09:55.339
<v Speaker 1>throwing some of them away, that's the trade

00:09:55.339 --> 00:09:56.459
<v Speaker 1>off with efficiency,

00:09:56.779 --> 00:09:57.899
<v Speaker 1>or you order,

00:09:58.380 --> 00:10:00.060
<v Speaker 1>like, the lower bound of pizzas, and then

00:10:00.060 --> 00:10:02.140
<v Speaker 1>people come there hungry, so that's your liability.

00:10:02.380 --> 00:10:04.139
<v Speaker 1>Or you do something complex where you're, like,

00:10:04.139 --> 00:10:05.420
<v Speaker 1>you're counting how many people come in the

00:10:05.420 --> 00:10:06.699
<v Speaker 1>door and you're looking at pizzas, then you

00:10:06.699 --> 00:10:08.540
<v Speaker 1>have delivery guy coming back and forth every

00:10:08.540 --> 00:10:09.660
<v Speaker 1>hour, and that's the trade off for the

00:10:09.660 --> 00:10:10.139
<v Speaker 1>simplicity.

00:10:10.915 --> 00:10:12.915
<v Speaker 1>But let's take that now into Kubernetes, and,

00:10:12.915 --> 00:10:14.595
<v Speaker 1>like, let's zoom in on Kubernetes.

00:10:14.675 --> 00:10:16.995
<v Speaker 1>So I wanna take an example.

00:10:17.075 --> 00:10:19.555
<v Speaker 1>Actually, I wanna take some statistics from a

00:10:19.555 --> 00:10:21.555
<v Speaker 1>recent cystic study, and they looked at all

00:10:21.555 --> 00:10:23.635
<v Speaker 1>the people running Kubernetes in production

00:10:23.955 --> 00:10:26.470
<v Speaker 1>that they have access to based on data

00:10:26.470 --> 00:10:28.230
<v Speaker 1>center system. They looked at all these giant

00:10:28.230 --> 00:10:30.630
<v Speaker 1>environments and lots of clusters, and they categorized

00:10:30.630 --> 00:10:33.350
<v Speaker 1>it by, like, big business and small business

00:10:33.350 --> 00:10:34.630
<v Speaker 1>and enterprise.

00:10:34.710 --> 00:10:36.150
<v Speaker 1>And if they're all these companies, what they

00:10:36.150 --> 00:10:38.230
<v Speaker 1>found is that 69%

00:10:38.230 --> 00:10:40.150
<v Speaker 1>of all the CPU that people are paying

00:10:40.150 --> 00:10:41.964
<v Speaker 1>for is actually wasted,

00:10:43.084 --> 00:10:46.045
<v Speaker 1>and that's an incredible number. And then to

00:10:46.045 --> 00:10:48.045
<v Speaker 1>translate this, though, people don't see that because,

00:10:48.045 --> 00:10:50.204
<v Speaker 1>like, you open up MENS or you look

00:10:50.204 --> 00:10:50.925
<v Speaker 1>at your cluster,

00:10:52.860 --> 00:10:54.540
<v Speaker 1>and you don't see you're not you're not

00:10:54.540 --> 00:10:55.740
<v Speaker 1>seeing anywhere 60

00:10:55.980 --> 00:10:58.540
<v Speaker 1>69% waste. What you see instead is you

00:10:58.540 --> 00:10:59.820
<v Speaker 1>go and you look at a node, and

00:10:59.820 --> 00:11:01.500
<v Speaker 1>you see, like, your node is full.

00:11:01.740 --> 00:11:02.220
<v Speaker 1>And

00:11:03.180 --> 00:11:04.700
<v Speaker 1>there's no room on your node, maybe you

00:11:04.700 --> 00:11:06.505
<v Speaker 1>have pending pods. But then you, like, look

00:11:06.505 --> 00:11:08.105
<v Speaker 1>into the node, you see the CPU is

00:11:08.105 --> 00:11:10.265
<v Speaker 1>at 10%, and you have low utilization.

00:11:10.824 --> 00:11:12.745
<v Speaker 1>So to understand all that, I think you

00:11:12.745 --> 00:11:14.024
<v Speaker 1>need to start at the basics,

00:11:14.345 --> 00:11:17.065
<v Speaker 1>which is setting just two numbers on Kubernetes.

00:11:17.065 --> 00:11:19.385
<v Speaker 1>Everything comes down to ultimately setting

00:11:19.385 --> 00:11:22.090
<v Speaker 1>two numbers, arguably four numbers. But we'll we'll

00:11:22.090 --> 00:11:23.930
<v Speaker 1>start with the two basics. Right? And that's

00:11:23.930 --> 00:11:26.330
<v Speaker 1>your CPU request and your memory request. And

00:11:26.330 --> 00:11:27.770
<v Speaker 1>the way to think about this for people

00:11:27.770 --> 00:11:30.010
<v Speaker 1>who aren't familiar is you're telling Kubernetes in

00:11:30.010 --> 00:11:32.945
<v Speaker 1>advance how much CPU will you need for

00:11:32.945 --> 00:11:34.865
<v Speaker 1>this pod and how much memory will you

00:11:34.865 --> 00:11:36.145
<v Speaker 1>need for this pod. And that's just the

00:11:36.145 --> 00:11:38.545
<v Speaker 1>request. It's just used by the scheduler to

00:11:38.545 --> 00:11:40.385
<v Speaker 1>decide which node it should put your pod

00:11:40.385 --> 00:11:40.945
<v Speaker 1>on.

00:11:42.545 --> 00:11:43.185
<v Speaker 0>Nice.

00:11:43.985 --> 00:11:44.705
<v Speaker 1>So

00:11:45.920 --> 00:11:47.600
<v Speaker 1>so so the reason why we're here today

00:11:47.600 --> 00:11:49.760
<v Speaker 1>and what we're doing with with KRR is

00:11:49.760 --> 00:11:51.120
<v Speaker 1>we're just trying to help you set those

00:11:51.120 --> 00:11:51.840
<v Speaker 1>two numbers.

00:11:52.960 --> 00:11:55.680
<v Speaker 0>Oh, it's a noble a noble mission. I'm

00:11:55.680 --> 00:11:56.880
<v Speaker 0>curious, though. Right? Like,

00:11:58.475 --> 00:12:00.475
<v Speaker 0>what where were you setting? What were you

00:12:00.475 --> 00:12:02.315
<v Speaker 0>doing when you were like, alright. Let's write

00:12:02.315 --> 00:12:03.995
<v Speaker 0>a new open source tool that goes and

00:12:03.995 --> 00:12:05.834
<v Speaker 0>does what KRR does. Like, what was the

00:12:05.834 --> 00:12:07.355
<v Speaker 0>motivation? How how

00:12:07.834 --> 00:12:09.595
<v Speaker 0>how did you decide this is my next

00:12:09.595 --> 00:12:10.555
<v Speaker 0>thing that I'm gonna build?

00:12:12.180 --> 00:12:13.140
<v Speaker 1>So it

00:12:13.620 --> 00:12:15.779
<v Speaker 1>came up because we heard again and again

00:12:15.940 --> 00:12:17.860
<v Speaker 1>from people who were using robust and were

00:12:17.860 --> 00:12:18.980
<v Speaker 1>using other stuff,

00:12:19.540 --> 00:12:20.899
<v Speaker 1>or other platforms.

00:12:21.220 --> 00:12:21.779
<v Speaker 1>And

00:12:22.420 --> 00:12:23.779
<v Speaker 1>they came to us again and again. They

00:12:23.779 --> 00:12:25.860
<v Speaker 1>said, okay. How can I set CPU requests?

00:12:26.154 --> 00:12:28.235
<v Speaker 1>How can I set memory requests? And what

00:12:28.235 --> 00:12:29.355
<v Speaker 1>should they be? And the reason it came

00:12:29.355 --> 00:12:31.035
<v Speaker 1>up is because we have alerts in robust

00:12:31.035 --> 00:12:32.715
<v Speaker 1>about reliability issues.

00:12:32.875 --> 00:12:34.555
<v Speaker 1>We didn't do anything related to efficiency, but

00:12:34.555 --> 00:12:36.395
<v Speaker 1>we have these reliability alerts.

00:12:36.395 --> 00:12:38.714
<v Speaker 1>Like, you're having CPU throttling because your request

00:12:38.714 --> 00:12:40.075
<v Speaker 1>is wrong or because

00:12:40.400 --> 00:12:42.720
<v Speaker 1>you set the limit that's blocking that. So

00:12:42.720 --> 00:12:45.200
<v Speaker 1>we started getting reliability questions,

00:12:45.520 --> 00:12:47.200
<v Speaker 1>and very often, the answer to that reliability

00:12:47.200 --> 00:12:49.280
<v Speaker 1>question was you need to set their

00:12:49.920 --> 00:12:51.680
<v Speaker 1>you need to set your allocations differently. You

00:12:51.680 --> 00:12:53.920
<v Speaker 1>need to, like, allocate more CPU up front

00:12:53.920 --> 00:12:55.120
<v Speaker 1>for this, or you need to allocate more

00:12:55.120 --> 00:12:56.705
<v Speaker 1>memory up front for this in order to

00:12:56.705 --> 00:12:58.145
<v Speaker 1>make sure it runs smoothly.

00:12:58.385 --> 00:13:00.225
<v Speaker 1>And we tell people that, then they come

00:13:00.225 --> 00:13:01.265
<v Speaker 1>back and say, okay. Well, what should I

00:13:01.265 --> 00:13:02.625
<v Speaker 1>put in for the CPU request? What should

00:13:02.625 --> 00:13:04.065
<v Speaker 1>I put in for the memory request?

00:13:04.225 --> 00:13:04.785
<v Speaker 1>And

00:13:05.105 --> 00:13:06.865
<v Speaker 1>our answer was always like, okay. Well, look

00:13:06.865 --> 00:13:08.625
<v Speaker 1>at the vertical pod autoscaler. Look at these

00:13:08.625 --> 00:13:09.745
<v Speaker 1>these other systems.

00:13:10.065 --> 00:13:10.545
<v Speaker 1>And

00:13:11.070 --> 00:13:12.750
<v Speaker 1>we kept on hearing from people, no. That's

00:13:12.750 --> 00:13:14.590
<v Speaker 1>not working for me because of x, y,

00:13:14.590 --> 00:13:16.510
<v Speaker 1>and z. That's not working for me because

00:13:16.510 --> 00:13:18.270
<v Speaker 1>I can't actually use the VPA because I'm

00:13:18.270 --> 00:13:20.350
<v Speaker 1>using an HPA at the same time. Or

00:13:20.350 --> 00:13:21.950
<v Speaker 1>I wanna use the VPA,

00:13:22.190 --> 00:13:24.455
<v Speaker 1>but how do I know, like, when there's

00:13:24.455 --> 00:13:26.135
<v Speaker 1>a new recommendation I should apply that's off

00:13:26.135 --> 00:13:27.175
<v Speaker 1>by 20%?

00:13:27.255 --> 00:13:28.935
<v Speaker 1>Or I wanna use the VPA,

00:13:29.415 --> 00:13:31.735
<v Speaker 1>but I wanna get recommendations that are stable

00:13:31.735 --> 00:13:34.215
<v Speaker 1>over time that aren't constantly changing. So we

00:13:34.215 --> 00:13:35.975
<v Speaker 1>started hearing about this, and

00:13:36.400 --> 00:13:38.000
<v Speaker 1>the first thought we had in our mind

00:13:38.000 --> 00:13:39.920
<v Speaker 1>was, okay. We're gonna take the VPA. For

00:13:39.920 --> 00:13:41.280
<v Speaker 1>people who don't know, the VPA or the

00:13:41.280 --> 00:13:42.720
<v Speaker 1>vertical pilot autoscaler,

00:13:42.960 --> 00:13:43.760
<v Speaker 1>was in

00:13:44.160 --> 00:13:46.720
<v Speaker 1>I think, like, until KRR, I think, just

00:13:46.720 --> 00:13:49.975
<v Speaker 1>the probably the only real way at scale

00:13:49.975 --> 00:13:51.975
<v Speaker 1>to determine this stuff in your cluster determine

00:13:51.975 --> 00:13:53.095
<v Speaker 1>request limits.

00:13:53.575 --> 00:13:55.335
<v Speaker 1>I'm

00:13:55.335 --> 00:13:57.015
<v Speaker 1>probably forgetting something. I shouldn't say that. I'm

00:13:57.015 --> 00:13:58.695
<v Speaker 1>probably forgetting other tools out there. But it

00:13:58.695 --> 00:13:59.895
<v Speaker 1>was the number one way.

00:14:02.110 --> 00:14:04.110
<v Speaker 0>Yeah. I mean, there are other tools out

00:14:04.110 --> 00:14:06.750
<v Speaker 0>there that don't actually integrate natively with Kubernetes

00:14:06.750 --> 00:14:08.990
<v Speaker 0>like Parker, which can profile our application and

00:14:08.990 --> 00:14:10.589
<v Speaker 0>tell you what values are over time. But

00:14:10.589 --> 00:14:12.029
<v Speaker 0>you still have to do the analysis. You

00:14:12.029 --> 00:14:13.230
<v Speaker 0>still have to kinda look at that and

00:14:13.230 --> 00:14:15.149
<v Speaker 0>work it out and configure it on VPN

00:14:15.149 --> 00:14:17.485
<v Speaker 0>and stuff like that. So definitely, there's a

00:14:17.485 --> 00:14:18.765
<v Speaker 0>lot of room for

00:14:19.485 --> 00:14:21.805
<v Speaker 0>tooling to improve and make this easier for

00:14:21.805 --> 00:14:22.445
<v Speaker 0>teams.

00:14:22.845 --> 00:14:24.605
<v Speaker 1>And there are proprietary solutions out there as

00:14:24.605 --> 00:14:26.525
<v Speaker 1>well, I should say. There are proprietary

00:14:26.605 --> 00:14:29.325
<v Speaker 1>solutions, I think, specifically for Kubernetes

00:14:29.840 --> 00:14:31.200
<v Speaker 1>that also can do it. But there there

00:14:31.200 --> 00:14:32.640
<v Speaker 1>was no other than the VPA, I think,

00:14:32.640 --> 00:14:35.440
<v Speaker 1>like, there was no open source way that

00:14:35.440 --> 00:14:36.800
<v Speaker 1>that we like to do it. And then

00:14:36.800 --> 00:14:38.080
<v Speaker 1>the first thought that went through our head,

00:14:38.080 --> 00:14:39.440
<v Speaker 1>and I said to the team, like, we're

00:14:39.440 --> 00:14:41.760
<v Speaker 1>not developing this from scratch. I I was

00:14:41.760 --> 00:14:43.845
<v Speaker 1>pretty stubborn for a very long time with

00:14:43.845 --> 00:14:45.204
<v Speaker 1>the team. The team said we need to

00:14:45.204 --> 00:14:47.764
<v Speaker 1>develop an alternative to the VPA. I said,

00:14:47.764 --> 00:14:50.005
<v Speaker 1>we're not developing an alternative to the VPA.

00:14:50.324 --> 00:14:52.725
<v Speaker 1>Take the output of the VPA and add

00:14:52.725 --> 00:14:55.524
<v Speaker 1>on reporting features, give a better experience, but

00:14:55.524 --> 00:14:57.204
<v Speaker 1>we are not developing an alternative to the

00:14:57.204 --> 00:14:58.324
<v Speaker 1>VPA. It doesn't make sense.

00:14:59.029 --> 00:14:59.589
<v Speaker 1>And,

00:15:00.949 --> 00:15:02.949
<v Speaker 1>eventually, I caved because

00:15:03.110 --> 00:15:05.029
<v Speaker 1>we looked at it, and we really looked

00:15:05.029 --> 00:15:06.949
<v Speaker 1>at it from the product requirements and, like,

00:15:06.949 --> 00:15:08.709
<v Speaker 1>from the product spec and what we wanted

00:15:08.709 --> 00:15:10.709
<v Speaker 1>to accomplish with this, and we discovered we

00:15:10.709 --> 00:15:12.149
<v Speaker 1>couldn't do it on top of the VPA.

00:15:12.149 --> 00:15:14.295
<v Speaker 1>So I was kinda dragged kicking and screaming

00:15:14.295 --> 00:15:16.135
<v Speaker 1>and, like, objecting the entire way and said,

00:15:16.135 --> 00:15:17.415
<v Speaker 1>okay. You know what? We will do our

00:15:17.415 --> 00:15:18.855
<v Speaker 1>own recommendations engine.

00:15:19.575 --> 00:15:20.215
<v Speaker 0>Okay.

00:15:20.535 --> 00:15:21.095
<v Speaker 0>So

00:15:21.654 --> 00:15:24.455
<v Speaker 0>I'm not that familiar with KRR.

00:15:24.935 --> 00:15:26.615
<v Speaker 0>Is it a replacement for the VPA?

00:15:28.100 --> 00:15:29.300
<v Speaker 0>We do. That's

00:15:30.260 --> 00:15:30.820
<v Speaker 0>Sorry. We

00:15:31.700 --> 00:15:32.900
<v Speaker 1>No. Go on. Yeah.

00:15:33.779 --> 00:15:36.100
<v Speaker 0>Sorry. I I like so I I seen

00:15:36.100 --> 00:15:38.100
<v Speaker 0>the announcement. I went on the GitHub page.

00:15:38.100 --> 00:15:39.380
<v Speaker 0>I kinda give it a scan. You know

00:15:39.380 --> 00:15:40.574
<v Speaker 0>how I like to do these, James? Like,

00:15:40.574 --> 00:15:41.615
<v Speaker 0>I I like to come out with fresh

00:15:41.615 --> 00:15:43.615
<v Speaker 0>eyes and really learn from from you, right,

00:15:43.615 --> 00:15:45.055
<v Speaker 0>rather than trying to make my own assumptions

00:15:45.055 --> 00:15:47.134
<v Speaker 0>upfront. But from what I've seen is is

00:15:47.134 --> 00:15:48.814
<v Speaker 0>a command I run locally, and there's nothing

00:15:48.814 --> 00:15:50.255
<v Speaker 0>in the cluster. So I

00:15:50.574 --> 00:15:52.254
<v Speaker 0>I I didn't come into this. It's back

00:15:52.254 --> 00:15:53.774
<v Speaker 0>not to replace the VPA. I saw it

00:15:53.774 --> 00:15:54.894
<v Speaker 0>to be something that gave me, like, a

00:15:54.894 --> 00:15:55.375
<v Speaker 0>VPA

00:15:56.390 --> 00:15:58.310
<v Speaker 0>starting point or something. I wasn't I wasn't

00:15:58.310 --> 00:15:59.830
<v Speaker 0>actually that sure. So maybe you can correct

00:15:59.830 --> 00:16:01.190
<v Speaker 0>some of my assumptions then.

00:16:01.590 --> 00:16:04.150
<v Speaker 1>So the VPA has two modes. One mode

00:16:04.150 --> 00:16:06.790
<v Speaker 1>is it runs in cluster, and it's constantly

00:16:06.790 --> 00:16:09.430
<v Speaker 1>monitoring your pods. It's very heavily weighing its

00:16:09.430 --> 00:16:09.910
<v Speaker 1>recommendations

00:16:10.455 --> 00:16:12.215
<v Speaker 1>towards the last twenty four hours.

00:16:12.695 --> 00:16:14.455
<v Speaker 1>You can tune that, of course, but that's

00:16:14.455 --> 00:16:15.255
<v Speaker 1>the default.

00:16:15.495 --> 00:16:16.535
<v Speaker 1>And

00:16:16.615 --> 00:16:19.015
<v Speaker 1>then it's constantly, like, updating your pods.

00:16:20.455 --> 00:16:22.775
<v Speaker 1>And that we're not doing. Like, we decide,

00:16:22.775 --> 00:16:24.455
<v Speaker 1>at least in phase one, we are not

00:16:25.019 --> 00:16:27.579
<v Speaker 1>we are not modifying anything in production. We

00:16:27.579 --> 00:16:28.940
<v Speaker 1>are read only. We will give you a

00:16:28.940 --> 00:16:29.660
<v Speaker 1>recommendation,

00:16:29.820 --> 00:16:31.180
<v Speaker 1>but it's up to you to apply that

00:16:31.180 --> 00:16:33.180
<v Speaker 1>recommendation to review it. And this is a

00:16:33.180 --> 00:16:34.540
<v Speaker 1>human in the loop process.

00:16:35.019 --> 00:16:36.860
<v Speaker 1>So we're not competing with the BPA that

00:16:36.860 --> 00:16:38.139
<v Speaker 1>runs in cluster.

00:16:38.175 --> 00:16:40.815
<v Speaker 1>But we actually did some studies.

00:16:41.135 --> 00:16:43.135
<v Speaker 1>Like, I did some surveys on LinkedIn, and

00:16:43.135 --> 00:16:45.695
<v Speaker 1>we had a valid user interviews.

00:16:46.175 --> 00:16:48.175
<v Speaker 1>And we discovered that most people aren't running

00:16:48.175 --> 00:16:49.295
<v Speaker 1>the VPA in in,

00:16:50.095 --> 00:16:50.735
<v Speaker 1>like, the

00:16:51.230 --> 00:16:53.230
<v Speaker 1>automatic mode. Most people are running the VPA

00:16:53.230 --> 00:16:55.390
<v Speaker 1>in your recommendation mode where the VPA gives

00:16:55.390 --> 00:16:57.550
<v Speaker 1>recommendations, and then the human looks at it.

00:16:57.630 --> 00:16:59.230
<v Speaker 1>And then you go and you apply that.

00:16:59.310 --> 00:17:00.750
<v Speaker 1>So for that mode,

00:17:00.990 --> 00:17:02.670
<v Speaker 1>we are competing with it. We just don't

00:17:02.670 --> 00:17:04.430
<v Speaker 1>make you run something constantly in your cluster.

00:17:06.234 --> 00:17:08.315
<v Speaker 0>Ah, got it. Okay.

00:17:08.795 --> 00:17:09.434
<v Speaker 0>Nice.

00:17:09.994 --> 00:17:11.835
<v Speaker 1>So that that comes back to also, like,

00:17:11.835 --> 00:17:13.595
<v Speaker 1>why we had to build this ourselves even

00:17:13.595 --> 00:17:14.634
<v Speaker 1>though I was very

00:17:15.194 --> 00:17:17.569
<v Speaker 1>very much against it at the beginning. To

00:17:17.569 --> 00:17:19.569
<v Speaker 1>use the VPA, you have to install something

00:17:19.569 --> 00:17:22.369
<v Speaker 1>in cluster, and you start getting recommendations

00:17:22.770 --> 00:17:23.730
<v Speaker 1>in the future.

00:17:24.210 --> 00:17:26.530
<v Speaker 1>Whereas most people already have a cluster. Like,

00:17:26.530 --> 00:17:28.050
<v Speaker 1>I have a cluster here set up locally

00:17:28.050 --> 00:17:28.690
<v Speaker 1>now.

00:17:28.930 --> 00:17:30.945
<v Speaker 1>It's had Prometheus forever.

00:17:32.625 --> 00:17:34.785
<v Speaker 1>I'm when I show KRR in a few

00:17:34.785 --> 00:17:36.145
<v Speaker 1>minutes, I'm just gonna run KRR on the

00:17:36.145 --> 00:17:38.465
<v Speaker 1>historical data, and I'm gonna get the recommendations

00:17:38.465 --> 00:17:39.265
<v Speaker 1>immediately.

00:17:39.425 --> 00:17:40.625
<v Speaker 1>And that ties back to what we wanna

00:17:40.625 --> 00:17:42.145
<v Speaker 1>do with Robusta, which is you have all

00:17:42.145 --> 00:17:43.345
<v Speaker 1>this observability data.

00:17:43.890 --> 00:17:45.409
<v Speaker 1>I wanna unlock,

00:17:45.410 --> 00:17:48.049
<v Speaker 1>like, the goal that's already hidden there. So

00:17:48.450 --> 00:17:50.130
<v Speaker 1>we wanted to make it as easy as

00:17:50.130 --> 00:17:51.330
<v Speaker 1>possible. We wanted to give you a command

00:17:51.330 --> 00:17:53.250
<v Speaker 1>you run locally. We didn't wanna make like,

00:17:53.250 --> 00:17:55.250
<v Speaker 1>add on any friction for running something in

00:17:55.250 --> 00:17:56.930
<v Speaker 1>your cluster because maybe you don't have permissions

00:17:56.930 --> 00:17:57.490
<v Speaker 1>to do so.

00:17:59.285 --> 00:18:00.004
<v Speaker 0>Okay.

00:18:00.725 --> 00:18:02.485
<v Speaker 0>Well, I guess we can dive straight into

00:18:02.485 --> 00:18:03.364
<v Speaker 0>the demo,

00:18:03.684 --> 00:18:05.285
<v Speaker 0>or I can guess what we're gonna see.

00:18:05.285 --> 00:18:07.205
<v Speaker 0>It's entirely up to you. But based on

00:18:07.285 --> 00:18:09.765
<v Speaker 0>you've you've mentioned Prometheus. You've mentioned KARs or

00:18:09.765 --> 00:18:12.164
<v Speaker 0>CLI tool. I'm assuming it's

00:18:12.164 --> 00:18:14.164
<v Speaker 0>a nice way of running queries against

00:18:14.450 --> 00:18:17.890
<v Speaker 0>workloads on Prometheus to get top, middle, bottom

00:18:17.890 --> 00:18:20.210
<v Speaker 0>values across in a certain amount of time

00:18:20.290 --> 00:18:22.050
<v Speaker 0>and then make recommendations based on top of

00:18:22.050 --> 00:18:25.730
<v Speaker 0>that. Yep. That's right. So let me just

00:18:26.050 --> 00:18:27.410
<v Speaker 1>let me clear

00:18:27.410 --> 00:18:30.095
<v Speaker 1>my screen, and I'm gonna share my screen.

00:18:30.095 --> 00:18:32.094
<v Speaker 1>Let's see. Yeah. Go for it.

00:18:36.575 --> 00:18:37.934
<v Speaker 1>And

00:18:37.934 --> 00:18:39.375
<v Speaker 1>let me blow this up. Can you see

00:18:39.375 --> 00:18:42.735
<v Speaker 1>my terminal window? I am just going to

00:18:42.735 --> 00:18:43.615
<v Speaker 0>make that happen.

00:18:49.190 --> 00:18:51.670
<v Speaker 0>Alright. Now we see your terminal. But can

00:18:51.670 --> 00:18:53.270
<v Speaker 0>you make the font a little bit bigger,

00:18:53.270 --> 00:18:56.070
<v Speaker 0>please? Yes. I will. There we go. Perfect.

00:18:56.550 --> 00:18:59.245
<v Speaker 1>And can you see I wanna get over

00:18:59.245 --> 00:19:01.165
<v Speaker 1>here. Let's see if it follows. So this

00:19:01.165 --> 00:19:02.285
<v Speaker 1>is just the GitHub.

00:19:04.045 --> 00:19:04.605
<v Speaker 1>So

00:19:05.005 --> 00:19:06.845
<v Speaker 1>first thing I wanna do, just to give

00:19:06.845 --> 00:19:08.045
<v Speaker 1>you a teaser, this is what we're gonna

00:19:08.045 --> 00:19:08.285
<v Speaker 1>get.

00:19:10.559 --> 00:19:12.080
<v Speaker 1>What we're gonna get in just a minute

00:19:12.080 --> 00:19:13.840
<v Speaker 1>is we're gonna get these recommendations. So for

00:19:13.840 --> 00:19:15.759
<v Speaker 1>everything that's running in my cluster,

00:19:16.000 --> 00:19:17.360
<v Speaker 1>I'm gonna find out what should be the

00:19:17.360 --> 00:19:19.440
<v Speaker 1>CPU request, the CPU limits, memory request, and

00:19:19.440 --> 00:19:20.559
<v Speaker 1>their memory limits.

00:19:22.080 --> 00:19:24.559
<v Speaker 1>And this is based on the historical Prometheus

00:19:24.559 --> 00:19:24.880
<v Speaker 1>data.

00:19:26.955 --> 00:19:29.595
<v Speaker 1>So let's go back, though. So the first

00:19:29.595 --> 00:19:30.715
<v Speaker 1>thing I'm gonna do is I'm just gonna

00:19:30.715 --> 00:19:32.715
<v Speaker 1>install this. And I can install this a

00:19:32.715 --> 00:19:33.755
<v Speaker 1>few different ways.

00:19:34.715 --> 00:19:36.794
<v Speaker 1>I can either install via Brew,

00:19:37.195 --> 00:19:38.795
<v Speaker 1>or I can install by just cloning the

00:19:38.795 --> 00:19:40.555
<v Speaker 1>code. So I'm gonna just clone the codes.

00:19:44.860 --> 00:19:46.059
<v Speaker 1>And there we go.

00:19:48.059 --> 00:19:50.299
<v Speaker 1>And I'm gonna go back now, and I'm

00:19:50.299 --> 00:19:51.659
<v Speaker 1>gonna install the requirements.

00:19:52.299 --> 00:19:53.820
<v Speaker 1>And this is with Python. Of course, you

00:19:53.820 --> 00:19:55.340
<v Speaker 1>don't have to have Python. You could just

00:19:55.340 --> 00:19:55.900
<v Speaker 1>use

00:19:57.394 --> 00:20:00.274
<v Speaker 1>you could just use, the prebuilt binaries, so

00:20:00.274 --> 00:20:01.554
<v Speaker 1>you don't actually have to have Python to

00:20:01.554 --> 00:20:02.354
<v Speaker 1>run this.

00:20:02.835 --> 00:20:05.715
<v Speaker 1>Whoops. Clear. And I'm gonna go back. Finally,

00:20:05.715 --> 00:20:07.154
<v Speaker 1>I'm just gonna copy this command.

00:20:10.720 --> 00:20:12.080
<v Speaker 1>And I believe in my case, it has

00:20:12.080 --> 00:20:13.759
<v Speaker 1>to be Python 3.9.

00:20:15.440 --> 00:20:17.200
<v Speaker 1>And I'm not gonna get anything by default.

00:20:17.200 --> 00:20:18.879
<v Speaker 1>It's gonna ask me to choose a strategy.

00:20:19.120 --> 00:20:20.880
<v Speaker 1>And right now, we have one strategy that

00:20:20.880 --> 00:20:21.919
<v Speaker 1>we call simple,

00:20:22.000 --> 00:20:23.519
<v Speaker 1>and this is the strategy

00:20:23.520 --> 00:20:24.720
<v Speaker 1>for how we calculate

00:20:25.144 --> 00:20:27.144
<v Speaker 1>what the CPU and the memory should be.

00:20:27.144 --> 00:20:27.624
<v Speaker 1>So

00:20:28.264 --> 00:20:30.264
<v Speaker 1>the default strategy and the first strategy that

00:20:30.264 --> 00:20:32.745
<v Speaker 1>we implemented, it will find the maximum amount

00:20:32.745 --> 00:20:33.464
<v Speaker 1>of memory,

00:20:34.024 --> 00:20:35.384
<v Speaker 1>and it's gonna use that as the baseline

00:20:35.384 --> 00:20:36.985
<v Speaker 1>for how much memory you your pod should

00:20:36.985 --> 00:20:38.745
<v Speaker 1>get, the maximum amount that they've used in

00:20:38.745 --> 00:20:39.065
<v Speaker 1>the past.

00:20:40.240 --> 00:20:41.600
<v Speaker 1>And for CPU,

00:20:41.679 --> 00:20:44.320
<v Speaker 1>it's gonna go, and I believe I will

00:20:44.320 --> 00:20:46.160
<v Speaker 1>have to zoom out to get something formatted

00:20:46.160 --> 00:20:46.960
<v Speaker 1>nicely.

00:20:47.120 --> 00:20:47.759
<v Speaker 0>Yep.

00:20:48.799 --> 00:20:50.399
<v Speaker 1>So let's just run that again.

00:20:50.799 --> 00:20:52.400
<v Speaker 1>And for CPU, it's gonna go and it's

00:20:52.400 --> 00:20:54.215
<v Speaker 1>gonna take the 99 percentile. So what we're

00:20:54.215 --> 00:20:55.495
<v Speaker 1>trying to do here is we're trying to

00:20:55.495 --> 00:20:58.295
<v Speaker 1>understand based on historical data, how much CPU

00:20:58.295 --> 00:21:00.375
<v Speaker 1>and memory your pods need. And then by

00:21:00.375 --> 00:21:01.975
<v Speaker 1>setting that number right,

00:21:02.215 --> 00:21:03.975
<v Speaker 1>then we're going to be able to reduce

00:21:03.975 --> 00:21:06.615
<v Speaker 1>costs while still maintaining reliability.

00:21:07.950 --> 00:21:10.030
<v Speaker 1>And that is really all there is to

00:21:10.030 --> 00:21:11.789
<v Speaker 1>it. So I just ran that, and it's

00:21:11.789 --> 00:21:14.110
<v Speaker 1>connected to Prometheus in my cluster, found it

00:21:14.110 --> 00:21:16.030
<v Speaker 1>automatically. So I didn't need to point out

00:21:16.030 --> 00:21:17.869
<v Speaker 1>Prometheus. I didn't need to do anything.

00:21:18.830 --> 00:21:19.070
<v Speaker 1>And

00:21:19.934 --> 00:21:21.215
<v Speaker 1>it went and it looked at each of

00:21:21.215 --> 00:21:22.734
<v Speaker 1>the workloads running in cluster,

00:21:22.975 --> 00:21:25.054
<v Speaker 1>and then it queried them in Prometheus.

00:21:25.134 --> 00:21:28.095
<v Speaker 1>And it found here the historical data, and

00:21:28.095 --> 00:21:29.695
<v Speaker 1>it's telling me what I should set based

00:21:29.695 --> 00:21:30.815
<v Speaker 1>on that historical data.

00:21:33.149 --> 00:21:33.789
<v Speaker 0>Nice.

00:21:34.750 --> 00:21:37.149
<v Speaker 1>So if I take an example here,

00:21:37.390 --> 00:21:39.710
<v Speaker 1>then here's an example, the Argo CD application

00:21:39.710 --> 00:21:41.789
<v Speaker 1>set controller. And right now, that has no

00:21:41.789 --> 00:21:43.629
<v Speaker 1>CPU request, so it's telling me,

00:21:43.950 --> 00:21:45.549
<v Speaker 1>one, I should set a CPU request, and

00:21:45.549 --> 00:21:47.965
<v Speaker 1>two, based on historical data, the right amount

00:21:47.965 --> 00:21:49.965
<v Speaker 1>for this would be five milli CPU.

00:21:50.524 --> 00:21:52.445
<v Speaker 1>The memory request is on set, but based

00:21:52.445 --> 00:21:54.684
<v Speaker 1>on the historical data, what this needs is

00:21:54.684 --> 00:21:56.364
<v Speaker 1>20 is 23

00:21:56.845 --> 00:21:58.205
<v Speaker 1>megabytes

00:21:58.205 --> 00:22:01.130
<v Speaker 1>of memory. And then for limits, for CPU

00:22:01.130 --> 00:22:02.650
<v Speaker 1>and memory limits, and there are different approaches

00:22:02.650 --> 00:22:04.809
<v Speaker 1>to how you set that. And there

00:22:05.049 --> 00:22:07.849
<v Speaker 1>is some controversy around that around different opinions.

00:22:08.169 --> 00:22:08.809
<v Speaker 1>But

00:22:09.850 --> 00:22:10.489
<v Speaker 1>we

00:22:11.049 --> 00:22:12.650
<v Speaker 1>have some options also that we're going to

00:22:12.650 --> 00:22:13.929
<v Speaker 1>expose for how you can control that.

00:22:16.635 --> 00:22:17.355
<v Speaker 0>Nice.

00:22:17.835 --> 00:22:19.435
<v Speaker 0>So it scans that cluster. It gives me

00:22:19.435 --> 00:22:20.315
<v Speaker 0>recommendations.

00:22:20.315 --> 00:22:21.915
<v Speaker 0>I can pump them straight into my GitOps

00:22:21.915 --> 00:22:23.755
<v Speaker 0>pipeline, and off I go. I got the

00:22:23.755 --> 00:22:25.035
<v Speaker 0>happier cluster. Right?

00:22:25.595 --> 00:22:26.315
<v Speaker 1>Yep.

00:22:27.035 --> 00:22:28.635
<v Speaker 0>Yeah. I think what's

00:22:29.450 --> 00:22:32.330
<v Speaker 0>interesting here is, like, you know, I work

00:22:32.810 --> 00:22:34.650
<v Speaker 0>guess I'm in a a unique position, right,

00:22:34.650 --> 00:22:36.250
<v Speaker 0>as a consultant these days. Right? I don't

00:22:36.250 --> 00:22:37.690
<v Speaker 0>work on my own cluster. I spend a

00:22:37.690 --> 00:22:39.370
<v Speaker 0>lot of my time working with everyone else's

00:22:39.370 --> 00:22:40.010
<v Speaker 0>clusters,

00:22:40.410 --> 00:22:42.105
<v Speaker 0>which is a blessing and a curse. But

00:22:42.105 --> 00:22:43.065
<v Speaker 0>what I've seen

00:22:44.025 --> 00:22:44.985
<v Speaker 0>constantly

00:22:45.145 --> 00:22:47.385
<v Speaker 0>is that people really overestimate

00:22:47.385 --> 00:22:50.345
<v Speaker 0>their CPU requests on all of their workloads.

00:22:50.345 --> 00:22:52.505
<v Speaker 0>Like, you know, usually, they're straight in there

00:22:52.505 --> 00:22:54.345
<v Speaker 0>with the 500 millicores, you know, half of

00:22:54.345 --> 00:22:56.970
<v Speaker 0>half a core. And you're like, like, why?

00:22:56.970 --> 00:22:59.210
<v Speaker 0>Like, that's that's a lot of CPU to

00:22:59.210 --> 00:23:01.290
<v Speaker 0>be thrown into your, you know, three meg

00:23:01.290 --> 00:23:04.410
<v Speaker 0>go application that runs once every fifteen seconds

00:23:04.410 --> 00:23:05.210
<v Speaker 0>or whatever.

00:23:06.010 --> 00:23:07.770
<v Speaker 0>And I think just having tools like this

00:23:07.770 --> 00:23:10.010
<v Speaker 0>that can get people visibility and to actually

00:23:10.285 --> 00:23:12.605
<v Speaker 0>what the average application really needs from a

00:23:12.605 --> 00:23:14.765
<v Speaker 0>CPU request kind of view is is much

00:23:14.765 --> 00:23:16.845
<v Speaker 0>less. Like, I I don't think I've ever

00:23:16.845 --> 00:23:19.005
<v Speaker 0>seen someone with a cluster that applies five

00:23:19.005 --> 00:23:20.524
<v Speaker 0>m as their CPU.

00:23:20.685 --> 00:23:22.765
<v Speaker 0>And I think that's very eye opening. I

00:23:22.765 --> 00:23:24.910
<v Speaker 0>hope it's eye opening for people that you

00:23:24.910 --> 00:23:26.590
<v Speaker 0>don't need to go nuts with this. And

00:23:26.590 --> 00:23:28.270
<v Speaker 0>even where it's not 500, they set it

00:23:28.270 --> 00:23:29.870
<v Speaker 0>to 200 or they set it to, like,

00:23:29.870 --> 00:23:31.710
<v Speaker 0>one fifty. It's like, it's always very high

00:23:31.710 --> 00:23:32.350
<v Speaker 0>numbers.

00:23:32.750 --> 00:23:34.990
<v Speaker 0>And there's I'll call out GKE here for

00:23:34.990 --> 00:23:37.845
<v Speaker 0>being notoriously bad. But if you run, sorry,

00:23:37.845 --> 00:23:40.085
<v Speaker 0>GKE. But if you run an autopilot cluster,

00:23:40.085 --> 00:23:43.365
<v Speaker 0>it actually does, like, automatic VPA for you.

00:23:43.605 --> 00:23:45.765
<v Speaker 0>And, that over provisions by a factor of

00:23:45.765 --> 00:23:47.685
<v Speaker 0>10 based on what I've seen as well.

00:23:47.685 --> 00:23:49.605
<v Speaker 0>Like, it's it's it's hugely over provisioning.

00:23:50.400 --> 00:23:52.960
<v Speaker 1>Yep. We've gotten some feedback from people that

00:23:52.960 --> 00:23:55.440
<v Speaker 1>we should over provision a little bit more,

00:23:55.840 --> 00:23:58.000
<v Speaker 1>to give a little more buffer. So we're

00:23:58.000 --> 00:24:01.280
<v Speaker 1>actually considering changing it from, like, five m

00:24:01.280 --> 00:24:02.080
<v Speaker 1>to 50 m.

00:24:02.804 --> 00:24:04.485
<v Speaker 1>But if you look at the big scale

00:24:04.485 --> 00:24:05.845
<v Speaker 1>of it, it's not even the 50 m

00:24:05.845 --> 00:24:07.924
<v Speaker 1>versus the five m. Right? Because how many

00:24:07.924 --> 00:24:09.524
<v Speaker 1>applications do you really have with five m

00:24:09.524 --> 00:24:12.004
<v Speaker 1>or 50 m? It's with those 500 megabyte

00:24:12.004 --> 00:24:13.445
<v Speaker 1>or those one core

00:24:13.684 --> 00:24:14.485
<v Speaker 1>machines.

00:24:14.840 --> 00:24:16.840
<v Speaker 1>And if you go back to that cystic

00:24:16.840 --> 00:24:19.800
<v Speaker 1>study, sixty nine percent is over provisioned on

00:24:19.800 --> 00:24:20.760
<v Speaker 1>CPU.

00:24:20.760 --> 00:24:23.240
<v Speaker 1>So take a typical company that has a

00:24:23.240 --> 00:24:25.240
<v Speaker 1>cloud bill of, let's say, don't

00:24:25.880 --> 00:24:27.825
<v Speaker 1>know, let's say 5 k a month on

00:24:27.825 --> 00:24:28.544
<v Speaker 1>GKE,

00:24:28.625 --> 00:24:31.664
<v Speaker 1>right, or on AWS or on Kubernetes.

00:24:31.745 --> 00:24:34.144
<v Speaker 1>So if you're paying 5 k a month

00:24:34.145 --> 00:24:35.585
<v Speaker 1>and you're let me just do the math

00:24:35.585 --> 00:24:37.424
<v Speaker 1>now. And you're 69%

00:24:37.424 --> 00:24:38.225
<v Speaker 1>over provisioned,

00:24:39.929 --> 00:24:40.649
<v Speaker 1>then

00:24:40.730 --> 00:24:42.809
<v Speaker 1>you're paying almost 3.5

00:24:42.809 --> 00:24:44.089
<v Speaker 1>k extra a month.

00:24:45.769 --> 00:24:48.009
<v Speaker 1>So more than half of like, that's slicing

00:24:48.009 --> 00:24:50.570
<v Speaker 1>more your cloud bill in more than half.

00:24:51.850 --> 00:24:52.250
<v Speaker 1>Yep.

00:24:54.625 --> 00:24:55.664
<v Speaker 0>Yeah. I

00:24:56.065 --> 00:24:57.425
<v Speaker 0>don't know. I I don't think I would

00:24:57.425 --> 00:24:59.025
<v Speaker 0>go from 5 m to 50 m even

00:24:59.025 --> 00:25:00.385
<v Speaker 0>if people are calling. I need to speak

00:25:00.385 --> 00:25:01.105
<v Speaker 0>higher.

00:25:01.185 --> 00:25:03.105
<v Speaker 0>Like, I would encourage people, you you know,

00:25:03.105 --> 00:25:05.025
<v Speaker 0>run KRR, but then back it up. Run

00:25:05.025 --> 00:25:07.025
<v Speaker 0>cube control top pods

00:25:06.820 --> 00:25:08.420
<v Speaker 0>and see the low numbers for the current

00:25:08.420 --> 00:25:10.980
<v Speaker 0>utilization of CPU. Like, it's never that high

00:25:10.980 --> 00:25:13.300
<v Speaker 0>unless you've got a Java workload in there

00:25:13.300 --> 00:25:15.060
<v Speaker 0>in which you've got other problems. I'm really

00:25:15.060 --> 00:25:18.020
<v Speaker 0>sorry. But, you know, you you don't need

00:25:18.020 --> 00:25:19.940
<v Speaker 0>high levels of CPU requests

00:25:20.100 --> 00:25:22.165
<v Speaker 0>for most of them. So this is what

00:25:22.165 --> 00:25:24.085
<v Speaker 1>I'm actually the most excited about in terms

00:25:24.085 --> 00:25:25.365
<v Speaker 1>of what we're working on.

00:25:25.925 --> 00:25:28.325
<v Speaker 1>You get a recommendation like this.

00:25:28.965 --> 00:25:30.485
<v Speaker 1>The first thing that you're gonna go and

00:25:30.485 --> 00:25:32.725
<v Speaker 1>do afterwards is you wanna validate this recommendation.

00:25:32.725 --> 00:25:33.045
<v Speaker 1>Right?

00:25:34.080 --> 00:25:35.440
<v Speaker 1>And you're gonna go and you're gonna look

00:25:35.440 --> 00:25:37.679
<v Speaker 1>at historical graph of this data. And then

00:25:37.679 --> 00:25:39.520
<v Speaker 1>based on that historical graph, you're gonna, like,

00:25:39.520 --> 00:25:41.440
<v Speaker 1>draw a line across that and say, okay.

00:25:41.520 --> 00:25:43.600
<v Speaker 1>How would I perform on this recommendation in

00:25:43.600 --> 00:25:45.440
<v Speaker 1>the past three three weeks? Right?

00:25:46.720 --> 00:25:47.440
<v Speaker 1>So

00:25:48.005 --> 00:25:50.725
<v Speaker 1>we're actually it's, like, being worked on the

00:25:50.725 --> 00:25:52.485
<v Speaker 1>right right now that I really wanted to

00:25:52.485 --> 00:25:54.005
<v Speaker 1>show on the show today, but it wasn't

00:25:54.005 --> 00:25:56.085
<v Speaker 1>on time. But just any day now, we're

00:25:56.085 --> 00:25:57.765
<v Speaker 1>gonna have this out, is to be able

00:25:57.765 --> 00:25:59.845
<v Speaker 1>to also show you the justification for these

00:25:59.845 --> 00:26:01.845
<v Speaker 1>recommendations to, like, show you it on a

00:26:01.845 --> 00:26:04.290
<v Speaker 1>graph and then to draw a dotted line

00:26:04.290 --> 00:26:06.450
<v Speaker 1>across, say, like, okay. Here's what we're recommending,

00:26:06.610 --> 00:26:07.970
<v Speaker 1>and then this is how that would perform

00:26:07.970 --> 00:26:09.330
<v Speaker 1>in historical data.

00:26:10.049 --> 00:26:10.690
<v Speaker 1>And

00:26:11.490 --> 00:26:12.850
<v Speaker 1>so that's one of the things that we're

00:26:12.850 --> 00:26:14.530
<v Speaker 1>working on right now that I'm really excited

00:26:14.530 --> 00:26:15.809
<v Speaker 1>about. Because like you said, you're not gonna

00:26:15.809 --> 00:26:17.945
<v Speaker 1>just go and apply this recommendation probably. You

00:26:17.945 --> 00:26:20.185
<v Speaker 1>wanna see why. Right? You wanna visualize it.

00:26:20.185 --> 00:26:21.865
<v Speaker 1>You wanna go through and do this whole

00:26:21.865 --> 00:26:22.505
<v Speaker 1>process.

00:26:22.665 --> 00:26:24.665
<v Speaker 1>And then when we think about, like, how

00:26:24.665 --> 00:26:25.225
<v Speaker 1>we

00:26:26.185 --> 00:26:28.425
<v Speaker 1>how we give you recommendations, it's not just

00:26:28.425 --> 00:26:30.665
<v Speaker 1>about giving you the recommendation and the number.

00:26:30.665 --> 00:26:32.185
<v Speaker 1>It's about giving you the confidence to apply

00:26:32.185 --> 00:26:32.425
<v Speaker 1>it.

00:26:34.299 --> 00:26:35.419
<v Speaker 0>Yeah. Definitely.

00:26:35.899 --> 00:26:37.179
<v Speaker 0>And, like, if we go back to what

00:26:37.179 --> 00:26:38.620
<v Speaker 0>you said at the start of this demo.

00:26:38.620 --> 00:26:42.139
<v Speaker 0>Right? Like, you're doing Prometheus request. You're looking

00:26:42.299 --> 00:26:43.740
<v Speaker 0>how long did you say the status over?

00:26:43.740 --> 00:26:45.340
<v Speaker 0>Like, what? Three months or longer? Is it

00:26:45.340 --> 00:26:48.045
<v Speaker 0>all time? So it depends on your it

00:26:48.045 --> 00:26:49.965
<v Speaker 1>depends on your prometheus. Yeah.

00:26:50.605 --> 00:26:52.285
<v Speaker 0>Yeah. So if you got a retention policy

00:26:52.285 --> 00:26:54.685
<v Speaker 0>of, say, three weeks or maybe it's thirty

00:26:54.685 --> 00:26:56.445
<v Speaker 0>days, it doesn't really matter. I mean, this

00:26:56.445 --> 00:26:59.245
<v Speaker 0>is the max value over that retention policy.

00:26:59.680 --> 00:27:01.920
<v Speaker 0>So but I think that's that's really cool

00:27:01.920 --> 00:27:03.440
<v Speaker 0>for people just to really get that front

00:27:03.440 --> 00:27:05.280
<v Speaker 0>and center. Like, you maybe don't need to

00:27:05.280 --> 00:27:06.720
<v Speaker 0>draw a graph in the terminal to even

00:27:06.720 --> 00:27:08.400
<v Speaker 0>just pop them a link and say, look

00:27:08.400 --> 00:27:10.000
<v Speaker 0>at this and yourself.

00:27:10.080 --> 00:27:11.280
<v Speaker 0>It's like, go and take a look and

00:27:11.280 --> 00:27:13.765
<v Speaker 0>see. And then maybe what maybe in some

00:27:13.765 --> 00:27:15.205
<v Speaker 0>cases, you know, we do see a couple

00:27:15.205 --> 00:27:17.205
<v Speaker 0>of large numbers there. You've got your

00:27:18.405 --> 00:27:19.445
<v Speaker 0>platform

00:27:19.445 --> 00:27:22.405
<v Speaker 0>relay using 261.

00:27:22.405 --> 00:27:22.885
<v Speaker 0>Right?

00:27:23.365 --> 00:27:24.885
<v Speaker 0>That could have been a spike for thirty

00:27:24.885 --> 00:27:27.525
<v Speaker 0>second window two weeks ago. Like, you really

00:27:28.270 --> 00:27:30.270
<v Speaker 0>don't You wanna see it. Yeah. You wanna

00:27:30.270 --> 00:27:32.030
<v Speaker 1>see it. But so I'll tell you something

00:27:32.030 --> 00:27:34.029
<v Speaker 1>that's very interesting. When

00:27:34.750 --> 00:27:36.429
<v Speaker 1>you look at what's the maximum,

00:27:36.510 --> 00:27:38.750
<v Speaker 1>actually depends a lot on what time periods

00:27:38.750 --> 00:27:40.990
<v Speaker 1>you're looking at. So imagine that you say,

00:27:41.070 --> 00:27:43.149
<v Speaker 1>I'm gonna just go into Grafana,

00:27:43.230 --> 00:27:44.385
<v Speaker 1>and I'm gonna look at a two week

00:27:44.385 --> 00:27:45.025
<v Speaker 1>period.

00:27:45.265 --> 00:27:46.625
<v Speaker 1>And then in that two week period in

00:27:46.705 --> 00:27:48.385
<v Speaker 1>like, if I'm looking at Grafana, I'm gonna

00:27:48.385 --> 00:27:50.945
<v Speaker 1>take the maximum, right, of CPU.

00:27:51.745 --> 00:27:53.185
<v Speaker 1>But it actually

00:27:53.585 --> 00:27:55.985
<v Speaker 1>like, what that maximum is actually depends on

00:27:56.390 --> 00:27:58.790
<v Speaker 1>what your interval is that you're sampling at.

00:27:59.190 --> 00:27:59.830
<v Speaker 1>Because

00:28:00.150 --> 00:28:02.710
<v Speaker 1>imagine you have a pod that, it, like,

00:28:02.710 --> 00:28:04.790
<v Speaker 1>runs and it's doing nothing all the time,

00:28:05.030 --> 00:28:06.470
<v Speaker 1>and then it spikes

00:28:06.630 --> 00:28:08.710
<v Speaker 1>for just one second, and it goes all

00:28:08.710 --> 00:28:10.230
<v Speaker 1>the way up to 10 CPU, and then

00:28:10.230 --> 00:28:12.325
<v Speaker 1>it goes back down. And now you take

00:28:12.325 --> 00:28:13.764
<v Speaker 1>it and you graph this data over two

00:28:13.764 --> 00:28:14.644
<v Speaker 1>week periods.

00:28:14.725 --> 00:28:16.404
<v Speaker 1>And when you're graphing over two week period,

00:28:16.404 --> 00:28:18.085
<v Speaker 1>you're, like, taking one hour intervals and the

00:28:18.085 --> 00:28:20.164
<v Speaker 1>average over each hour. So you're never gonna

00:28:20.164 --> 00:28:22.245
<v Speaker 1>see that spike on that graph. It's just

00:28:22.245 --> 00:28:23.845
<v Speaker 1>gonna be a flat zero the entire time.

00:28:26.639 --> 00:28:29.919
<v Speaker 0>Yeah. And I think that's still okay. Right?

00:28:29.919 --> 00:28:30.639
<v Speaker 0>I mean,

00:28:30.960 --> 00:28:33.840
<v Speaker 0>these TPU requests are a scheduling concern. They're

00:28:33.840 --> 00:28:36.080
<v Speaker 0>not a runtime concern.

00:28:36.080 --> 00:28:38.239
<v Speaker 0>It's not like your application can consume

00:28:38.480 --> 00:28:40.399
<v Speaker 0>above these requests. So

00:28:41.385 --> 00:28:42.905
<v Speaker 0>they're they're there to help us schedule or

00:28:42.905 --> 00:28:45.385
<v Speaker 0>so you don't over provision nodes and you

00:28:45.385 --> 00:28:48.105
<v Speaker 0>reduce a class of problems that probably don't

00:28:48.105 --> 00:28:49.865
<v Speaker 0>really need to exist.

00:28:49.945 --> 00:28:52.585
<v Speaker 0>Well, considering the cost factor of just a

00:28:52.585 --> 00:28:54.265
<v Speaker 0>finitely scaling the number of nodes that you

00:28:54.265 --> 00:28:54.425
<v Speaker 0>have.

00:28:56.340 --> 00:28:58.100
<v Speaker 0>We have a question from Russell in the

00:28:58.100 --> 00:28:59.460
<v Speaker 0>chat. So I'm gonna pop that up. But

00:28:59.460 --> 00:29:00.500
<v Speaker 0>Russell asked,

00:29:01.140 --> 00:29:02.740
<v Speaker 0>does this tool or could this tool, I

00:29:02.740 --> 00:29:04.500
<v Speaker 0>guess, have a report that shows the idle

00:29:04.500 --> 00:29:06.500
<v Speaker 0>time of each node? So, you know, really

00:29:06.500 --> 00:29:08.625
<v Speaker 0>trying to hammer home that point about that

00:29:08.625 --> 00:29:09.425
<v Speaker 0>69%

00:29:09.425 --> 00:29:11.425
<v Speaker 0>nonutilization.

00:29:11.425 --> 00:29:13.184
<v Speaker 0>Like, can you show that on a node

00:29:13.265 --> 00:29:15.505
<v Speaker 0>basis or pod basis for these workloads? I

00:29:15.505 --> 00:29:16.945
<v Speaker 0>think that that's a good question.

00:29:17.505 --> 00:29:19.745
<v Speaker 1>Yes. So so not yet, but, yeah, I

00:29:19.745 --> 00:29:20.865
<v Speaker 1>mean, it's also coming. So

00:29:21.570 --> 00:29:24.210
<v Speaker 1>when you think and it also matters so

00:29:24.210 --> 00:29:25.490
<v Speaker 1>it's both on the node level and the

00:29:25.490 --> 00:29:27.330
<v Speaker 1>pod level. Like, on the node level,

00:29:27.809 --> 00:29:30.370
<v Speaker 1>you wanna understand, like, what's the utilization? What's

00:29:30.370 --> 00:29:32.529
<v Speaker 1>the idle time? Right? Like, how well am

00:29:32.529 --> 00:29:34.529
<v Speaker 1>I doing or is my company or team

00:29:34.529 --> 00:29:35.169
<v Speaker 1>doing

00:29:35.330 --> 00:29:36.929
<v Speaker 1>as a whole when it comes to efficiency?

00:29:37.305 --> 00:29:39.305
<v Speaker 1>Like, am I doing nine out of 10,

00:29:39.305 --> 00:29:40.665
<v Speaker 1>and then I'm not gonna worry about improving

00:29:40.665 --> 00:29:41.625
<v Speaker 1>this as much?

00:29:41.945 --> 00:29:44.505
<v Speaker 1>Or am I doing, like, one out of

00:29:44.505 --> 00:29:45.945
<v Speaker 1>10, and then I should really go and

00:29:45.945 --> 00:29:48.185
<v Speaker 1>focus on this? So we don't yet show

00:29:48.185 --> 00:29:50.345
<v Speaker 1>the nodes, but it's something that's being planned.

00:29:50.585 --> 00:29:51.865
<v Speaker 1>And then to tie into what you said

00:29:51.865 --> 00:29:53.960
<v Speaker 1>about the pods, it's not just the number

00:29:53.960 --> 00:29:56.360
<v Speaker 1>here. Like, if this number matters,

00:29:56.360 --> 00:29:58.840
<v Speaker 1>it actually is the difference here times the

00:29:58.840 --> 00:29:59.960
<v Speaker 1>number of pods.

00:30:03.320 --> 00:30:04.440
<v Speaker 1>Because think about it. Like, if you have

00:30:04.440 --> 00:30:06.600
<v Speaker 1>an application, a replica set that has a

00:30:06.600 --> 00:30:07.400
<v Speaker 1>hundred replicas.

00:30:07.625 --> 00:30:10.665
<v Speaker 0>Yeah. And you have a hundred megabyte difference

00:30:10.665 --> 00:30:12.345
<v Speaker 1>on all of them, then it's actually a

00:30:12.345 --> 00:30:13.385
<v Speaker 1>hundred times,

00:30:13.865 --> 00:30:14.505
<v Speaker 1>like,

00:30:14.825 --> 00:30:16.585
<v Speaker 1>a hundred megabytes. So it comes up to

00:30:16.585 --> 00:30:17.385
<v Speaker 1>be bigger.

00:30:18.345 --> 00:30:18.985
<v Speaker 0>Yeah.

00:30:20.265 --> 00:30:22.585
<v Speaker 0>That those number could get crazy pretty quickly,

00:30:22.585 --> 00:30:22.905
<v Speaker 0>actually.

00:30:24.210 --> 00:30:25.490
<v Speaker 0>You know? So

00:30:26.450 --> 00:30:27.169
<v Speaker 0>you

00:30:27.730 --> 00:30:30.130
<v Speaker 0>mentioned that this is a simple strategy and

00:30:30.130 --> 00:30:32.210
<v Speaker 0>that more strategies are gonna come. I'm assuming

00:30:32.210 --> 00:30:35.570
<v Speaker 0>you're looking into things like linear prediction, whole

00:30:35.570 --> 00:30:39.090
<v Speaker 0>winters, other statistical proxies for understanding

00:30:39.424 --> 00:30:41.905
<v Speaker 0>growth over time, averages over time, etcetera. Maybe

00:30:41.905 --> 00:30:44.065
<v Speaker 0>you could talk about what's coming, or what

00:30:44.065 --> 00:30:45.985
<v Speaker 0>ideas you've got for other strategies.

00:30:46.385 --> 00:30:48.304
<v Speaker 1>Yeah. Okay. So imagine I'll I'll give you

00:30:48.304 --> 00:30:49.264
<v Speaker 1>a scenario,

00:30:49.825 --> 00:30:50.465
<v Speaker 1>mate.

00:30:50.625 --> 00:30:52.865
<v Speaker 0>And don't say OpenAI chat GPT. I don't

00:30:52.865 --> 00:30:54.920
<v Speaker 0>wanna hear that. I'm not gonna say it.

00:30:54.920 --> 00:30:57.480
<v Speaker 1>I'm gonna say it. Then I just see

00:30:57.480 --> 00:30:59.000
<v Speaker 1>Russell said here in the chat that you

00:30:59.000 --> 00:31:00.200
<v Speaker 1>wanna see over time so you can see

00:31:00.200 --> 00:31:02.519
<v Speaker 1>your nose on something like 30% average.

00:31:02.680 --> 00:31:03.880
<v Speaker 1>And then on times, can go up to

00:31:03.880 --> 00:31:05.639
<v Speaker 1>70%. So, yeah, absolutely.

00:31:05.880 --> 00:31:07.080
<v Speaker 1>I don't know if that's

00:31:07.665 --> 00:31:09.105
<v Speaker 1>I don't know if that's something we're gonna

00:31:09.105 --> 00:31:10.705
<v Speaker 1>expose in the CLI or if it's something

00:31:10.705 --> 00:31:12.625
<v Speaker 1>we're gonna show in the SaaS platform. Maybe

00:31:12.625 --> 00:31:14.145
<v Speaker 1>I can just share my screen

00:31:14.705 --> 00:31:16.545
<v Speaker 1>for a second. And I still wanna answer

00:31:16.545 --> 00:31:18.065
<v Speaker 1>your question about the strategies.

00:31:18.145 --> 00:31:20.625
<v Speaker 1>I mean Yeah. Of course. Let me see.

00:31:28.600 --> 00:31:30.120
<v Speaker 1>Alright. So I'm gonna answer the question about

00:31:30.120 --> 00:31:32.040
<v Speaker 1>the strategies after first, and then maybe I'll

00:31:32.040 --> 00:31:33.080
<v Speaker 1>show something else afterwards.

00:31:36.005 --> 00:31:36.884
<v Speaker 1>So

00:31:37.284 --> 00:31:39.044
<v Speaker 1>imagine you have a workload now,

00:31:39.205 --> 00:31:42.084
<v Speaker 1>where you're running this workload and, like, every

00:31:42.164 --> 00:31:44.004
<v Speaker 1>weekend, it does nothing. Right?

00:31:44.884 --> 00:31:47.125
<v Speaker 1>And then the entire week, it goes and

00:31:47.125 --> 00:31:49.205
<v Speaker 1>it's, like, burning CPU or it has some

00:31:49.205 --> 00:31:50.245
<v Speaker 1>psych thickness. Right?

00:31:51.350 --> 00:31:52.230
<v Speaker 1>So

00:31:53.430 --> 00:31:54.950
<v Speaker 1>what you wanna do is you wanna apply

00:31:54.950 --> 00:31:57.030
<v Speaker 1>an algorithm, let's say, like, winters. Right? And

00:31:57.030 --> 00:31:59.430
<v Speaker 1>you wanna understand the, like, the you wanna

00:31:59.430 --> 00:32:01.350
<v Speaker 1>understand that there's some psychic behavior here, and,

00:32:01.350 --> 00:32:03.830
<v Speaker 1>like, there's some period periodicity.

00:32:04.345 --> 00:32:05.945
<v Speaker 1>Then on certain days, it performs a different

00:32:05.945 --> 00:32:08.105
<v Speaker 1>way. But you can't go and just set

00:32:08.105 --> 00:32:09.865
<v Speaker 1>a request and then limit because when you're

00:32:09.865 --> 00:32:11.625
<v Speaker 1>sending a request and then limit, you're sending

00:32:11.625 --> 00:32:12.665
<v Speaker 1>a flat value.

00:32:14.185 --> 00:32:16.825
<v Speaker 1>So in order to do this properly,

00:32:17.280 --> 00:32:18.560
<v Speaker 1>then what you have to do is you

00:32:18.560 --> 00:32:20.000
<v Speaker 1>have to have two parts that go hand

00:32:20.000 --> 00:32:21.840
<v Speaker 1>in hand together. On the one hand, you

00:32:21.840 --> 00:32:23.920
<v Speaker 1>have a better algorithm that understands the historical

00:32:23.920 --> 00:32:24.640
<v Speaker 1>data.

00:32:25.120 --> 00:32:26.960
<v Speaker 1>But on the other hand, your output, it

00:32:26.960 --> 00:32:28.640
<v Speaker 1>has to be richer output

00:32:28.800 --> 00:32:29.520
<v Speaker 1>than

00:32:30.145 --> 00:32:32.865
<v Speaker 1>just getting, like, just getting the request in

00:32:32.865 --> 00:32:34.145
<v Speaker 1>a moment. And then there are two approaches

00:32:34.145 --> 00:32:35.425
<v Speaker 1>you can go with this. Like, the first

00:32:35.425 --> 00:32:37.585
<v Speaker 1>approach is, okay. We're gonna run write this

00:32:37.585 --> 00:32:39.745
<v Speaker 1>really complicated Kubernetes operator,

00:32:39.825 --> 00:32:41.505
<v Speaker 1>and we're gonna apply all these different stuff

00:32:41.505 --> 00:32:43.425
<v Speaker 1>dynamically in cluster. We're gonna do it time

00:32:43.425 --> 00:32:44.865
<v Speaker 1>based, or we're do it based on, like,

00:32:45.510 --> 00:32:47.590
<v Speaker 1>what's coming into Kafka now. We're gonna do

00:32:47.590 --> 00:32:49.270
<v Speaker 1>it based on the CPU loads.

00:32:49.510 --> 00:32:50.070
<v Speaker 1>But,

00:32:51.030 --> 00:32:53.510
<v Speaker 1>actually, all that already exists. Like, if I

00:32:53.510 --> 00:32:55.990
<v Speaker 1>wanna do it scale based on time, then

00:32:55.990 --> 00:32:58.950
<v Speaker 1>KEDA has this really cool thing called, like,

00:32:58.950 --> 00:32:59.670
<v Speaker 1>a time scaler.

00:33:00.055 --> 00:33:02.055
<v Speaker 1>And I can say with CADA, this patch

00:33:02.055 --> 00:33:03.735
<v Speaker 1>run however many replicas on this day of

00:33:03.735 --> 00:33:04.935
<v Speaker 1>the week. On this day of the week,

00:33:04.935 --> 00:33:06.055
<v Speaker 1>it shouldn't run at all. On this day

00:33:06.055 --> 00:33:07.095
<v Speaker 1>of the week, it should run five times

00:33:07.095 --> 00:33:08.375
<v Speaker 1>as many replicas.

00:33:08.695 --> 00:33:09.495
<v Speaker 1>So

00:33:09.895 --> 00:33:11.975
<v Speaker 1>what we wanna do with other strategies

00:33:12.470 --> 00:33:14.950
<v Speaker 1>is to have strategies that have better algorithms.

00:33:15.110 --> 00:33:17.030
<v Speaker 1>And in terms of the output, though, we're

00:33:17.030 --> 00:33:18.950
<v Speaker 1>not giving you just a request in limit.

00:33:18.950 --> 00:33:20.790
<v Speaker 1>We're giving you an HPA config. We're giving

00:33:20.790 --> 00:33:23.190
<v Speaker 1>you a KLA config. We're giving you like,

00:33:23.190 --> 00:33:25.350
<v Speaker 1>we're generating the KLA or the HPA config

00:33:25.350 --> 00:33:27.510
<v Speaker 1>for for you based on that historical data.

00:33:29.534 --> 00:33:31.455
<v Speaker 0>Yeah. I think that would be really powerful.

00:33:31.855 --> 00:33:33.455
<v Speaker 0>You know, go back to the the example

00:33:33.455 --> 00:33:36.254
<v Speaker 0>that you were talking about. Let's assume your

00:33:36.255 --> 00:33:39.534
<v Speaker 0>ecommerce store that your traffic sets at five

00:33:39.534 --> 00:33:40.254
<v Speaker 0>m

00:33:40.335 --> 00:33:42.575
<v Speaker 0>for eleven months of the year or ten

00:33:42.575 --> 00:33:44.780
<v Speaker 0>months of the year. Right? But then at

00:33:44.780 --> 00:33:47.419
<v Speaker 0>Black Friday and over Christmas, you've got these

00:33:47.740 --> 00:33:50.140
<v Speaker 0>incredible spike in traffic and sales and you

00:33:50.140 --> 00:33:51.580
<v Speaker 0>don't wanna lose track you don't wanna lose

00:33:51.580 --> 00:33:53.100
<v Speaker 0>anything. Right? You don't wanna lose money just

00:33:53.100 --> 00:33:55.020
<v Speaker 0>because your infrastructure can't cope. This is for

00:33:55.020 --> 00:33:56.940
<v Speaker 0>a whole bunch of some of them really

00:33:56.940 --> 00:33:59.174
<v Speaker 0>excel because they can detect that the cycle

00:33:59.174 --> 00:34:01.094
<v Speaker 0>within the data and what's the

00:34:01.495 --> 00:34:03.495
<v Speaker 0>what the effect and other factors are if

00:34:03.495 --> 00:34:05.015
<v Speaker 0>you have that kind of data. So I

00:34:05.015 --> 00:34:06.695
<v Speaker 0>think it'd be really cool to have that

00:34:06.695 --> 00:34:08.295
<v Speaker 0>kind of thing in KRR.

00:34:09.094 --> 00:34:10.455
<v Speaker 0>And, again, this comes down to people having

00:34:10.455 --> 00:34:12.775
<v Speaker 0>retention periods and doing downsampling of their metrics,

00:34:12.739 --> 00:34:14.339
<v Speaker 0>which I think is a whole other world

00:34:14.339 --> 00:34:15.699
<v Speaker 0>of stuff that people need to get better

00:34:15.699 --> 00:34:18.020
<v Speaker 0>at. I think Prometheus has

00:34:18.500 --> 00:34:20.179
<v Speaker 0>has just taught people to have a thirty

00:34:20.179 --> 00:34:22.980
<v Speaker 0>day retention window, only look back thirty days.

00:34:22.980 --> 00:34:24.500
<v Speaker 0>But, actually, you need to be done sampling

00:34:24.500 --> 00:34:25.940
<v Speaker 0>that they unsore in it for long term

00:34:25.940 --> 00:34:28.420
<v Speaker 0>analysis over twelve, twenty four, thirty six months.

00:34:28.875 --> 00:34:30.715
<v Speaker 0>And really, if you wanna get good at

00:34:30.715 --> 00:34:32.955
<v Speaker 0>the stuff, that is, of course. But

00:34:33.114 --> 00:34:35.514
<v Speaker 0>having KRR be able to understand that, work

00:34:35.514 --> 00:34:37.435
<v Speaker 0>with that data, and create a KRR profile

00:34:37.435 --> 00:34:38.875
<v Speaker 0>would be really, really cool,

00:34:39.195 --> 00:34:41.550
<v Speaker 0>especially for these use cases where there are

00:34:41.550 --> 00:34:43.070
<v Speaker 0>a lot of ecommerce companies where they have

00:34:43.070 --> 00:34:44.350
<v Speaker 0>to scale up at Black Friday and at

00:34:44.350 --> 00:34:47.070
<v Speaker 0>Christmas. There are seasonal companies like holiday travel

00:34:47.070 --> 00:34:49.150
<v Speaker 0>agents that get really busy when people want

00:34:49.150 --> 00:34:50.430
<v Speaker 0>to pick their summer holidays.

00:34:50.989 --> 00:34:52.830
<v Speaker 0>There are Scottish people who just drink a

00:34:52.830 --> 00:34:54.270
<v Speaker 0>lot of whiskey at New Year, and, of

00:34:54.270 --> 00:34:56.594
<v Speaker 0>course, that's gonna take traffic somewhere on the

00:34:56.594 --> 00:34:59.395
<v Speaker 0>Internet. Maybe Twitter. Who knows? But, you know,

00:34:59.395 --> 00:35:01.395
<v Speaker 0>this opens that all up. So that's a

00:35:01.395 --> 00:35:02.915
<v Speaker 0>really cool enhancement. I'd love to see that

00:35:02.915 --> 00:35:04.195
<v Speaker 0>come. KRR.

00:35:04.195 --> 00:35:06.115
<v Speaker 0>And especially, who can indicate that because KRR

00:35:06.115 --> 00:35:08.049
<v Speaker 0>is a really cool project in this space

00:35:08.049 --> 00:35:08.690
<v Speaker 0>as well. So

00:35:09.569 --> 00:35:11.569
<v Speaker 1>I can say that, like, as a now

00:35:11.569 --> 00:35:12.849
<v Speaker 1>as someone who used to be a developer

00:35:12.849 --> 00:35:15.089
<v Speaker 1>and someone, like, who occasionally has to set

00:35:15.089 --> 00:35:16.210
<v Speaker 1>the values in these.

00:35:16.849 --> 00:35:17.730
<v Speaker 1>There's,

00:35:17.890 --> 00:35:20.049
<v Speaker 1>like, a black art to how you determine

00:35:20.454 --> 00:35:22.214
<v Speaker 1>the right k to configure, how you determine

00:35:22.214 --> 00:35:23.415
<v Speaker 1>the right HBA

00:35:23.415 --> 00:35:24.375
<v Speaker 1>scaler.

00:35:24.694 --> 00:35:25.494
<v Speaker 1>And

00:35:26.214 --> 00:35:28.135
<v Speaker 1>what we wanna do is we wanna make

00:35:28.135 --> 00:35:29.655
<v Speaker 1>it all, like, really simple. So if I

00:35:29.655 --> 00:35:30.934
<v Speaker 1>I take it back to, like, where we

00:35:30.934 --> 00:35:33.015
<v Speaker 1>started this conversation and people are moving to

00:35:33.015 --> 00:35:34.615
<v Speaker 1>the cloud and you're moving to Kubernetes and

00:35:34.615 --> 00:35:35.750
<v Speaker 1>the cloud is supposed to bring you all

00:35:35.750 --> 00:35:37.190
<v Speaker 1>these awesome cost savings.

00:35:37.510 --> 00:35:38.630
<v Speaker 1>And then you look at it, and you

00:35:38.630 --> 00:35:40.869
<v Speaker 1>have these nodes that are at 30% utilization.

00:35:40.869 --> 00:35:41.430
<v Speaker 1>Right?

00:35:41.910 --> 00:35:44.390
<v Speaker 0>Mhmm. It didn't deliver on that promise.

00:35:44.790 --> 00:35:47.190
<v Speaker 1>And a big part of what we're trying

00:35:47.190 --> 00:35:49.785
<v Speaker 1>to do is with open source software and

00:35:49.785 --> 00:35:52.505
<v Speaker 1>with your existing observability data, let's say, okay.

00:35:52.505 --> 00:35:54.905
<v Speaker 1>Yeah. There's some added complexity here

00:35:56.185 --> 00:35:57.385
<v Speaker 1>with Kubernetes

00:35:57.385 --> 00:35:58.105
<v Speaker 1>and,

00:35:59.065 --> 00:36:00.665
<v Speaker 1>of course, with the cloud and with Keta

00:36:00.665 --> 00:36:02.505
<v Speaker 1>and with HPA. All these things add complexity,

00:36:02.980 --> 00:36:05.220
<v Speaker 1>but we can get you to this maximum

00:36:05.220 --> 00:36:06.980
<v Speaker 1>that is way, way better than where you

00:36:06.980 --> 00:36:08.180
<v Speaker 1>were when you were running on prem where

00:36:08.180 --> 00:36:09.700
<v Speaker 1>you had your own data center.

00:36:10.020 --> 00:36:10.660
<v Speaker 1>But

00:36:10.900 --> 00:36:12.180
<v Speaker 1>you have to be able to take that

00:36:12.180 --> 00:36:13.780
<v Speaker 1>data in to be able to extract meaningful

00:36:13.780 --> 00:36:14.580
<v Speaker 1>insights from that.

00:36:17.695 --> 00:36:18.255
<v Speaker 0>So

00:36:18.815 --> 00:36:20.575
<v Speaker 0>is this that's something kicking around in my

00:36:20.575 --> 00:36:22.175
<v Speaker 0>head. And I know you've said, right, that

00:36:22.175 --> 00:36:24.335
<v Speaker 0>you were dragged kicking and screaming into not

00:36:24.335 --> 00:36:25.695
<v Speaker 0>working with the VPA.

00:36:25.855 --> 00:36:27.695
<v Speaker 0>And you've also told us on the stream

00:36:27.695 --> 00:36:29.695
<v Speaker 0>that, you know, you don't it's read only.

00:36:29.695 --> 00:36:31.295
<v Speaker 0>You don't want to modify a cluster.

00:36:32.070 --> 00:36:34.070
<v Speaker 0>But I'm sitting here and I'm looking like,

00:36:34.550 --> 00:36:37.590
<v Speaker 0>I have a robusta in my cluster already.

00:36:38.070 --> 00:36:41.270
<v Speaker 0>Right? You know, there's there's an operator that

00:36:41.270 --> 00:36:42.150
<v Speaker 0>does things.

00:36:42.950 --> 00:36:44.070
<v Speaker 0>Would you not

00:36:45.095 --> 00:36:47.415
<v Speaker 0>would you not look at just kind of

00:36:48.135 --> 00:36:50.454
<v Speaker 0>broadening the scope a little bit and trying

00:36:50.454 --> 00:36:51.015
<v Speaker 0>to

00:36:51.255 --> 00:36:53.335
<v Speaker 0>who can to, like, create in these horizontal

00:36:53.335 --> 00:36:56.454
<v Speaker 0>pod things, updating objects in Kubernetes cluster. Is

00:36:56.454 --> 00:36:57.415
<v Speaker 0>that not something

00:36:57.849 --> 00:36:59.770
<v Speaker 0>I mean, you're you're you're right there. Right?

00:36:59.770 --> 00:37:01.210
<v Speaker 0>It's like open that door. Are you gonna

00:37:01.210 --> 00:37:02.490
<v Speaker 0>open that door? You gonna just keep it

00:37:02.490 --> 00:37:03.610
<v Speaker 0>shut? I'm not sure.

00:37:04.250 --> 00:37:06.650
<v Speaker 1>Okay. So the big question here is GitOps.

00:37:07.130 --> 00:37:09.770
<v Speaker 1>I mean, but I will put GitOps aside

00:37:09.770 --> 00:37:10.650
<v Speaker 1>for a second. Okay?

00:37:12.755 --> 00:37:15.715
<v Speaker 1>We're open to generating these HPA like, to

00:37:15.715 --> 00:37:18.995
<v Speaker 1>generating HPA or to generating a KDA config.

00:37:19.875 --> 00:37:20.835
<v Speaker 1>We're

00:37:20.835 --> 00:37:22.115
<v Speaker 1>open to creating,

00:37:23.315 --> 00:37:25.315
<v Speaker 1>like, the what you need in order to

00:37:25.315 --> 00:37:27.900
<v Speaker 1>scale. What we don't wanna do is we

00:37:27.900 --> 00:37:29.500
<v Speaker 1>don't wanna be the one who's, like, constantly

00:37:29.500 --> 00:37:31.580
<v Speaker 1>watching that pod and then scaling it. We

00:37:31.580 --> 00:37:33.420
<v Speaker 1>wanna reuse what's already there in KTH or

00:37:33.420 --> 00:37:34.220
<v Speaker 1>HBI.

00:37:35.660 --> 00:37:36.380
<v Speaker 0>Okay. Cool.

00:37:38.964 --> 00:37:40.405
<v Speaker 0>I think that was closer to a yes

00:37:40.405 --> 00:37:41.845
<v Speaker 0>than a no, but I'll leave that up

00:37:41.845 --> 00:37:44.005
<v Speaker 0>to interpretation of anyone watching. So

00:37:45.045 --> 00:37:46.805
<v Speaker 0>I Did you sorry. On you go.

00:37:47.365 --> 00:37:49.204
<v Speaker 1>No. I there's is there some lag here,

00:37:49.204 --> 00:37:50.565
<v Speaker 1>or is it just on on my end?

00:37:50.565 --> 00:37:52.859
<v Speaker 1>I'm curious. No. I've noticed a little bit

00:37:52.859 --> 00:37:54.700
<v Speaker 0>of lag too. I don't know if it's

00:37:54.700 --> 00:37:56.619
<v Speaker 0>just the Internet. I don't know if it's

00:37:56.619 --> 00:37:59.099
<v Speaker 0>the streaming software. I don't know if it's

00:37:59.099 --> 00:38:01.580
<v Speaker 0>the weather. Who knows? But so far,

00:38:01.900 --> 00:38:03.420
<v Speaker 0>I don't think we've dropped anything, so I

00:38:03.420 --> 00:38:03.900
<v Speaker 0>think we're okay.

00:38:04.575 --> 00:38:06.335
<v Speaker 0>You said you were possibly gonna show something

00:38:06.335 --> 00:38:08.095
<v Speaker 0>else. Would you like to do that now?

00:38:09.135 --> 00:38:11.775
<v Speaker 1>I mean, so we're also sending this data

00:38:11.775 --> 00:38:14.335
<v Speaker 1>to the robust SaaS platform. And,

00:38:15.055 --> 00:38:16.255
<v Speaker 1>I mean, I just wanna show a few

00:38:16.255 --> 00:38:17.935
<v Speaker 1>other ways that you can use this actually.

00:38:21.490 --> 00:38:23.970
<v Speaker 1>So I wanna start with stack reports.

00:38:32.195 --> 00:38:33.875
<v Speaker 0>Are you sharing your screen? Sorry.

00:38:34.115 --> 00:38:36.195
<v Speaker 1>Yeah. I'm about to start sharing it. Alright.

00:38:36.195 --> 00:38:36.755
<v Speaker 0>Okay.

00:38:43.235 --> 00:38:44.755
<v Speaker 1>So I wonder if the DAG might be

00:38:44.755 --> 00:38:46.915
<v Speaker 1>on my end because my browser is freezing

00:38:46.915 --> 00:38:47.155
<v Speaker 1>up.

00:38:47.940 --> 00:38:49.300
<v Speaker 1>But Let me see.

00:39:03.025 --> 00:39:05.425
<v Speaker 1>One moment. I'm just gonna pull this up.

00:39:05.585 --> 00:39:06.385
<v Speaker 0>Yeah. Of course.

00:39:28.665 --> 00:39:30.185
<v Speaker 1>Okay. So I wanna start,

00:39:30.425 --> 00:39:31.145
<v Speaker 1>with

00:39:31.385 --> 00:39:33.385
<v Speaker 1>another use case that's come up that we've

00:39:33.385 --> 00:39:34.585
<v Speaker 1>heard from a lot of people,

00:39:34.905 --> 00:39:35.545
<v Speaker 1>which is

00:39:36.105 --> 00:39:37.785
<v Speaker 1>okay. I ran KRR,

00:39:37.785 --> 00:39:39.705
<v Speaker 1>and I, like, I got a report once.

00:39:39.945 --> 00:39:41.625
<v Speaker 1>But what happens if two weeks from now,

00:39:41.625 --> 00:39:42.425
<v Speaker 1>some new applications,

00:39:42.849 --> 00:39:43.410
<v Speaker 1>like,

00:39:43.890 --> 00:39:45.730
<v Speaker 1>is deployed to the cluster or something goes

00:39:45.730 --> 00:39:47.650
<v Speaker 1>way off, two weeks from now and something

00:39:47.650 --> 00:39:49.250
<v Speaker 1>changes in one of my applications?

00:39:51.250 --> 00:39:53.329
<v Speaker 1>So what we've actually done is we've done

00:39:53.329 --> 00:39:55.089
<v Speaker 1>an integration with our other open source software

00:39:55.089 --> 00:39:57.010
<v Speaker 1>for robusta. It's, like you said, already running

00:39:57.010 --> 00:39:57.490
<v Speaker 1>cluster.

00:39:57.895 --> 00:40:00.215
<v Speaker 1>So we can then do a scan on

00:40:00.215 --> 00:40:02.695
<v Speaker 1>on a specific schedule that you set, like,

00:40:02.695 --> 00:40:05.175
<v Speaker 1>let's say, a week. And then we generate

00:40:05.175 --> 00:40:07.415
<v Speaker 1>a PDF report for that, and we just

00:40:07.415 --> 00:40:08.535
<v Speaker 1>send it over to you in Slack.

00:40:18.790 --> 00:40:20.070
<v Speaker 1>Can you hear me, David?

00:40:21.030 --> 00:40:22.550
<v Speaker 0>Yeah. I think you may be dropped off

00:40:22.550 --> 00:40:23.990
<v Speaker 0>for a second there now, but you're back.

00:40:23.990 --> 00:40:26.645
<v Speaker 0>Okay. So you generate a PDF report and

00:40:26.645 --> 00:40:27.605
<v Speaker 0>you send over?

00:40:28.885 --> 00:40:30.965
<v Speaker 1>Let me let me refresh my screen for

00:40:30.965 --> 00:40:32.405
<v Speaker 1>one second. I might drop out of the

00:40:32.405 --> 00:40:34.005
<v Speaker 1>call, but I think that'll fix the

00:40:34.244 --> 00:40:35.845
<v Speaker 1>issue. Is that okay? Of course.

00:40:42.760 --> 00:40:44.520
<v Speaker 1>Okay. Welcome back.

00:40:45.400 --> 00:40:47.640
<v Speaker 1>Yeah. Okay. Am I back in the livestream?

00:40:47.640 --> 00:40:48.680
<v Speaker 0>You are indeed.

00:40:49.160 --> 00:40:50.600
<v Speaker 1>Okay. So let's try that again.

00:40:51.915 --> 00:40:53.835
<v Speaker 1>So, yeah, what I was saying is, one

00:40:53.835 --> 00:40:55.675
<v Speaker 1>thing that we want you to do is

00:40:55.755 --> 00:40:57.995
<v Speaker 1>we wanna help you put this on autopilot

00:40:57.995 --> 00:40:59.835
<v Speaker 1>and then kind of forget about it until

00:40:59.835 --> 00:41:01.835
<v Speaker 1>there's something else that requires your attention.

00:41:02.155 --> 00:41:04.555
<v Speaker 1>So one thing that you can do that

00:41:04.555 --> 00:41:05.995
<v Speaker 1>I don't think we're highlighting enough in the

00:41:05.995 --> 00:41:08.440
<v Speaker 1>main repo yet is you can configure a

00:41:08.440 --> 00:41:10.120
<v Speaker 1>scan that runs periodically,

00:41:10.520 --> 00:41:11.720
<v Speaker 1>like, once a week.

00:41:12.200 --> 00:41:12.760
<v Speaker 1>And

00:41:13.320 --> 00:41:14.760
<v Speaker 1>when that happens, then you just get a

00:41:14.760 --> 00:41:17.320
<v Speaker 1>report here in stock, and you can go

00:41:17.320 --> 00:41:19.000
<v Speaker 1>over that and and then see any new

00:41:19.000 --> 00:41:19.480
<v Speaker 1>recommendations.

00:41:21.075 --> 00:41:24.515
<v Speaker 0>Oh, nice. Okay. So Robusta and Cluster does

00:41:24.515 --> 00:41:26.515
<v Speaker 0>the KRR scan for you, and then you

00:41:26.515 --> 00:41:27.235
<v Speaker 0>can get

00:41:27.715 --> 00:41:30.755
<v Speaker 0>a nice little report whenever you want, showing

00:41:30.755 --> 00:41:33.089
<v Speaker 0>you how good or how bad your current

00:41:33.089 --> 00:41:35.410
<v Speaker 0>setup is. Right? Exactly.

00:41:35.410 --> 00:41:37.410
<v Speaker 1>So we're sending that to two different places.

00:41:37.410 --> 00:41:38.930
<v Speaker 1>The first is to stack for people who

00:41:38.930 --> 00:41:40.690
<v Speaker 1>are just using the open source side. So

00:41:40.690 --> 00:41:41.890
<v Speaker 1>we wanna make sure they have a way

00:41:41.890 --> 00:41:44.050
<v Speaker 1>to get periodic weekly recommendations.

00:41:44.529 --> 00:41:46.655
<v Speaker 1>And then the other thing we're doing is

00:41:46.655 --> 00:41:49.295
<v Speaker 1>for people who are using, the UI,

00:41:49.375 --> 00:41:51.135
<v Speaker 1>which is part of our SaaS offering,

00:41:51.214 --> 00:41:52.975
<v Speaker 1>then we're also sending stuff,

00:41:53.455 --> 00:41:55.295
<v Speaker 1>over here. And here, you can see the

00:41:55.295 --> 00:41:56.495
<v Speaker 1>same recommendations,

00:41:56.895 --> 00:41:58.255
<v Speaker 1>in the SaaS platform.

00:41:58.895 --> 00:41:59.615
<v Speaker 0>Oh, cool.

00:42:01.550 --> 00:42:02.350
<v Speaker 0>Nice.

00:42:02.830 --> 00:42:04.750
<v Speaker 0>And there's value add to the SaaS platform

00:42:04.750 --> 00:42:05.710
<v Speaker 0>now then.

00:42:06.910 --> 00:42:07.550
<v Speaker 1>Yep.

00:42:08.030 --> 00:42:08.590
<v Speaker 1>Mean,

00:42:09.150 --> 00:42:11.150
<v Speaker 1>I think, like, philosophically,

00:42:11.150 --> 00:42:12.590
<v Speaker 1>then we wanna make sure that we're always

00:42:12.590 --> 00:42:14.270
<v Speaker 1>balancing the two things. Like, on the one

00:42:14.270 --> 00:42:16.635
<v Speaker 1>hand, adding functionality to the SaaS, but on

00:42:16.635 --> 00:42:19.515
<v Speaker 1>the other hand, making sure that people also

00:42:19.515 --> 00:42:21.355
<v Speaker 1>have a workaround and they can still get

00:42:21.355 --> 00:42:22.715
<v Speaker 1>this in a pure open source way if

00:42:22.715 --> 00:42:23.355
<v Speaker 1>they want.

00:42:24.955 --> 00:42:26.955
<v Speaker 0>Yeah. And I think it gives people the

00:42:26.955 --> 00:42:29.355
<v Speaker 0>ability to kinda onboard themselves to this flow

00:42:29.490 --> 00:42:31.170
<v Speaker 0>at their own pace. Like, if you just

00:42:31.170 --> 00:42:32.930
<v Speaker 0>want to run KRR once a month from

00:42:32.930 --> 00:42:34.130
<v Speaker 0>a developer's laptop,

00:42:35.170 --> 00:42:37.170
<v Speaker 0>sure. Right? But once you get a bit

00:42:37.170 --> 00:42:38.850
<v Speaker 0>more, I don't know, confidence

00:42:38.930 --> 00:42:40.930
<v Speaker 0>and you wanna start pushing that more through

00:42:40.930 --> 00:42:41.890
<v Speaker 0>more automation,

00:42:42.050 --> 00:42:43.985
<v Speaker 0>you get the report. And then, you know,

00:42:43.985 --> 00:42:45.265
<v Speaker 0>I'm sure we'll be chatting in six months

00:42:45.265 --> 00:42:47.185
<v Speaker 0>time when robust is just hooking into Kira

00:42:47.185 --> 00:42:49.505
<v Speaker 0>and doing other cool stuff too. So looking

00:42:49.505 --> 00:42:51.025
<v Speaker 0>forward to seeing that in the future.

00:42:51.505 --> 00:42:53.505
<v Speaker 1>I wish I wish we were there already,

00:42:53.505 --> 00:42:55.745
<v Speaker 1>but it's gonna take some time. But I

00:42:57.309 --> 00:42:59.150
<v Speaker 1>it's one of the things that most excites

00:42:59.150 --> 00:43:01.390
<v Speaker 1>me being able to, like, even just speak

00:43:01.390 --> 00:43:03.710
<v Speaker 1>to people at conference and just having a

00:43:03.710 --> 00:43:06.030
<v Speaker 1>casual conversation. Someone says to me, like, okay.

00:43:06.109 --> 00:43:06.670
<v Speaker 1>Well,

00:43:07.230 --> 00:43:08.829
<v Speaker 1>how should I set like, how should I

00:43:08.829 --> 00:43:11.145
<v Speaker 1>scale this microservice that's new here? And just

00:43:11.145 --> 00:43:14.025
<v Speaker 1>say, like, run one command. You're done. It'll

00:43:14.025 --> 00:43:15.785
<v Speaker 1>check, like, five different strategies. It'll tell you

00:43:15.785 --> 00:43:18.185
<v Speaker 1>what's most optimal. It'll give you the menu.

00:43:18.505 --> 00:43:19.945
<v Speaker 1>You can look it over and choose, or

00:43:19.945 --> 00:43:20.985
<v Speaker 1>you can take the first one if you

00:43:20.985 --> 00:43:21.865
<v Speaker 1>don't wanna think about it.

00:43:23.319 --> 00:43:25.240
<v Speaker 1>I wanna be able to say to people,

00:43:25.559 --> 00:43:28.440
<v Speaker 1>like, you're now speaking as someone who's, like,

00:43:28.440 --> 00:43:30.520
<v Speaker 1>an advocate for Kubernetes, I wanna be able

00:43:30.520 --> 00:43:32.359
<v Speaker 1>to say to people, like,

00:43:32.760 --> 00:43:35.240
<v Speaker 1>scaling is a no brainer. Like, there's you

00:43:35.240 --> 00:43:36.359
<v Speaker 1>don't even need to think about it. Like,

00:43:36.359 --> 00:43:38.615
<v Speaker 1>you wanna configure all the scanning? Just run

00:43:38.615 --> 00:43:39.974
<v Speaker 1>one command. Don't think about it. You wanna

00:43:39.974 --> 00:43:42.454
<v Speaker 1>configure KDA? Just run one command. Don't think

00:43:42.454 --> 00:43:43.175
<v Speaker 1>about it.

00:43:44.775 --> 00:43:46.295
<v Speaker 0>Nice. Awesome.

00:43:46.615 --> 00:43:47.255
<v Speaker 0>So

00:43:47.575 --> 00:43:49.415
<v Speaker 0>I've finished with one last question.

00:43:50.855 --> 00:43:51.335
<v Speaker 0>Right now,

00:43:52.110 --> 00:43:54.590
<v Speaker 0>we then actually dive into, like well, we

00:43:54.590 --> 00:43:57.230
<v Speaker 0>did. Right? It's it's looking into the metrics

00:43:57.230 --> 00:43:59.230
<v Speaker 0>of the I guess, this is from metric

00:43:59.230 --> 00:44:01.150
<v Speaker 0>server from the Kuplet where it's looking at

00:44:01.150 --> 00:44:03.470
<v Speaker 0>CPU utilization and the memory utilization.

00:44:04.085 --> 00:44:06.644
<v Speaker 0>Is there a future where it tries to

00:44:06.644 --> 00:44:09.845
<v Speaker 0>go beyond those metrics and actually, like, take

00:44:09.924 --> 00:44:10.885
<v Speaker 0>a look at the

00:44:11.204 --> 00:44:13.605
<v Speaker 0>it should be latency to understand, well, actually,

00:44:13.605 --> 00:44:15.605
<v Speaker 0>you're only using this much CPU,

00:44:15.605 --> 00:44:17.380
<v Speaker 0>but if maybe that's because there's other things

00:44:17.380 --> 00:44:19.380
<v Speaker 0>in place, maybe the CPU limits or whatever

00:44:19.380 --> 00:44:20.500
<v Speaker 0>or contention,

00:44:20.740 --> 00:44:22.260
<v Speaker 0>but your latency is quite high. And it's

00:44:22.260 --> 00:44:23.860
<v Speaker 0>like, well, okay. Well, you could probably bring

00:44:23.860 --> 00:44:26.900
<v Speaker 0>this latency down by allocating more CPU or

00:44:27.060 --> 00:44:28.900
<v Speaker 0>maybe even it's running on a different type

00:44:28.900 --> 00:44:30.900
<v Speaker 0>of hardware. Like, maybe how maybe there's cloud

00:44:30.900 --> 00:44:33.175
<v Speaker 0>awareness that comes into KRR that says you're

00:44:33.175 --> 00:44:34.935
<v Speaker 0>running on, like, a XS

00:44:34.935 --> 00:44:37.895
<v Speaker 0>tiny box on Amazon with, a one megabit

00:44:37.895 --> 00:44:39.735
<v Speaker 0>network card. Maybe

00:44:39.974 --> 00:44:41.655
<v Speaker 0>you need to start looking at bigger ones.

00:44:41.655 --> 00:44:43.415
<v Speaker 0>Maybe you need gigabit A four nets. Maybe

00:44:43.415 --> 00:44:44.695
<v Speaker 0>you need multiple A four nets because you've

00:44:44.695 --> 00:44:46.214
<v Speaker 0>got a lot of contention on a single

00:44:46.214 --> 00:44:48.240
<v Speaker 0>box. Maybe you need to switch to ARM

00:44:48.240 --> 00:44:49.760
<v Speaker 0>process. I don't know. Like, there's a whole

00:44:49.760 --> 00:44:51.920
<v Speaker 0>wealth of stuff out there. Like, scheduling isn't

00:44:51.920 --> 00:44:53.600
<v Speaker 0>just taking the hardware that we have. It's

00:44:53.600 --> 00:44:54.000
<v Speaker 0>capacity

00:44:54.640 --> 00:44:56.160
<v Speaker 0>planning and looking at how do we improve

00:44:56.160 --> 00:44:58.240
<v Speaker 0>that. And I wonder, maybe it's not in

00:44:58.240 --> 00:44:59.520
<v Speaker 0>scope now, but maybe it's something in the

00:44:59.520 --> 00:45:01.665
<v Speaker 0>future that could be a value proposition for

00:45:01.665 --> 00:45:03.185
<v Speaker 0>KRR and robusta too.

00:45:03.505 --> 00:45:05.185
<v Speaker 1>So you're the second person in two days

00:45:05.185 --> 00:45:06.545
<v Speaker 1>to say that to me about each of

00:45:06.545 --> 00:45:07.585
<v Speaker 1>the group agency.

00:45:07.825 --> 00:45:09.665
<v Speaker 1>So I think there is something to it.

00:45:09.825 --> 00:45:10.625
<v Speaker 1>It's

00:45:11.265 --> 00:45:12.945
<v Speaker 1>not I think in the first drop, we're

00:45:12.945 --> 00:45:14.225
<v Speaker 1>gonna be looking at

00:45:14.940 --> 00:45:16.460
<v Speaker 1>like, if if I think of how we're

00:45:16.460 --> 00:45:18.940
<v Speaker 1>building this, then, okay, we have base recommendations

00:45:18.940 --> 00:45:19.500
<v Speaker 1>today.

00:45:19.900 --> 00:45:21.339
<v Speaker 1>The next thing to do is to get

00:45:21.339 --> 00:45:23.740
<v Speaker 1>better explainability about why we're giving recommendations. We

00:45:23.740 --> 00:45:25.500
<v Speaker 1>haven't done. It's, like, really gonna land any

00:45:25.500 --> 00:45:26.300
<v Speaker 1>day now.

00:45:26.859 --> 00:45:28.060
<v Speaker 1>So you can see that graph. You can

00:45:28.060 --> 00:45:30.465
<v Speaker 1>understand why we're recommending what we're recommending and

00:45:30.465 --> 00:45:32.385
<v Speaker 1>play with the thresholds if you want to.

00:45:32.785 --> 00:45:35.025
<v Speaker 1>And then the next thing, like, the next

00:45:35.025 --> 00:45:36.625
<v Speaker 1>level is to be able to say, okay.

00:45:36.625 --> 00:45:38.705
<v Speaker 1>I'm not just gonna recommend this output

00:45:39.025 --> 00:45:39.825
<v Speaker 1>taking,

00:45:41.265 --> 00:45:41.985
<v Speaker 1>like,

00:45:42.145 --> 00:45:43.745
<v Speaker 1>a set a fixed request or a fixed

00:45:43.745 --> 00:45:45.105
<v Speaker 1>limit. I'm gonna be able to recommend

00:45:46.490 --> 00:45:48.250
<v Speaker 1>HPA config. I'm gonna need to recommend the

00:45:48.250 --> 00:45:50.730
<v Speaker 1>KDA config and so on. And then for

00:45:50.730 --> 00:45:52.650
<v Speaker 1>me, the dev will come like, one beyond

00:45:52.650 --> 00:45:54.569
<v Speaker 1>that is then to be able to say,

00:45:54.569 --> 00:45:56.250
<v Speaker 1>okay. Now I'm gonna look at when I

00:45:56.250 --> 00:45:58.170
<v Speaker 1>optimize this and when I analyze it, I'm

00:45:58.170 --> 00:45:59.770
<v Speaker 1>not just gonna look at CPU and memory.

00:45:59.770 --> 00:46:00.730
<v Speaker 1>I'm going to be able to take a

00:46:00.730 --> 00:46:02.415
<v Speaker 1>look at this bigger or this picture.

00:46:03.055 --> 00:46:05.295
<v Speaker 1>And then that's one side of it. And

00:46:05.295 --> 00:46:06.975
<v Speaker 1>then the other thing that you touched on

00:46:06.975 --> 00:46:10.015
<v Speaker 1>is there's this fundamental relationship between your nodes

00:46:10.015 --> 00:46:11.775
<v Speaker 1>and your cluster autoscaler

00:46:11.775 --> 00:46:12.735
<v Speaker 1>in between

00:46:12.975 --> 00:46:14.495
<v Speaker 1>your pods. And

00:46:14.890 --> 00:46:16.810
<v Speaker 1>the good example of that is, like, let's

00:46:16.810 --> 00:46:18.250
<v Speaker 1>say we give you a recommendation

00:46:18.490 --> 00:46:19.930
<v Speaker 1>that you

00:46:20.250 --> 00:46:23.130
<v Speaker 1>should set, I don't know, like, one CPU

00:46:23.130 --> 00:46:24.410
<v Speaker 1>and one gigabyte,

00:46:24.650 --> 00:46:26.890
<v Speaker 1>but all your nodes have 10 CPUs and

00:46:26.890 --> 00:46:27.690
<v Speaker 1>20 gigabytes.

00:46:28.434 --> 00:46:30.434
<v Speaker 1>Then based on the ratios, you will always

00:46:30.434 --> 00:46:32.195
<v Speaker 1>have wasted memory on your nodes.

00:46:32.994 --> 00:46:36.115
<v Speaker 1>So there's a relationship here between scaling your

00:46:36.115 --> 00:46:38.595
<v Speaker 1>pods and, like, setting up request and limits

00:46:38.595 --> 00:46:41.875
<v Speaker 1>for your pods and between handling

00:46:41.430 --> 00:46:43.510
<v Speaker 1>the actual nodes in the cluster autoscaler side.

00:46:43.510 --> 00:46:45.030
<v Speaker 1>So that is something we're looking into as

00:46:45.030 --> 00:46:45.510
<v Speaker 1>well.

00:46:46.470 --> 00:46:49.510
<v Speaker 0>Ah, so, yeah, identifying those parts of memory

00:46:49.510 --> 00:46:51.829
<v Speaker 0>that can't be allocated because the CPU is

00:46:51.829 --> 00:46:53.030
<v Speaker 0>is utilized.

00:46:53.030 --> 00:46:54.470
<v Speaker 0>Right? Or not even utilized, just that it's

00:46:54.470 --> 00:46:56.675
<v Speaker 0>been allocated by the scheduler. So it's never

00:46:56.675 --> 00:46:56.915
<v Speaker 0>gonna

00:46:57.875 --> 00:46:59.235
<v Speaker 0>that RAM's sitting there unless you can get

00:46:59.235 --> 00:47:01.475
<v Speaker 0>a workload that actually takes over. Yeah. That's

00:47:01.475 --> 00:47:03.075
<v Speaker 0>a nice idea. I like that a lot.

00:47:03.395 --> 00:47:05.075
<v Speaker 1>But you have to freeze stuff. So you're

00:47:05.075 --> 00:47:07.075
<v Speaker 1>looking at this, like, 10 dimensional problem,

00:47:07.410 --> 00:47:09.170
<v Speaker 1>and you can't run on all the dimensions

00:47:09.170 --> 00:47:11.089
<v Speaker 1>at once. You have to, like, freeze, like,

00:47:11.089 --> 00:47:12.770
<v Speaker 1>nine dimensions and say, okay. Right now, we're

00:47:12.770 --> 00:47:15.089
<v Speaker 1>just optimizing for CPU and memory. And then

00:47:15.089 --> 00:47:16.770
<v Speaker 1>as time goes on, as you build something

00:47:16.770 --> 00:47:18.050
<v Speaker 1>that's more and more complex,

00:47:18.130 --> 00:47:19.569
<v Speaker 1>then you start adding more, like,

00:47:20.535 --> 00:47:22.615
<v Speaker 1>you start adding, like, more

00:47:22.615 --> 00:47:24.615
<v Speaker 1>degrees of control and, like, unfreezing some of

00:47:24.615 --> 00:47:25.494
<v Speaker 1>those dimensions.

00:47:26.295 --> 00:47:27.974
<v Speaker 0>Yeah. Well, we have a question about one

00:47:27.974 --> 00:47:29.895
<v Speaker 0>of those other dimensions, which is important for

00:47:29.895 --> 00:47:31.895
<v Speaker 0>our stateful workloads on Kubernetes in the chat.

00:47:32.370 --> 00:47:35.330
<v Speaker 0>Russell is asking about tooling that surfaces the

00:47:35.330 --> 00:47:38.690
<v Speaker 0>availability of IOPS within container workloads on nodes.

00:47:38.850 --> 00:47:40.850
<v Speaker 0>I guess that could be another dimension

00:47:40.850 --> 00:47:42.530
<v Speaker 0>that KRR could hook into.

00:47:43.810 --> 00:47:44.370
<v Speaker 1>Yeah.

00:47:44.610 --> 00:47:44.930
<v Speaker 1>Me.

00:47:46.845 --> 00:47:49.405
<v Speaker 1>I hit a nasty issue with this once

00:47:50.045 --> 00:47:52.125
<v Speaker 1>where I had a Kubernetes node that was

00:47:52.125 --> 00:47:52.765
<v Speaker 1>going

00:47:53.245 --> 00:47:55.405
<v Speaker 1>missing an action. It just would suddenly freeze

00:47:55.405 --> 00:47:56.605
<v Speaker 1>up on AWS.

00:47:56.765 --> 00:47:58.205
<v Speaker 1>And when you looked at it, it was

00:47:58.205 --> 00:47:59.965
<v Speaker 1>because it was, like, an AWS node with,

00:47:59.965 --> 00:48:00.765
<v Speaker 1>like, a burstable

00:48:01.160 --> 00:48:03.080
<v Speaker 1>number of iApps, you used up the burst

00:48:03.080 --> 00:48:04.600
<v Speaker 1>quote on the iApps, and the whole node

00:48:04.600 --> 00:48:05.400
<v Speaker 1>froze up.

00:48:06.120 --> 00:48:06.760
<v Speaker 1>So

00:48:07.160 --> 00:48:08.440
<v Speaker 1>I don't I mean, you could get that

00:48:08.440 --> 00:48:09.880
<v Speaker 1>in a graph on the dashboard.

00:48:10.040 --> 00:48:10.680
<v Speaker 1>Think

00:48:11.080 --> 00:48:13.080
<v Speaker 1>the team is adding that to robusta as

00:48:13.080 --> 00:48:14.600
<v Speaker 1>well in the dashboards we have.

00:48:15.160 --> 00:48:16.360
<v Speaker 1>You can

00:48:16.825 --> 00:48:17.945
<v Speaker 1>can you get it

00:48:19.465 --> 00:48:20.745
<v Speaker 1>can you get it you can't get it

00:48:20.745 --> 00:48:22.185
<v Speaker 1>in KubeCat on top. Right? I don't think

00:48:22.185 --> 00:48:23.385
<v Speaker 1>it shows you IOPs.

00:48:23.785 --> 00:48:25.865
<v Speaker 0>No. It doesn't. You would have to

00:48:26.585 --> 00:48:28.265
<v Speaker 0>yeah. It wouldn't it's not it's not surface

00:48:28.265 --> 00:48:30.185
<v Speaker 0>to the metrics server on the KubeLit. So

00:48:30.185 --> 00:48:31.225
<v Speaker 0>I don't think, yeah, you're not gonna have

00:48:31.225 --> 00:48:31.785
<v Speaker 0>access to them.

00:48:32.600 --> 00:48:33.880
<v Speaker 1>And you can get it probably with the

00:48:33.880 --> 00:48:35.960
<v Speaker 1>e b b f's based stuff too. Right?

00:48:35.960 --> 00:48:37.480
<v Speaker 1>Like, with me.

00:48:38.040 --> 00:48:40.280
<v Speaker 0>Yeah. I mean, I I think from the

00:48:40.280 --> 00:48:42.200
<v Speaker 0>Kubelet metrics, you may get, like, the flush

00:48:42.200 --> 00:48:44.120
<v Speaker 0>time and FSN rules.

00:48:44.280 --> 00:48:46.360
<v Speaker 0>I I don't know if you're specifically gonna

00:48:46.005 --> 00:48:47.605
<v Speaker 0>get the reason right. It's been a while

00:48:47.605 --> 00:48:49.204
<v Speaker 0>since I've dug into that, to be honest.

00:48:49.204 --> 00:48:51.525
<v Speaker 0>So I'm not entirely sure. There may be

00:48:51.525 --> 00:48:52.724
<v Speaker 0>stuff there, but it may be that you

00:48:52.724 --> 00:48:55.045
<v Speaker 0>have to instrument that another way or

00:48:55.684 --> 00:48:57.924
<v Speaker 0>use a node exporter from Prometheus to grab

00:48:57.924 --> 00:48:59.125
<v Speaker 0>extra metrics there.

00:48:59.605 --> 00:48:59.924
<v Speaker 1>Yep.

00:49:01.539 --> 00:49:03.539
<v Speaker 0>Alright. Awesome. Well, thank you so much for

00:49:03.539 --> 00:49:05.059
<v Speaker 0>your time today. Is there anything else you

00:49:05.059 --> 00:49:06.740
<v Speaker 0>want to cover before we finish up?

00:49:07.700 --> 00:49:10.020
<v Speaker 1>Just a request from everyone. Like, this is

00:49:10.020 --> 00:49:12.900
<v Speaker 1>new. It's still in beta from our perspective.

00:49:12.900 --> 00:49:14.900
<v Speaker 1>So we're testing this out. We're getting feedback

00:49:14.900 --> 00:49:17.765
<v Speaker 1>from people, and we're taking all that feedback

00:49:17.765 --> 00:49:19.285
<v Speaker 1>to try and make this better.

00:49:19.605 --> 00:49:21.285
<v Speaker 1>Like, just this in the past day or

00:49:21.285 --> 00:49:22.805
<v Speaker 1>so, got feedback from someone,

00:49:23.045 --> 00:49:24.885
<v Speaker 1>who's using this with HPA.

00:49:25.204 --> 00:49:27.045
<v Speaker 1>And about an edge case there where, like,

00:49:27.045 --> 00:49:28.645
<v Speaker 1>the HPA was trying to drive stuff in

00:49:28.645 --> 00:49:29.125
<v Speaker 1>one direction

00:49:29.730 --> 00:49:32.290
<v Speaker 1>and was trying to reach 50% utilization, but

00:49:32.290 --> 00:49:34.450
<v Speaker 1>the recommendations we were giving were trying to

00:49:34.450 --> 00:49:36.930
<v Speaker 1>reach, like, 90 the p 99. Right?

00:49:37.250 --> 00:49:37.890
<v Speaker 1>So

00:49:38.690 --> 00:49:40.050
<v Speaker 1>we're trying to

00:49:40.690 --> 00:49:42.930
<v Speaker 1>really make this, like, the best tool out

00:49:42.930 --> 00:49:44.690
<v Speaker 1>there for determining

00:49:44.275 --> 00:49:47.155
<v Speaker 1>request and limits on Kubernetes for maximizing utilization

00:49:47.155 --> 00:49:48.515
<v Speaker 1>and to do that in an open source

00:49:48.515 --> 00:49:50.595
<v Speaker 1>way. But to make it happen,

00:49:51.395 --> 00:49:53.075
<v Speaker 1>we need help from everyone out there. We

00:49:53.075 --> 00:49:55.075
<v Speaker 1>need feedback. Like, the biggest challenge

00:49:55.235 --> 00:49:56.994
<v Speaker 1>when you build an open source project is

00:49:56.994 --> 00:49:59.160
<v Speaker 1>that people take your project and they use

00:49:59.160 --> 00:50:00.839
<v Speaker 1>it, and they either love it or they

00:50:00.839 --> 00:50:01.880
<v Speaker 1>hate it and say, I'm never gonna use

00:50:01.880 --> 00:50:03.640
<v Speaker 1>this again, and they just never speak to

00:50:03.640 --> 00:50:05.000
<v Speaker 1>you. Whereas if you think of, like, a

00:50:05.000 --> 00:50:07.319
<v Speaker 1>company that's sales driven, then it never happens

00:50:07.319 --> 00:50:09.319
<v Speaker 1>because you're like, people have to jump on

00:50:09.319 --> 00:50:10.680
<v Speaker 1>calls with you and you see the feedback.

00:50:11.175 --> 00:50:13.095
<v Speaker 1>And if you give them bad advice, you

00:50:13.095 --> 00:50:14.935
<v Speaker 1>hear about it, you give them good advice,

00:50:14.935 --> 00:50:17.575
<v Speaker 1>you see they're happy. So I just wanna

00:50:17.575 --> 00:50:19.495
<v Speaker 1>request from everyone out there who's listening to

00:50:19.495 --> 00:50:19.975
<v Speaker 1>this,

00:50:20.295 --> 00:50:22.055
<v Speaker 1>please give us your feedback and open up

00:50:22.055 --> 00:50:24.455
<v Speaker 1>GitHub issues, jump on the Slack channels, speak

00:50:24.455 --> 00:50:26.850
<v Speaker 1>to us, message me on Twitter or LinkedIn.

00:50:26.930 --> 00:50:28.450
<v Speaker 1>We wanna hear from you how we can

00:50:28.450 --> 00:50:30.370
<v Speaker 1>make this work for you. And

00:50:30.770 --> 00:50:33.490
<v Speaker 1>I'd say the number one thing that I've

00:50:33.490 --> 00:50:36.130
<v Speaker 1>learned also as a founder is

00:50:36.530 --> 00:50:37.089
<v Speaker 1>when you

00:50:37.809 --> 00:50:40.035
<v Speaker 1>like, I can't count the number of times

00:50:40.035 --> 00:50:41.955
<v Speaker 1>that I've had someone say to me, you

00:50:41.955 --> 00:50:42.435
<v Speaker 1>know,

00:50:42.915 --> 00:50:44.835
<v Speaker 1>I guess there is some feedback I could

00:50:44.835 --> 00:50:46.835
<v Speaker 1>give, but I think it's only specific to

00:50:46.835 --> 00:50:48.435
<v Speaker 1>my company or it's just about how we

00:50:48.435 --> 00:50:50.595
<v Speaker 1>work. And then you double click on that,

00:50:50.595 --> 00:50:52.275
<v Speaker 1>and you discover it's something that applies to,

00:50:52.275 --> 00:50:53.970
<v Speaker 1>like, nine out of 10 people. So even

00:50:53.970 --> 00:50:55.490
<v Speaker 1>if it's specific to your company,

00:50:55.730 --> 00:50:57.890
<v Speaker 1>speak up and tell us because very often,

00:50:57.890 --> 00:50:58.930
<v Speaker 1>it's not just you.

00:51:00.450 --> 00:51:03.090
<v Speaker 0>Awesome. Alright, everyone. If you're listening,

00:51:03.330 --> 00:51:05.904
<v Speaker 0>go check out the repository on GitHub. Open

00:51:05.904 --> 00:51:07.664
<v Speaker 0>up issues with ideas,

00:51:07.664 --> 00:51:08.385
<v Speaker 0>bugs,

00:51:08.545 --> 00:51:09.984
<v Speaker 0>and make sure you give them as much

00:51:09.984 --> 00:51:11.665
<v Speaker 0>feedback as you possibly can.

00:51:13.345 --> 00:51:15.184
<v Speaker 0>Oh, we did get a last question sneak

00:51:15.184 --> 00:51:16.625
<v Speaker 0>in there right at the end.

00:51:17.105 --> 00:51:19.345
<v Speaker 1>Have answer one. I have to address it.

00:51:19.345 --> 00:51:20.865
<v Speaker 1>I I have to.

00:51:22.380 --> 00:51:25.020
<v Speaker 1>It's an excellent question. And

00:51:25.339 --> 00:51:27.180
<v Speaker 1>so let me give background on what Goldilocks

00:51:27.180 --> 00:51:29.660
<v Speaker 1>is for people who don't know. So Goldilocks

00:51:29.660 --> 00:51:30.619
<v Speaker 1>is

00:51:30.619 --> 00:51:32.540
<v Speaker 1>a platform on top of the VPA

00:51:32.619 --> 00:51:34.540
<v Speaker 1>that exposes the VPA recommendations

00:51:34.815 --> 00:51:36.175
<v Speaker 1>in the web UI,

00:51:36.255 --> 00:51:38.255
<v Speaker 1>and they also prevent you from having to

00:51:38.255 --> 00:51:40.895
<v Speaker 1>configure a VPA for each and every workload.

00:51:40.895 --> 00:51:42.335
<v Speaker 1>So they're doing two things. They're giving you

00:51:42.335 --> 00:51:44.415
<v Speaker 1>the VPA recommendations in the web UI,

00:51:44.734 --> 00:51:46.815
<v Speaker 1>and they're configuring the VPA for you so

00:51:46.815 --> 00:51:47.935
<v Speaker 1>that you don't have to configure it for

00:51:47.935 --> 00:51:49.775
<v Speaker 1>each and every deployment in your cluster.

00:51:50.619 --> 00:51:51.340
<v Speaker 1>So

00:51:52.940 --> 00:51:54.300
<v Speaker 1>I guess the difference

00:51:54.859 --> 00:51:55.980
<v Speaker 1>between the

00:51:56.300 --> 00:51:58.940
<v Speaker 1>between KRR and the and Goldilocks is the

00:51:58.940 --> 00:52:01.180
<v Speaker 1>difference between KRR and the VPA.

00:52:01.500 --> 00:52:02.460
<v Speaker 1>And

00:52:02.460 --> 00:52:04.380
<v Speaker 1>just to go over that again briefly,

00:52:04.780 --> 00:52:05.020
<v Speaker 1>so

00:52:06.974 --> 00:52:09.455
<v Speaker 1>we don't require installing anything in your cluster.

00:52:09.535 --> 00:52:10.175
<v Speaker 1>You

00:52:10.335 --> 00:52:12.575
<v Speaker 1>just run a command line. We take the

00:52:12.575 --> 00:52:14.974
<v Speaker 1>historical data, we can give you recommendations immediately.

00:52:15.454 --> 00:52:16.015
<v Speaker 1>And

00:52:16.974 --> 00:52:18.974
<v Speaker 1>in terms of what we're, like, trying to

00:52:18.974 --> 00:52:20.974
<v Speaker 1>accomplish, the the biggest thing that we're trying

00:52:20.974 --> 00:52:21.615
<v Speaker 1>to accomplish

00:52:21.960 --> 00:52:24.280
<v Speaker 1>is to, one, give explainability.

00:52:24.280 --> 00:52:26.040
<v Speaker 1>So not just to give you a recommendation,

00:52:26.119 --> 00:52:27.560
<v Speaker 1>but also to show you the data that

00:52:27.560 --> 00:52:29.480
<v Speaker 1>supports that recommendation and give you the confidence

00:52:29.480 --> 00:52:31.480
<v Speaker 1>to apply that. And that's something

00:52:31.800 --> 00:52:34.040
<v Speaker 1>that we're gonna like, just any moment really

00:52:34.040 --> 00:52:34.680
<v Speaker 1>something for.

00:52:35.775 --> 00:52:37.375
<v Speaker 1>And then the other thing that we're trying

00:52:37.375 --> 00:52:40.175
<v Speaker 1>to do is to give more complex recommendations

00:52:40.494 --> 00:52:43.055
<v Speaker 1>and, like, analyze this historical data, not just

00:52:43.055 --> 00:52:45.375
<v Speaker 1>output a single request or a single limit,

00:52:45.535 --> 00:52:46.975
<v Speaker 1>but to be able to output

00:52:47.295 --> 00:52:49.694
<v Speaker 1>something that's richer than that, like HPA,

00:52:50.160 --> 00:52:51.920
<v Speaker 1>like a Kata config and so on. And

00:52:51.920 --> 00:52:54.400
<v Speaker 1>then, of course, in terms of reporting features,

00:52:54.799 --> 00:52:55.840
<v Speaker 1>we're

00:52:56.480 --> 00:52:58.799
<v Speaker 1>very, like, focused not just on the recommendations,

00:52:58.799 --> 00:53:00.480
<v Speaker 1>but how you consume them as well, like

00:53:00.480 --> 00:53:01.760
<v Speaker 1>getting a weekly Slack report,

00:53:02.335 --> 00:53:04.335
<v Speaker 1>getting the table, like, showing what's the biggest

00:53:04.335 --> 00:53:06.175
<v Speaker 1>priority thing to look at and so on.

00:53:07.375 --> 00:53:08.815
<v Speaker 0>Nice. Awesome.

00:53:08.895 --> 00:53:11.135
<v Speaker 0>Hope that answers your question, Jason.

00:53:11.615 --> 00:53:14.175
<v Speaker 1>Like, Goldilocks is good. Like, Goldilocks if you're

00:53:14.175 --> 00:53:15.935
<v Speaker 1>using VBA, you should use Goldilocks too.

00:53:17.619 --> 00:53:18.260
<v Speaker 1>So

00:53:18.740 --> 00:53:19.460
<v Speaker 1>me.

00:53:20.020 --> 00:53:21.940
<v Speaker 0>It's not a project I'm familiar with, but

00:53:21.940 --> 00:53:24.820
<v Speaker 0>I'll something I'll look into after the stream

00:53:24.820 --> 00:53:25.700
<v Speaker 0>and take a look.

00:53:26.180 --> 00:53:26.820
<v Speaker 0>Alright.

00:53:27.060 --> 00:53:28.820
<v Speaker 0>Thank you again for your time. It's always

00:53:28.820 --> 00:53:30.740
<v Speaker 0>fun to see what new ideas you'll come

00:53:30.740 --> 00:53:32.420
<v Speaker 0>up with at robusta. Thank you for coming

00:53:32.420 --> 00:53:34.035
<v Speaker 0>on to this channel and sharing them with

00:53:34.035 --> 00:53:36.035
<v Speaker 0>people. I hope people do get involved open

00:53:36.035 --> 00:53:38.515
<v Speaker 0>issues and give you feedback, and hopefully, we'll

00:53:38.515 --> 00:53:40.435
<v Speaker 0>see each other against it. So thank you,

00:53:40.435 --> 00:53:42.035
<v Speaker 0>Nathan. Have a a wonderful day. And to

00:53:42.035 --> 00:53:43.955
<v Speaker 0>everyone that watched, we'll see you next time.

00:53:44.435 --> 00:53:44.755
<v Speaker 0>Bye.
