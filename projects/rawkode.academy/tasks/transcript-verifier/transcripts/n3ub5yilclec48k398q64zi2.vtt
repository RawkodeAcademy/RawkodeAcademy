WEBVTT

NOTE
Transcription provided by Deepgram
Request Id: aff6e1b7-f17b-4aa5-b733-67aa1b8d3f23
Created: 2025-04-29T14:57:10.087Z
Duration: 3121.5579
Channels: 1

00:00:05.040 --> 00:00:07.600
<v Speaker 0>Hello, and welcome back to the Rawkode Academy.

00:00:07.600 --> 00:00:09.920
<v Speaker 0>I'm your host, David Flanagan, although you probably

00:00:09.920 --> 00:00:12.400
<v Speaker 0>know me from across the Internet and this

00:00:12.400 --> 00:00:13.759
<v Speaker 0>channel as Rawkode.

00:00:15.265 --> 00:00:17.025
<v Speaker 0>Today is our first video in a new

00:00:17.025 --> 00:00:19.345
<v Speaker 0>series called Rawkode deep dives.

00:00:19.905 --> 00:00:22.545
<v Speaker 0>I wanna thank our sponsor, Commodore,

00:00:22.545 --> 00:00:24.625
<v Speaker 0>who have kindly sponsored my time to put

00:00:24.625 --> 00:00:26.065
<v Speaker 0>this first deep dive together.

00:00:27.220 --> 00:00:28.420
<v Speaker 0>Thank you, Commodore.

00:00:29.780 --> 00:00:31.860
<v Speaker 0>This deep dive is on Kubernetes

00:00:31.860 --> 00:00:32.820
<v Speaker 0>scheduling.

00:00:33.060 --> 00:00:34.820
<v Speaker 0>Now we will start off with a few

00:00:34.820 --> 00:00:36.820
<v Speaker 0>of the basics just to set the scene,

00:00:37.059 --> 00:00:39.220
<v Speaker 0>but we're gonna move pretty quickly into parts

00:00:39.220 --> 00:00:40.340
<v Speaker 0>of the scheduler

00:00:40.500 --> 00:00:42.020
<v Speaker 0>that you may not be familiar with.

00:00:43.274 --> 00:00:45.355
<v Speaker 0>These are parts of the scheduler that become

00:00:45.355 --> 00:00:48.235
<v Speaker 0>more important as your cluster size grows as

00:00:48.235 --> 00:00:50.875
<v Speaker 0>your cluster grows by number of pods and

00:00:50.875 --> 00:00:51.995
<v Speaker 0>number of nodes.

00:00:52.954 --> 00:00:54.394
<v Speaker 0>We've got a lot to cover today,

00:00:54.830 --> 00:00:57.230
<v Speaker 0>so let's dive right in. Okay.

00:00:58.590 --> 00:01:01.070
<v Speaker 0>So before we move on to the deep

00:01:01.070 --> 00:01:02.110
<v Speaker 0>dive content,

00:01:03.470 --> 00:01:04.909
<v Speaker 0>let's just understand

00:01:06.190 --> 00:01:07.790
<v Speaker 0>on the simplest level

00:01:08.765 --> 00:01:11.245
<v Speaker 0>what the scheduler is responsible for.

00:01:19.805 --> 00:01:22.525
<v Speaker 0>So here I have a Lima virtual machine

00:01:22.525 --> 00:01:23.645
<v Speaker 0>running on my machine,

00:01:25.340 --> 00:01:28.380
<v Speaker 0>which provisions a standard kubeadmin cluster.

00:01:29.259 --> 00:01:30.619
<v Speaker 0>Now kubeadmin

00:01:30.619 --> 00:01:31.420
<v Speaker 0>clusters

00:01:31.420 --> 00:01:33.899
<v Speaker 0>have static manifests that run the control plane

00:01:33.899 --> 00:01:34.700
<v Speaker 0>component.

00:01:37.020 --> 00:01:39.500
<v Speaker 0>Let's not delete it, but let's move

00:01:39.815 --> 00:01:41.975
<v Speaker 0>our scheduler to the temp directory.

00:01:42.535 --> 00:01:43.895
<v Speaker 0>And what's gonna happen,

00:01:45.495 --> 00:01:47.015
<v Speaker 0>probably already happened,

00:01:47.975 --> 00:01:49.095
<v Speaker 0>is this

00:01:49.175 --> 00:01:51.895
<v Speaker 0>matter matter pod for the scheduler has been

00:01:51.895 --> 00:01:52.295
<v Speaker 0>removed.

00:01:53.130 --> 00:01:54.570
<v Speaker 0>Now this just means

00:01:55.290 --> 00:01:56.970
<v Speaker 0>that if we run get pods,

00:01:57.530 --> 00:01:59.450
<v Speaker 0>we'll see the things that are running continue

00:01:59.450 --> 00:02:00.250
<v Speaker 0>to run.

00:02:01.370 --> 00:02:03.050
<v Speaker 0>But let's try and schedule

00:02:03.530 --> 00:02:04.330
<v Speaker 0>a new pod.

00:02:05.915 --> 00:02:08.634
<v Speaker 0>Here, I have the simplest pod spec that

00:02:08.634 --> 00:02:10.715
<v Speaker 0>we can deploy to our cluster,

00:02:11.595 --> 00:02:13.755
<v Speaker 0>and this is just applying NGINX and doesn't

00:02:13.755 --> 00:02:15.355
<v Speaker 0>specify anything else.

00:02:18.395 --> 00:02:19.595
<v Speaker 0>We can use Vim

00:02:22.710 --> 00:02:24.070
<v Speaker 0>to drop this in

00:02:24.710 --> 00:02:26.550
<v Speaker 0>and we can kubectl apply

00:02:29.430 --> 00:02:30.470
<v Speaker 0>for our cluster.

00:02:31.910 --> 00:02:33.270
<v Speaker 0>Now when we run get pods,

00:02:35.275 --> 00:02:36.955
<v Speaker 0>we'll see that our NGINX

00:02:37.194 --> 00:02:38.715
<v Speaker 0>is in a pending

00:02:38.715 --> 00:02:39.595
<v Speaker 0>status.

00:02:40.635 --> 00:02:42.474
<v Speaker 0>I'm not sure why Lima is running in

00:02:42.474 --> 00:02:44.555
<v Speaker 0>NGINX, but I'm gonna let it go.

00:02:45.995 --> 00:02:48.075
<v Speaker 0>Now because we removed

00:02:48.394 --> 00:02:49.275
<v Speaker 0>the scheduler,

00:02:50.250 --> 00:02:51.770
<v Speaker 0>this will never schedule.

00:02:53.370 --> 00:02:55.770
<v Speaker 0>In fact, we can do describe pod nginx,

00:02:56.490 --> 00:02:58.970
<v Speaker 0>and you don't really get any error messages.

00:02:59.690 --> 00:03:01.290
<v Speaker 0>But if we scroll to here or look

00:03:01.290 --> 00:03:03.530
<v Speaker 0>here, we'll see that the node is none,

00:03:04.205 --> 00:03:06.445
<v Speaker 0>which is why the status is pending.

00:03:07.325 --> 00:03:09.485
<v Speaker 0>Now let's look at our get pods again.

00:03:10.765 --> 00:03:12.685
<v Speaker 0>Let's bring back our scheduler

00:03:15.725 --> 00:03:17.485
<v Speaker 0>to the static manifest directory.

00:03:19.000 --> 00:03:21.240
<v Speaker 0>We'll run get pods on all namespaces.

00:03:21.799 --> 00:03:23.560
<v Speaker 0>We'll see our scheduler has now been running

00:03:23.560 --> 00:03:26.040
<v Speaker 0>for six seconds, and not only that, but

00:03:26.040 --> 00:03:27.799
<v Speaker 0>our NGINX pod was scheduled.

00:03:32.395 --> 00:03:34.475
<v Speaker 0>We run describe on it one more time,

00:03:35.194 --> 00:03:36.795
<v Speaker 0>we'll see we have a node

00:03:39.995 --> 00:03:41.595
<v Speaker 0>and the status is running.

00:03:44.795 --> 00:03:45.115
<v Speaker 0>So

00:03:47.410 --> 00:03:49.570
<v Speaker 0>in a naive way, we can say the

00:03:49.570 --> 00:03:52.290
<v Speaker 0>scheduler's one job in this entire world

00:03:52.610 --> 00:03:55.170
<v Speaker 0>is to populate pod specs with a node

00:03:55.170 --> 00:03:55.810
<v Speaker 0>name.

00:03:56.690 --> 00:03:57.730
<v Speaker 0>That is all.

00:03:58.690 --> 00:04:00.930
<v Speaker 0>However, there is a lot of complexity

00:04:01.365 --> 00:04:03.685
<v Speaker 0>in how the scheduler determines

00:04:03.765 --> 00:04:05.445
<v Speaker 0>what that node name should be.

00:04:06.405 --> 00:04:08.965
<v Speaker 0>That is what we're gonna dive into today.

00:04:09.605 --> 00:04:11.205
<v Speaker 0>So the first thing we're going to address

00:04:11.205 --> 00:04:12.965
<v Speaker 0>is bypassing the scheduler.

00:04:13.540 --> 00:04:14.100
<v Speaker 0>Why?

00:04:14.820 --> 00:04:15.540
<v Speaker 0>Well,

00:04:15.620 --> 00:04:17.620
<v Speaker 0>sometimes you just gotta do what you just

00:04:17.620 --> 00:04:18.420
<v Speaker 0>gotta do.

00:04:19.380 --> 00:04:22.100
<v Speaker 0>We're now looking at a real Kubernetes cluster.

00:04:22.100 --> 00:04:24.180
<v Speaker 0>This time it's hosted on Google Cloud with

00:04:24.180 --> 00:04:25.060
<v Speaker 0>JKE.

00:04:26.020 --> 00:04:28.020
<v Speaker 0>I'm gonna copy one of these node names,

00:04:28.755 --> 00:04:30.995
<v Speaker 0>this time being x 42

00:04:30.995 --> 00:04:33.235
<v Speaker 0>using the abbreviation at the end of this

00:04:33.235 --> 00:04:33.875
<v Speaker 0>tuple,

00:04:34.435 --> 00:04:36.675
<v Speaker 0>and I'm gonna go to our pod spec.

00:04:39.555 --> 00:04:41.634
<v Speaker 0>We can drop the node name in on

00:04:41.634 --> 00:04:43.235
<v Speaker 0>the spec and just like so,

00:04:43.720 --> 00:04:45.560
<v Speaker 0>and this is 100%

00:04:45.560 --> 00:04:46.440
<v Speaker 0>valid.

00:04:48.120 --> 00:04:50.760
<v Speaker 0>We can either a cube control apply pod,

00:04:53.640 --> 00:04:56.040
<v Speaker 0>and if we run get pods dash o

00:04:56.040 --> 00:04:57.960
<v Speaker 0>wide so that we can see the node

00:04:57.960 --> 00:04:58.680
<v Speaker 0>that it's running on,

00:04:59.414 --> 00:05:01.815
<v Speaker 0>We'll see that it's running on x 42.

00:05:02.775 --> 00:05:05.095
<v Speaker 0>Now the scheduler doesn't do anything in this

00:05:05.095 --> 00:05:07.414
<v Speaker 0>case because the node name is already provided

00:05:07.414 --> 00:05:10.055
<v Speaker 0>by me, the operator or developer.

00:05:11.950 --> 00:05:13.950
<v Speaker 0>When that node name is missing, the scheduler

00:05:13.950 --> 00:05:15.790
<v Speaker 0>kicks in and plays a whole bunch of

00:05:15.790 --> 00:05:18.350
<v Speaker 0>rules and heuristics to determine where to schedule

00:05:18.350 --> 00:05:19.230
<v Speaker 0>each node.

00:05:22.190 --> 00:05:23.310
<v Speaker 0>Let's delete

00:05:24.270 --> 00:05:24.910
<v Speaker 0>our pod.

00:05:26.444 --> 00:05:27.165
<v Speaker 0>The

00:05:27.405 --> 00:05:29.245
<v Speaker 0>next way is scheduling a pod, we're going

00:05:29.245 --> 00:05:30.604
<v Speaker 0>to take a look at is through the

00:05:30.604 --> 00:05:31.725
<v Speaker 0>node selector.

00:05:32.205 --> 00:05:34.044
<v Speaker 0>Now in order to use a node selector,

00:05:34.044 --> 00:05:36.044
<v Speaker 0>you have to understand your nodes a little

00:05:36.044 --> 00:05:36.845
<v Speaker 0>bit more.

00:05:37.245 --> 00:05:39.485
<v Speaker 0>So we're going to run kubectl

00:05:39.485 --> 00:05:40.525
<v Speaker 0>describe

00:05:41.004 --> 00:05:42.444
<v Speaker 0>node

00:05:42.000 --> 00:05:44.240
<v Speaker 0>and paste in our node name.

00:05:45.040 --> 00:05:47.520
<v Speaker 0>Now this returns a lot of information, but

00:05:47.520 --> 00:05:49.600
<v Speaker 0>what we want to look at are the

00:05:49.600 --> 00:05:50.960
<v Speaker 0>labels at the top.

00:05:52.240 --> 00:05:53.760
<v Speaker 0>So you can see here that this node

00:05:53.760 --> 00:05:54.400
<v Speaker 0>has a name,

00:05:55.534 --> 00:05:57.694
<v Speaker 0>has a role of which is none in

00:05:57.694 --> 00:05:58.415
<v Speaker 0>this case,

00:05:58.814 --> 00:06:01.215
<v Speaker 0>but it has a whole bunch of labels.

00:06:02.254 --> 00:06:04.175
<v Speaker 0>We have a label that tells us the

00:06:04.175 --> 00:06:06.175
<v Speaker 0>architecture of the machine, in this case, it's

00:06:06.175 --> 00:06:07.375
<v Speaker 0>AMD sixty four.

00:06:08.520 --> 00:06:10.200
<v Speaker 0>We are also able to get some more

00:06:10.200 --> 00:06:12.840
<v Speaker 0>information from the cloud, like the instance type,

00:06:13.080 --> 00:06:15.160
<v Speaker 0>this one being an e two medium.

00:06:16.120 --> 00:06:18.120
<v Speaker 0>This also tells us the operating system is

00:06:18.120 --> 00:06:18.840
<v Speaker 0>Linux.

00:06:20.040 --> 00:06:21.560
<v Speaker 0>We then have a bunch of Google Cloud

00:06:21.560 --> 00:06:24.354
<v Speaker 0>labels telling us about the boot disk, the

00:06:24.354 --> 00:06:25.555
<v Speaker 0>container runtime,

00:06:25.794 --> 00:06:28.755
<v Speaker 0>CPU scaling level, log in variant, max pods

00:06:28.755 --> 00:06:32.835
<v Speaker 0>per node, node pool, OS distribution, machine family,

00:06:32.835 --> 00:06:34.195
<v Speaker 0>and private node.

00:06:35.634 --> 00:06:37.715
<v Speaker 0>We then have some failure domain information.

00:06:38.670 --> 00:06:41.310
<v Speaker 0>This allows us to schedule things across different

00:06:41.310 --> 00:06:43.230
<v Speaker 0>zones within a region and we'll take a

00:06:43.230 --> 00:06:45.150
<v Speaker 0>look at that in just a little bit.

00:06:46.270 --> 00:06:47.550
<v Speaker 0>Now the one we're going to use just

00:06:47.550 --> 00:06:49.310
<v Speaker 0>now to play with the node selector,

00:06:51.115 --> 00:06:52.794
<v Speaker 0>it's Kubernetes IO

00:06:53.435 --> 00:06:54.875
<v Speaker 0>slash Arch.

00:06:56.955 --> 00:06:59.035
<v Speaker 0>As we approach more and more clusters

00:06:59.914 --> 00:07:01.675
<v Speaker 0>that aren't homogenous,

00:07:02.074 --> 00:07:03.034
<v Speaker 0>they're heterogeneous.

00:07:03.810 --> 00:07:06.690
<v Speaker 0>We have clusters with AMD architecture

00:07:06.770 --> 00:07:08.370
<v Speaker 0>and ARM architecture.

00:07:10.850 --> 00:07:13.730
<v Speaker 0>You may even have machines with GPUs for

00:07:13.730 --> 00:07:16.210
<v Speaker 0>machine learning and artificial intelligence.

00:07:17.250 --> 00:07:19.885
<v Speaker 0>So we need the ability using the node

00:07:19.885 --> 00:07:20.765
<v Speaker 0>selector

00:07:21.325 --> 00:07:23.805
<v Speaker 0>to give the scheduler a clue that some

00:07:23.805 --> 00:07:25.885
<v Speaker 0>workloads are not like others, and we have

00:07:25.885 --> 00:07:28.045
<v Speaker 0>to be a bit more specific about where

00:07:28.045 --> 00:07:29.165
<v Speaker 0>we schedule them.

00:07:29.565 --> 00:07:31.165
<v Speaker 0>So let's go back to our pods pack.

00:07:32.210 --> 00:07:34.930
<v Speaker 0>And instead of setting the node name, we'll

00:07:34.930 --> 00:07:36.930
<v Speaker 0>say node selector.

00:07:37.970 --> 00:07:39.890
<v Speaker 0>And now I'm going to use those labels

00:07:39.890 --> 00:07:41.170
<v Speaker 0>that we've seen earlier.

00:07:41.810 --> 00:07:43.730
<v Speaker 0>The one I'm interested in

00:07:44.050 --> 00:07:47.410
<v Speaker 0>is beta.kubernetes.io/arch.

00:07:49.065 --> 00:07:50.665
<v Speaker 0>Now that all the nodes in my cluster

00:07:50.665 --> 00:07:54.025
<v Speaker 0>are AMD 64 like Copilot is suggesting.

00:07:54.665 --> 00:07:55.465
<v Speaker 0>However,

00:07:55.625 --> 00:07:58.905
<v Speaker 0>let's break this and say that this workload

00:07:58.985 --> 00:08:01.865
<v Speaker 0>can only run on ARM 64 machine.

00:08:03.970 --> 00:08:05.410
<v Speaker 0>We can now pop back over to the

00:08:05.410 --> 00:08:07.570
<v Speaker 0>terminal and do kubectl apply

00:08:08.210 --> 00:08:08.930
<v Speaker 0>pod.

00:08:10.050 --> 00:08:11.890
<v Speaker 0>Now when we run get pods,

00:08:12.690 --> 00:08:15.250
<v Speaker 0>we'll see that this is pending.

00:08:16.610 --> 00:08:18.690
<v Speaker 0>We can describe our pod

00:08:19.805 --> 00:08:21.645
<v Speaker 0>and we get this nice message at the

00:08:21.645 --> 00:08:22.285
<v Speaker 0>bottom.

00:08:22.845 --> 00:08:25.485
<v Speaker 0>The scheduler is telling us that it failed

00:08:25.485 --> 00:08:26.445
<v Speaker 0>to schedule.

00:08:27.645 --> 00:08:29.725
<v Speaker 0>It was the three nodes that are available,

00:08:30.045 --> 00:08:32.205
<v Speaker 0>none of them match the constraints that we

00:08:32.205 --> 00:08:32.765
<v Speaker 0>specified.

00:08:37.270 --> 00:08:38.630
<v Speaker 0>So let's change this.

00:08:39.190 --> 00:08:41.030
<v Speaker 0>So let's apply this again.

00:08:46.950 --> 00:08:47.830
<v Speaker 0>We run

00:08:48.310 --> 00:08:50.790
<v Speaker 0>get pods

00:08:50.045 --> 00:08:51.165
<v Speaker 0>and it's running.

00:08:54.045 --> 00:08:55.805
<v Speaker 0>Alright. Let's do one more thing with the

00:08:55.805 --> 00:08:56.845
<v Speaker 0>node scheduler.

00:08:59.964 --> 00:09:03.245
<v Speaker 0>The node selector doesn't just take one parameter.

00:09:03.800 --> 00:09:06.120
<v Speaker 0>This works very much like match labels on

00:09:06.120 --> 00:09:08.680
<v Speaker 0>your deployment spec. You can add as many

00:09:08.680 --> 00:09:09.560
<v Speaker 0>as you want.

00:09:10.040 --> 00:09:13.480
<v Speaker 0>So let's remove the beta to Kubernetes.io

00:09:13.480 --> 00:09:14.680
<v Speaker 0>slash OS

00:09:14.839 --> 00:09:15.959
<v Speaker 0>of Linux.

00:09:16.120 --> 00:09:18.120
<v Speaker 0>So now we need a Linux machine

00:09:19.404 --> 00:09:22.285
<v Speaker 0>on an AMD sixty four chip.

00:09:23.485 --> 00:09:25.005
<v Speaker 0>We can now apply this

00:09:26.045 --> 00:09:27.325
<v Speaker 0>from get pods,

00:09:30.045 --> 00:09:30.765
<v Speaker 0>and it's running.

00:09:31.710 --> 00:09:33.870
<v Speaker 0>So the node selector is a really great

00:09:33.870 --> 00:09:35.950
<v Speaker 0>way to take those labels of the knowledge

00:09:35.950 --> 00:09:38.270
<v Speaker 0>and understanding the context that we have about

00:09:38.270 --> 00:09:39.790
<v Speaker 0>the nodes within our cluster

00:09:40.110 --> 00:09:43.150
<v Speaker 0>and move our workloads around as required.

00:09:43.550 --> 00:09:45.835
<v Speaker 0>We're not enforcing too many constraints. We're not

00:09:45.835 --> 00:09:48.075
<v Speaker 0>being very specific about how it works or

00:09:48.075 --> 00:09:50.715
<v Speaker 0>lives with other pods within our cluster, but

00:09:50.715 --> 00:09:52.795
<v Speaker 0>we're gonna get to that in just a

00:09:52.795 --> 00:09:53.435
<v Speaker 0>second.

00:09:53.915 --> 00:09:56.395
<v Speaker 0>The notes later is pretty easy to grok.

00:09:56.555 --> 00:09:57.435
<v Speaker 0>You have labels.

00:09:58.490 --> 00:09:59.690
<v Speaker 0>You have pods,

00:10:00.089 --> 00:10:01.290
<v Speaker 0>match them together.

00:10:02.410 --> 00:10:04.649
<v Speaker 0>Sometimes when scheduling pods,

00:10:04.889 --> 00:10:06.889
<v Speaker 0>you need a little bit more logic than

00:10:06.889 --> 00:10:09.050
<v Speaker 0>just basic label selectors,

00:10:09.610 --> 00:10:11.850
<v Speaker 0>and that's where node affinity comes in.

00:10:12.875 --> 00:10:14.315
<v Speaker 0>So before we take a look at node

00:10:14.315 --> 00:10:17.435
<v Speaker 0>affinity, I'm going to run kubectl get nodes.

00:10:17.675 --> 00:10:19.355
<v Speaker 0>Only this time, I'm going to add show

00:10:19.355 --> 00:10:20.155
<v Speaker 0>labels.

00:10:21.195 --> 00:10:22.555
<v Speaker 0>And these are all the labels that we

00:10:22.555 --> 00:10:24.635
<v Speaker 0>can use as part of the node selector

00:10:24.635 --> 00:10:26.795
<v Speaker 0>or node affinity configuration.

00:10:29.950 --> 00:10:32.430
<v Speaker 0>We'll see here that I have a machine

00:10:33.150 --> 00:10:35.070
<v Speaker 0>using the topology labels

00:10:35.310 --> 00:10:37.230
<v Speaker 0>in Europe north one a,

00:10:37.550 --> 00:10:39.230
<v Speaker 0>Europe North 1 c,

00:10:39.630 --> 00:10:40.590
<v Speaker 0>and Europe

00:10:40.590 --> 00:10:41.550
<v Speaker 0>North 1 b.

00:10:43.085 --> 00:10:45.485
<v Speaker 0>Let's jump back to our pod spec.

00:10:47.005 --> 00:10:49.405
<v Speaker 0>And this time, we're gonna specify

00:10:49.885 --> 00:10:50.845
<v Speaker 0>affinity.

00:10:52.685 --> 00:10:54.045
<v Speaker 0>And we're gonna use

00:10:54.890 --> 00:10:55.930
<v Speaker 0>Node Affinity.

00:10:57.850 --> 00:11:00.490
<v Speaker 0>And you can see Copilot is jumping straight

00:11:00.490 --> 00:11:02.570
<v Speaker 0>ahead here, but I'm just gonna let it

00:11:02.570 --> 00:11:04.570
<v Speaker 0>complete and we'll run through it quickly.

00:11:06.090 --> 00:11:07.930
<v Speaker 0>Now when it comes to Node Affinity, there

00:11:07.930 --> 00:11:09.210
<v Speaker 0>are two keys

00:11:09.435 --> 00:11:10.715
<v Speaker 0>that can be used.

00:11:12.555 --> 00:11:13.435
<v Speaker 0>There is

00:11:15.595 --> 00:11:17.355
<v Speaker 0>preferred during scheduling,

00:11:17.355 --> 00:11:20.715
<v Speaker 0>ignored during execution, and required during scheduling,

00:11:20.715 --> 00:11:21.835
<v Speaker 0>ignored during execution.

00:11:22.880 --> 00:11:24.720
<v Speaker 0>Let's come back to preferred

00:11:24.800 --> 00:11:26.080
<v Speaker 0>in just a moment.

00:11:27.760 --> 00:11:31.440
<v Speaker 0>Now required during scheduling and ignored during execution

00:11:31.440 --> 00:11:34.320
<v Speaker 0>just means that the constraints that we specify

00:11:34.960 --> 00:11:36.400
<v Speaker 0>must be true

00:11:36.735 --> 00:11:39.055
<v Speaker 0>at the time the pod is scheduled.

00:11:40.175 --> 00:11:40.975
<v Speaker 0>Remember,

00:11:41.055 --> 00:11:44.095
<v Speaker 0>Kubernetes is eventually consistent through and through.

00:11:44.415 --> 00:11:46.815
<v Speaker 0>Labels can change over time.

00:11:47.295 --> 00:11:49.055
<v Speaker 0>Just because a pod ends up on a

00:11:49.055 --> 00:11:50.654
<v Speaker 0>node now

00:11:50.190 --> 00:11:52.030
<v Speaker 0>doesn't mean the labels that were used for

00:11:52.030 --> 00:11:54.510
<v Speaker 0>that scheduling decision will still be true in

00:11:54.510 --> 00:11:56.910
<v Speaker 0>five minutes, five days, or five months.

00:11:57.390 --> 00:11:59.710
<v Speaker 0>Hopefully, your pods aren't running for five months.

00:12:01.150 --> 00:12:01.790
<v Speaker 0>Now

00:12:02.315 --> 00:12:05.435
<v Speaker 0>affinities are very much like node selectors. We're

00:12:05.435 --> 00:12:07.195
<v Speaker 0>still going to be using labels,

00:12:07.274 --> 00:12:09.915
<v Speaker 0>but we have the ability to apply operators

00:12:09.915 --> 00:12:11.435
<v Speaker 0>in multiple values

00:12:12.075 --> 00:12:14.394
<v Speaker 0>rather than just basic 100%

00:12:14.394 --> 00:12:14.795
<v Speaker 0>match.

00:12:15.820 --> 00:12:17.500
<v Speaker 0>So you'll see here that we have the

00:12:17.500 --> 00:12:19.180
<v Speaker 0>node selector terms

00:12:19.339 --> 00:12:22.060
<v Speaker 0>and the ability to match expressions.

00:12:23.180 --> 00:12:25.180
<v Speaker 0>I'm going to replace this

00:12:25.980 --> 00:12:28.220
<v Speaker 0>with a label that we copied earlier.

00:12:28.700 --> 00:12:30.860
<v Speaker 0>However, I'll remove the value and remove the

00:12:30.860 --> 00:12:31.260
<v Speaker 0>equals,

00:12:31.875 --> 00:12:33.555
<v Speaker 0>and we'll say that we want any value

00:12:33.555 --> 00:12:34.355
<v Speaker 0>in

00:12:35.955 --> 00:12:37.555
<v Speaker 0>Europe North 1 A

00:12:38.835 --> 00:12:40.675
<v Speaker 0>and Europe North 1 B.

00:12:42.355 --> 00:12:44.275
<v Speaker 0>Let's apply this to our cluster.

00:12:55.160 --> 00:12:57.160
<v Speaker 0>And you'll see here that our pod was

00:12:57.160 --> 00:12:59.240
<v Speaker 0>scheduled on X 42,

00:13:00.095 --> 00:13:02.815
<v Speaker 0>which is Europe North 1 A.

00:13:09.455 --> 00:13:11.295
<v Speaker 0>Let's delete and apply again.

00:13:16.600 --> 00:13:18.200
<v Speaker 0>We got the same node.

00:13:19.320 --> 00:13:20.680
<v Speaker 0>Let's try it one more time for the

00:13:20.680 --> 00:13:21.240
<v Speaker 0>last.

00:13:26.120 --> 00:13:27.000
<v Speaker 0>Oh, well.

00:13:27.320 --> 00:13:28.440
<v Speaker 0>Let's mix it up a bit.

00:13:29.095 --> 00:13:32.535
<v Speaker 0>Let's change our constraint to only accept b

00:13:32.695 --> 00:13:33.655
<v Speaker 0>and c.

00:13:34.455 --> 00:13:36.215
<v Speaker 0>We'll delete and apply.

00:13:38.855 --> 00:13:41.335
<v Speaker 0>And now we're scheduled on F Zed F

00:13:41.335 --> 00:13:41.655
<v Speaker 0>9.

00:13:42.670 --> 00:13:43.390
<v Speaker 0>This

00:13:43.870 --> 00:13:45.470
<v Speaker 0>is North 1 C.

00:13:47.150 --> 00:13:48.830
<v Speaker 0>So using node affinities,

00:13:49.070 --> 00:13:50.910
<v Speaker 0>we can do a match expressions

00:13:51.710 --> 00:13:54.350
<v Speaker 0>using the same labels from the node selector,

00:13:55.225 --> 00:13:57.705
<v Speaker 0>the ability to select an operator

00:13:57.865 --> 00:14:00.345
<v Speaker 0>and work with multiple values. So let's make

00:14:00.345 --> 00:14:02.185
<v Speaker 0>a change to one of these operators.

00:14:02.825 --> 00:14:05.385
<v Speaker 0>Currently, are scheduled to North 1 C.

00:14:06.985 --> 00:14:08.585
<v Speaker 0>So instead of doing in

00:14:09.020 --> 00:14:10.220
<v Speaker 0>North 1 C,

00:14:11.740 --> 00:14:13.260
<v Speaker 0>let's say not in.

00:14:14.380 --> 00:14:16.300
<v Speaker 0>Now we can jump back to our terminal,

00:14:16.620 --> 00:14:17.660
<v Speaker 0>delete and apply,

00:14:21.705 --> 00:14:24.425
<v Speaker 0>and now we're back on North 1 A.

00:14:25.465 --> 00:14:28.025
<v Speaker 0>What operators are available? Well, we have the

00:14:28.025 --> 00:14:30.985
<v Speaker 0>ability to do does not exist, exist, greater

00:14:30.985 --> 00:14:33.305
<v Speaker 0>than, in, less than, and not in.

00:14:34.020 --> 00:14:36.900
<v Speaker 0>So enough operators to cover most of the

00:14:36.900 --> 00:14:39.300
<v Speaker 0>basic scheduling constraints that you would that you

00:14:39.300 --> 00:14:41.780
<v Speaker 0>would want to apply to your workload.

00:14:42.500 --> 00:14:44.260
<v Speaker 0>Now in the interest of this being a

00:14:44.260 --> 00:14:46.660
<v Speaker 0>deep dive and being complete, I will also

00:14:46.660 --> 00:14:49.300
<v Speaker 0>add the match expressions isn't the only option.

00:14:49.704 --> 00:14:51.865
<v Speaker 0>We also have the ability to match on

00:14:51.865 --> 00:14:52.665
<v Speaker 0>fields.

00:14:57.865 --> 00:14:58.745
<v Speaker 0>Like so.

00:14:59.305 --> 00:15:01.225
<v Speaker 0>Now this isn't a feature that you're likely

00:15:01.225 --> 00:15:03.680
<v Speaker 0>to use unless you're scheduling a daemon set

00:15:03.680 --> 00:15:06.000
<v Speaker 0>on a subset of nodes within your cluster.

00:15:06.639 --> 00:15:08.399
<v Speaker 0>The reason is is that is that the

00:15:08.399 --> 00:15:10.000
<v Speaker 0>labels we have on a node include the

00:15:10.000 --> 00:15:13.040
<v Speaker 0>host name, but the host name isn't always

00:15:13.200 --> 00:15:14.000
<v Speaker 0>your node name.

00:15:14.865 --> 00:15:17.345
<v Speaker 0>So this match fields was added purely for

00:15:17.345 --> 00:15:18.145
<v Speaker 0>that case,

00:15:18.545 --> 00:15:20.385
<v Speaker 0>and really the only key you can actually

00:15:20.385 --> 00:15:22.865
<v Speaker 0>match on is the metadata dot name.

00:15:24.145 --> 00:15:26.385
<v Speaker 0>So I've covered it. You don't really need

00:15:26.385 --> 00:15:26.865
<v Speaker 0>it.

00:15:27.345 --> 00:15:28.545
<v Speaker 0>You can just forget about it.

00:15:29.410 --> 00:15:31.810
<v Speaker 0>Let's move on. So as I said, we

00:15:31.810 --> 00:15:33.650
<v Speaker 0>have required during scheduling,

00:15:33.650 --> 00:15:36.050
<v Speaker 0>but also preferred during scheduling.

00:15:37.410 --> 00:15:38.930
<v Speaker 0>So let's set this now.

00:15:40.530 --> 00:15:42.290
<v Speaker 0>This time, I'll let Copilot

00:15:43.025 --> 00:15:44.065
<v Speaker 0>do its thing.

00:15:44.625 --> 00:15:46.225
<v Speaker 0>We're gonna come back up here

00:15:49.265 --> 00:15:51.745
<v Speaker 0>and use the Kubernetes IO arch

00:15:51.745 --> 00:15:52.625
<v Speaker 0>label.

00:15:53.505 --> 00:15:55.585
<v Speaker 0>Now we know this cluster has no arch

00:15:55.585 --> 00:15:56.625
<v Speaker 0>60 fours,

00:15:57.025 --> 00:15:58.225
<v Speaker 0>so let's drop that in.

00:15:59.350 --> 00:16:01.190
<v Speaker 0>Now we'll go back to the terminal

00:16:03.110 --> 00:16:04.550
<v Speaker 0>and apply our pod.

00:16:05.189 --> 00:16:06.870
<v Speaker 0>What do you think is gonna happen?

00:16:07.990 --> 00:16:10.389
<v Speaker 0>We have no arch 64

00:16:10.745 --> 00:16:11.625
<v Speaker 0>nodes.

00:16:12.505 --> 00:16:14.665
<v Speaker 0>However, when we run get pods,

00:16:15.625 --> 00:16:17.305
<v Speaker 0>our container was scheduled.

00:16:17.704 --> 00:16:20.425
<v Speaker 0>And that's because while the scheduler couldn't find

00:16:20.425 --> 00:16:22.264
<v Speaker 0>a node that was a complete match,

00:16:23.890 --> 00:16:25.410
<v Speaker 0>It was just a preference,

00:16:25.970 --> 00:16:27.730
<v Speaker 0>so it settled for another.

00:16:28.850 --> 00:16:31.490
<v Speaker 0>Now you can chain lots of these together

00:16:31.730 --> 00:16:33.490
<v Speaker 0>and set multiple preferences

00:16:34.930 --> 00:16:38.930
<v Speaker 0>and increase the weight from one to 100

00:16:38.165 --> 00:16:39.045
<v Speaker 0>depending

00:16:39.125 --> 00:16:42.085
<v Speaker 0>on how much you favor each preference.

00:16:42.645 --> 00:16:44.405
<v Speaker 0>Okay. So we're taking a look at node

00:16:44.405 --> 00:16:45.205
<v Speaker 0>affinities.

00:16:45.685 --> 00:16:48.165
<v Speaker 0>Now let's take a look at pod affinities.

00:16:48.965 --> 00:16:49.845
<v Speaker 0>What's the difference?

00:16:50.760 --> 00:16:53.400
<v Speaker 0>Well, node affinity allows us to say that

00:16:53.400 --> 00:16:55.800
<v Speaker 0>a workload should run on these nodes with

00:16:55.800 --> 00:16:56.840
<v Speaker 0>these constraints.

00:16:58.680 --> 00:16:59.960
<v Speaker 0>Pod affinities

00:17:00.200 --> 00:17:02.280
<v Speaker 0>allow us to take that one step further.

00:17:02.855 --> 00:17:04.214
<v Speaker 0>Now we can say that we want this

00:17:04.214 --> 00:17:06.454
<v Speaker 0>pod to run as close to these pods

00:17:06.455 --> 00:17:08.935
<v Speaker 0>or these pods to run as far away

00:17:08.935 --> 00:17:10.135
<v Speaker 0>from those pods.

00:17:10.535 --> 00:17:12.934
<v Speaker 0>This gives us the ability to repel

00:17:13.415 --> 00:17:16.054
<v Speaker 0>or co locate pods within our infrastructure.

00:17:16.855 --> 00:17:18.135
<v Speaker 0>Why would we want to do that?

00:17:18.769 --> 00:17:21.009
<v Speaker 0>Well, if you have components within a mission

00:17:21.009 --> 00:17:22.049
<v Speaker 0>critical system,

00:17:22.609 --> 00:17:24.929
<v Speaker 0>it's likely you won't want to schedule them

00:17:24.929 --> 00:17:26.209
<v Speaker 0>on the same node

00:17:27.009 --> 00:17:28.849
<v Speaker 0>because nodes go away.

00:17:29.090 --> 00:17:30.209
<v Speaker 0>Nodes fail.

00:17:30.450 --> 00:17:32.210
<v Speaker 0>And if your node fails, do you really

00:17:32.210 --> 00:17:35.305
<v Speaker 0>want to lose multiple multiple components of a

00:17:35.305 --> 00:17:36.665
<v Speaker 0>mission critical system?

00:17:37.145 --> 00:17:39.305
<v Speaker 0>We have to minimize the blast radius of

00:17:39.305 --> 00:17:41.705
<v Speaker 0>the collateral damage when we lose a node.

00:17:42.745 --> 00:17:45.065
<v Speaker 0>On the other hand, if performance is something

00:17:45.065 --> 00:17:47.480
<v Speaker 0>that you're more sensitive to, it could be

00:17:47.480 --> 00:17:50.120
<v Speaker 0>that you want your customer facing API as

00:17:50.120 --> 00:17:52.760
<v Speaker 0>close to your database as possible to minimize

00:17:52.760 --> 00:17:55.240
<v Speaker 0>latency and network hops around your infrastructure.

00:17:55.640 --> 00:17:58.600
<v Speaker 0>So we'd use affinity to colocate and anti

00:17:58.600 --> 00:17:59.880
<v Speaker 0>affinity to repel.

00:18:01.735 --> 00:18:03.894
<v Speaker 0>Let's take a look at this by example.

00:18:04.855 --> 00:18:07.575
<v Speaker 0>Here, I have four pod specifications.

00:18:08.295 --> 00:18:10.695
<v Speaker 0>I have a pod called NGINX East one,

00:18:10.855 --> 00:18:11.895
<v Speaker 0>NGINX East two,

00:18:12.570 --> 00:18:15.849
<v Speaker 0>and NGINX West one, and NGINX West two.

00:18:16.090 --> 00:18:17.929
<v Speaker 0>They're very creative, I know.

00:18:18.970 --> 00:18:21.370
<v Speaker 0>We're gonna set up some pod affinities to

00:18:21.370 --> 00:18:23.529
<v Speaker 0>schedule the East pods as close to each

00:18:23.529 --> 00:18:25.975
<v Speaker 0>other as possible and the West pods as

00:18:25.975 --> 00:18:27.495
<v Speaker 0>close to each other as possible.

00:18:27.895 --> 00:18:29.895
<v Speaker 0>And we won't bother with anti affinity just

00:18:29.895 --> 00:18:30.534
<v Speaker 0>yet.

00:18:31.415 --> 00:18:33.255
<v Speaker 0>So we're gonna come into our pod spec

00:18:33.255 --> 00:18:35.254
<v Speaker 0>and we use the affinity key.

00:18:36.215 --> 00:18:37.655
<v Speaker 0>And now we can see that we actually

00:18:37.655 --> 00:18:39.335
<v Speaker 0>want pod

00:18:39.575 --> 00:18:39.975
<v Speaker 0>affinity.

00:18:41.470 --> 00:18:44.029
<v Speaker 0>And I'll let Copilot drop this straight on,

00:18:44.029 --> 00:18:45.629
<v Speaker 0>but we'll walk right through it.

00:18:46.350 --> 00:18:48.029
<v Speaker 0>Now the first thing we need to do

00:18:48.350 --> 00:18:50.590
<v Speaker 0>is configure our label selector.

00:18:51.309 --> 00:18:53.230
<v Speaker 0>The label selector is a bit different from

00:18:53.230 --> 00:18:54.830
<v Speaker 0>the node selector this time

00:18:56.044 --> 00:18:58.125
<v Speaker 0>because we're using the selector

00:18:58.125 --> 00:19:01.725
<v Speaker 0>to group the pods with the shared affinity.

00:19:02.525 --> 00:19:03.725
<v Speaker 0>Which pods

00:19:03.725 --> 00:19:05.725
<v Speaker 0>do we want to run as close to

00:19:05.725 --> 00:19:08.845
<v Speaker 0>each other as possible? Now Copilot's already guessed

00:19:08.845 --> 00:19:10.205
<v Speaker 0>exactly what I was gonna do here,

00:19:11.880 --> 00:19:13.799
<v Speaker 0>which is to match on the names

00:19:14.520 --> 00:19:16.360
<v Speaker 0>and say that we want to group east

00:19:16.360 --> 00:19:17.799
<v Speaker 0>one and east two

00:19:18.280 --> 00:19:19.159
<v Speaker 0>together.

00:19:22.040 --> 00:19:24.440
<v Speaker 0>The last part of this pod affinity is

00:19:24.440 --> 00:19:25.640
<v Speaker 0>the topology key,

00:19:26.575 --> 00:19:28.895
<v Speaker 0>and all this is doing is telling us

00:19:28.895 --> 00:19:31.055
<v Speaker 0>which label on the node to use as

00:19:31.055 --> 00:19:31.935
<v Speaker 0>a grouping.

00:19:32.495 --> 00:19:34.815
<v Speaker 0>So when we match these pods together, we

00:19:34.815 --> 00:19:36.655
<v Speaker 0>want to ensure that they all end up

00:19:36.655 --> 00:19:37.935
<v Speaker 0>in the same zone,

00:19:38.610 --> 00:19:40.049
<v Speaker 0>which is why we're using

00:19:41.250 --> 00:19:44.049
<v Speaker 0>the filler domain beta Kubernetes IO zone.

00:19:44.690 --> 00:19:46.850
<v Speaker 0>Of course, if we describe our nodes

00:19:49.570 --> 00:19:50.610
<v Speaker 0>or show labels,

00:19:53.445 --> 00:19:56.725
<v Speaker 0>We could also use this topology code here

00:19:56.725 --> 00:19:58.644
<v Speaker 0>if we wanted to do this regionally,

00:19:59.445 --> 00:20:01.445
<v Speaker 0>or we could use the zone code here.

00:20:05.205 --> 00:20:06.804
<v Speaker 0>And instead of using the filler domain,

00:20:07.470 --> 00:20:10.109
<v Speaker 0>let's use topology Kubernetes zone.

00:20:11.150 --> 00:20:13.149
<v Speaker 0>So let's copy this affinity,

00:20:16.110 --> 00:20:17.549
<v Speaker 0>apply it here too,

00:20:19.630 --> 00:20:21.575
<v Speaker 0>and we'll do the same for West, only

00:20:21.575 --> 00:20:23.255
<v Speaker 0>this time we'll update

00:20:23.895 --> 00:20:25.255
<v Speaker 0>the West keys,

00:20:33.815 --> 00:20:34.295
<v Speaker 0>like so.

00:20:36.840 --> 00:20:39.880
<v Speaker 0>Let's apply our four pods to the cluster.

00:20:40.920 --> 00:20:42.520
<v Speaker 0>Now what we should see is that the

00:20:42.520 --> 00:20:44.440
<v Speaker 0>two east pods are scheduled in the same

00:20:44.440 --> 00:20:47.160
<v Speaker 0>zone and the two west pods are scheduled

00:20:47.160 --> 00:20:48.040
<v Speaker 0>in the same zone.

00:20:48.915 --> 00:20:50.275
<v Speaker 0>But right now,

00:20:50.355 --> 00:20:52.835
<v Speaker 0>that zone could be the same for all

00:20:52.835 --> 00:20:54.195
<v Speaker 0>four pods

00:20:54.755 --> 00:20:58.195
<v Speaker 0>because we haven't built any repel or anti

00:20:58.195 --> 00:20:58.995
<v Speaker 0>affinity

00:20:59.555 --> 00:21:01.235
<v Speaker 0>into the two different pod specs.

00:21:02.730 --> 00:21:04.809
<v Speaker 0>So let's see where they all ended up.

00:21:09.290 --> 00:21:10.090
<v Speaker 0>Alright.

00:21:10.570 --> 00:21:13.370
<v Speaker 0>West is on F Zed, and East is

00:21:13.370 --> 00:21:14.650
<v Speaker 0>on X 42.

00:21:15.434 --> 00:21:18.075
<v Speaker 0>So updating these pod manifests

00:21:18.315 --> 00:21:19.835
<v Speaker 0>is a little painful.

00:21:20.315 --> 00:21:22.874
<v Speaker 0>So let's delete them for the last time.

00:21:23.595 --> 00:21:26.394
<v Speaker 0>And now let's work with some deployment objects

00:21:26.635 --> 00:21:28.715
<v Speaker 0>that we can scale up and down

00:21:29.430 --> 00:21:31.670
<v Speaker 0>to see how these affinities and anti affinities

00:21:31.670 --> 00:21:33.670
<v Speaker 0>work in a slightly larger scale.

00:21:33.910 --> 00:21:35.590
<v Speaker 0>So let's take a look at our new

00:21:35.590 --> 00:21:36.870
<v Speaker 0>deployment specs.

00:21:38.150 --> 00:21:40.390
<v Speaker 0>Here we have an NGINX East deployment

00:21:40.390 --> 00:21:42.870
<v Speaker 0>replicas 10 with a pod affinity

00:21:43.945 --> 00:21:46.025
<v Speaker 0>to group all of its pods together

00:21:46.345 --> 00:21:47.705
<v Speaker 0>on the same host.

00:21:49.705 --> 00:21:52.345
<v Speaker 0>Down here, we have NGINX West replicas 10

00:21:52.345 --> 00:21:54.105
<v Speaker 0>with the same affinity to group all of

00:21:54.105 --> 00:21:56.665
<v Speaker 0>NGINX West pods together

00:21:56.310 --> 00:21:57.670
<v Speaker 0>on the same host.

00:21:59.510 --> 00:22:01.830
<v Speaker 0>We can apply this to our cluster,

00:22:05.750 --> 00:22:08.070
<v Speaker 0>run get pods, and we'll see the containers

00:22:08.070 --> 00:22:09.030
<v Speaker 0>are creating.

00:22:09.875 --> 00:22:11.555
<v Speaker 0>Let's run that again and hope it's been

00:22:11.555 --> 00:22:14.035
<v Speaker 0>enough time to get the output of white.

00:22:14.675 --> 00:22:17.635
<v Speaker 0>We'll see 10 pods on f zed f

00:22:17.635 --> 00:22:19.235
<v Speaker 0>for nginx west

00:22:19.475 --> 00:22:21.795
<v Speaker 0>and 10 pods on x 42

00:22:21.795 --> 00:22:22.915
<v Speaker 0>for nginx east.

00:22:23.840 --> 00:22:26.240
<v Speaker 0>So we've removed NGINX East and West,

00:22:26.720 --> 00:22:28.880
<v Speaker 0>and we're trying to represent something that is

00:22:28.880 --> 00:22:31.519
<v Speaker 0>a more traditional three tier application setup.

00:22:32.080 --> 00:22:34.399
<v Speaker 0>Here, we have some sort of critical application.

00:22:34.800 --> 00:22:36.800
<v Speaker 0>This could be your front end proxy, like

00:22:36.800 --> 00:22:39.645
<v Speaker 0>h a proxy, NGINX, whatever you're using to

00:22:39.645 --> 00:22:41.804
<v Speaker 0>deliver cached assets to your customers.

00:22:42.685 --> 00:22:44.445
<v Speaker 0>It's very likely you never want this to

00:22:44.445 --> 00:22:46.685
<v Speaker 0>fall over because then your customers aren't getting

00:22:46.685 --> 00:22:47.805
<v Speaker 0>aren't

00:22:47.805 --> 00:22:49.164
<v Speaker 0>getting their responses.

00:22:49.885 --> 00:22:52.045
<v Speaker 0>So we're gonna call it critical app a.

00:22:53.030 --> 00:22:55.750
<v Speaker 0>Next, we have web app a. This is

00:22:55.750 --> 00:22:57.590
<v Speaker 0>the back end for the critical app.

00:22:57.909 --> 00:22:59.669
<v Speaker 0>So if we can't deliver something from a

00:22:59.669 --> 00:23:01.030
<v Speaker 0>cache, we have to go speak to the

00:23:01.030 --> 00:23:02.149
<v Speaker 0>back end service

00:23:03.110 --> 00:23:05.715
<v Speaker 0>and deliver the result. We're gonna call this

00:23:05.715 --> 00:23:06.674
<v Speaker 0>web app a,

00:23:07.155 --> 00:23:09.235
<v Speaker 0>and web app a needs to speak to

00:23:09.235 --> 00:23:11.955
<v Speaker 0>a database. So we have database a.

00:23:12.835 --> 00:23:14.115
<v Speaker 0>My name, it sucks.

00:23:14.595 --> 00:23:17.075
<v Speaker 0>So we have our three tier application setup.

00:23:17.075 --> 00:23:18.835
<v Speaker 0>Now all of these containers are just NGINX

00:23:18.835 --> 00:23:20.940
<v Speaker 0>under the hood. It's not important.

00:23:21.100 --> 00:23:23.740
<v Speaker 0>We're just looking at the scheduling constraints.

00:23:24.620 --> 00:23:26.620
<v Speaker 0>So in a production like environment, what do

00:23:26.620 --> 00:23:27.500
<v Speaker 0>we need to do?

00:23:28.300 --> 00:23:30.940
<v Speaker 0>Well, we're very concerned about the performance of

00:23:30.940 --> 00:23:33.020
<v Speaker 0>our web applications speaking to our database.

00:23:33.345 --> 00:23:35.025
<v Speaker 0>So let's do something that we already know

00:23:35.025 --> 00:23:35.825
<v Speaker 0>how to do

00:23:36.705 --> 00:23:39.904
<v Speaker 0>and couple these together through affinity.

00:23:41.585 --> 00:23:43.345
<v Speaker 0>Well, let's go to our affinity.

00:23:43.505 --> 00:23:45.105
<v Speaker 0>We'll do pod affinity.

00:23:47.770 --> 00:23:50.250
<v Speaker 0>We'll let Copilot drop in some things.

00:23:50.330 --> 00:23:51.929
<v Speaker 0>And what we're going to say here

00:23:54.250 --> 00:23:56.410
<v Speaker 0>is that we want to schedule web apps

00:23:56.410 --> 00:23:58.490
<v Speaker 0>next to database apps,

00:23:59.130 --> 00:24:01.130
<v Speaker 0>and we want them on the same host

00:24:01.130 --> 00:24:01.530
<v Speaker 0>name.

00:24:04.215 --> 00:24:06.615
<v Speaker 0>Doesn't matter where the databases end up.

00:24:07.095 --> 00:24:09.255
<v Speaker 0>We're only gonna set the affinity on the

00:24:09.255 --> 00:24:11.575
<v Speaker 0>web app. So whenever those come to be

00:24:11.575 --> 00:24:12.375
<v Speaker 0>scheduled,

00:24:12.775 --> 00:24:15.015
<v Speaker 0>we always find a node or hopefully always

00:24:15.015 --> 00:24:16.695
<v Speaker 0>find a node where there is a database

00:24:16.695 --> 00:24:18.935
<v Speaker 0>product available

00:24:17.820 --> 00:24:19.339
<v Speaker 0>to couple those together.

00:24:22.860 --> 00:24:24.700
<v Speaker 0>Let's apply this to our cluster

00:24:28.059 --> 00:24:29.499
<v Speaker 0>and run get pods.

00:24:30.380 --> 00:24:31.825
<v Speaker 0>Now we can see we have two copies

00:24:31.825 --> 00:24:33.904
<v Speaker 0>of the critical app are on f zed

00:24:33.904 --> 00:24:35.424
<v Speaker 0>f and x 42.

00:24:35.904 --> 00:24:38.544
<v Speaker 0>Our database ended up on x 42,

00:24:38.544 --> 00:24:40.145
<v Speaker 0>which means our two web apps ended up

00:24:40.145 --> 00:24:41.505
<v Speaker 0>on x 42.

00:24:43.110 --> 00:24:46.150
<v Speaker 0>So let's increase the replicas for our web

00:24:46.150 --> 00:24:47.029
<v Speaker 0>application.

00:24:49.030 --> 00:24:50.629
<v Speaker 0>Now let's say we need 10.

00:24:52.070 --> 00:24:53.990
<v Speaker 0>Where do you think they're gonna end up?

00:24:55.750 --> 00:24:58.070
<v Speaker 0>Well, let's run get pods

00:24:58.365 --> 00:24:59.325
<v Speaker 0>await,

00:25:00.845 --> 00:25:03.565
<v Speaker 0>and we're now starting to heavily overload our

00:25:03.565 --> 00:25:05.325
<v Speaker 0>x 42 server.

00:25:06.205 --> 00:25:06.924
<v Speaker 0>Why?

00:25:07.405 --> 00:25:09.405
<v Speaker 0>Because it's the only place we have a

00:25:09.405 --> 00:25:10.285
<v Speaker 0>scheduled database,

00:25:10.660 --> 00:25:13.300
<v Speaker 0>and the pod affinity rules mandate that all

00:25:13.300 --> 00:25:15.940
<v Speaker 0>of those web application pods should run on

00:25:15.940 --> 00:25:17.220
<v Speaker 0>the same host name.

00:25:18.580 --> 00:25:21.700
<v Speaker 0>So let's horizontally scale our database application.

00:25:22.900 --> 00:25:24.980
<v Speaker 0>And I'm gonna scale back our web application

00:25:24.980 --> 00:25:27.115
<v Speaker 0>at the same time just so that we

00:25:27.115 --> 00:25:29.115
<v Speaker 0>can scale it back up to see even

00:25:29.115 --> 00:25:29.994
<v Speaker 0>distribution

00:25:30.635 --> 00:25:32.235
<v Speaker 0>across more than one node.

00:25:33.195 --> 00:25:35.034
<v Speaker 0>And we'll scale up our database

00:25:35.355 --> 00:25:36.155
<v Speaker 0>to be three.

00:25:42.040 --> 00:25:43.960
<v Speaker 0>Now when we run get pods,

00:25:44.520 --> 00:25:45.640
<v Speaker 0>drags,

00:25:45.720 --> 00:25:48.120
<v Speaker 0>we have three databases running on the same

00:25:48.120 --> 00:25:48.840
<v Speaker 0>node.

00:25:50.120 --> 00:25:51.480
<v Speaker 0>That's not ideal.

00:25:52.360 --> 00:25:54.040
<v Speaker 0>So now we need to do something else.

00:25:55.495 --> 00:25:57.575
<v Speaker 0>We have our web applications matched in Affinity

00:25:57.575 --> 00:25:59.014
<v Speaker 0>with the database nodes.

00:25:59.335 --> 00:26:01.735
<v Speaker 0>Our two critical applications got schedules on different

00:26:01.735 --> 00:26:03.255
<v Speaker 0>nodes by chance,

00:26:03.655 --> 00:26:06.294
<v Speaker 0>and our three database nodes got scheduled together

00:26:06.295 --> 00:26:08.830
<v Speaker 0>by chance on the same node. And I'm

00:26:08.830 --> 00:26:10.990
<v Speaker 0>not a big fan of by chance in

00:26:10.990 --> 00:26:12.429
<v Speaker 0>a Kubernetes environment.

00:26:13.389 --> 00:26:15.709
<v Speaker 0>So we need to apply anti affinities

00:26:15.950 --> 00:26:18.909
<v Speaker 0>to our database and our critical application.

00:26:20.909 --> 00:26:22.350
<v Speaker 0>So we can come back in and add

00:26:22.350 --> 00:26:23.470
<v Speaker 0>the affinity key

00:26:23.885 --> 00:26:26.445
<v Speaker 0>and say that we want pod anti affinity.

00:26:28.685 --> 00:26:31.245
<v Speaker 0>We're now going to say, thank you, Copilot,

00:26:32.845 --> 00:26:35.245
<v Speaker 0>that all of our critical applications, which is

00:26:35.245 --> 00:26:37.005
<v Speaker 0>all the apps with a label

00:26:37.279 --> 00:26:39.759
<v Speaker 0>app with value critical app a,

00:26:41.120 --> 00:26:42.719
<v Speaker 0>will be repelled

00:26:42.880 --> 00:26:44.320
<v Speaker 0>from the host name.

00:26:45.519 --> 00:26:47.839
<v Speaker 0>That means when we schedule

00:26:48.240 --> 00:26:50.080
<v Speaker 0>three of these critical applications,

00:26:50.695 --> 00:26:52.935
<v Speaker 0>we will see one on each node within

00:26:52.935 --> 00:26:53.815
<v Speaker 0>the cluster.

00:26:54.695 --> 00:26:56.375
<v Speaker 0>In fact, we'll take that a step further

00:26:56.375 --> 00:26:57.895
<v Speaker 0>and we'll request four.

00:26:58.295 --> 00:27:01.335
<v Speaker 0>Now because we have a required during scheduling,

00:27:01.575 --> 00:27:05.015
<v Speaker 0>we'll see that one critical web app is

00:27:03.910 --> 00:27:05.750
<v Speaker 0>actually gonna fail to schedule.

00:27:06.870 --> 00:27:08.870
<v Speaker 0>While I'm here, I'm also gonna copy this

00:27:08.870 --> 00:27:10.790
<v Speaker 0>affinity block for our database.

00:27:19.345 --> 00:27:22.144
<v Speaker 0>We will update the value here, the database

00:27:22.144 --> 00:27:22.784
<v Speaker 0>a,

00:27:23.504 --> 00:27:25.424
<v Speaker 0>and it's the exact same rule.

00:27:25.825 --> 00:27:28.465
<v Speaker 0>We never want to schedule two databases on

00:27:28.465 --> 00:27:30.544
<v Speaker 0>the same node because

00:27:30.169 --> 00:27:33.210
<v Speaker 0>we need these to be resilient across node

00:27:33.210 --> 00:27:34.090
<v Speaker 0>failures.

00:27:34.890 --> 00:27:37.289
<v Speaker 0>And we'll say that we want three replicas,

00:27:37.289 --> 00:27:38.729
<v Speaker 0>one on each node.

00:27:42.570 --> 00:27:44.330
<v Speaker 0>Well, let's take a look where these all

00:27:44.330 --> 00:27:44.970
<v Speaker 0>ended up.

00:27:46.945 --> 00:27:48.705
<v Speaker 0>We can see now that we have three

00:27:48.705 --> 00:27:50.065
<v Speaker 0>copies of our database,

00:27:50.784 --> 00:27:52.544
<v Speaker 0>all on three different nodes.

00:27:53.904 --> 00:27:56.385
<v Speaker 0>We can see that our critical app actually

00:27:56.385 --> 00:27:59.020
<v Speaker 0>can't schedule right now, and that's because we

00:27:59.020 --> 00:28:01.100
<v Speaker 0>are in a small cluster with pod anti

00:28:01.100 --> 00:28:01.980
<v Speaker 0>affinities.

00:28:02.380 --> 00:28:04.300
<v Speaker 0>So they're actually never gonna be able to

00:28:04.300 --> 00:28:05.020
<v Speaker 0>schedule.

00:28:07.420 --> 00:28:09.900
<v Speaker 0>We can help that along by deleting the

00:28:09.900 --> 00:28:11.740
<v Speaker 0>old replica set like so.

00:28:17.985 --> 00:28:20.144
<v Speaker 0>And now we can see our web applications

00:28:20.705 --> 00:28:22.705
<v Speaker 0>are both still scheduled on the same node.

00:28:24.780 --> 00:28:27.420
<v Speaker 0>So let's kick some and see where they

00:28:27.420 --> 00:28:27.820
<v Speaker 0>end up.

00:28:41.235 --> 00:28:43.475
<v Speaker 0>And now they're on different nodes because they

00:28:43.475 --> 00:28:44.835
<v Speaker 0>have multiple options,

00:28:44.915 --> 00:28:48.115
<v Speaker 0>because we have multiple availabilities of the database

00:28:48.115 --> 00:28:48.914
<v Speaker 0>pod.

00:28:49.875 --> 00:28:50.195
<v Speaker 0>Perfect.

00:28:51.150 --> 00:28:53.230
<v Speaker 0>And as we said, with critical apps, and

00:28:53.230 --> 00:28:55.150
<v Speaker 0>at four replicas and three nodes with the

00:28:55.150 --> 00:28:56.110
<v Speaker 0>anti affinity,

00:28:56.190 --> 00:28:58.750
<v Speaker 0>we have one pod which we'll never schedule.

00:28:59.230 --> 00:29:02.429
<v Speaker 0>So pod affinities and especially anti affinities

00:29:02.990 --> 00:29:04.830
<v Speaker 0>are great resources

00:29:06.325 --> 00:29:09.205
<v Speaker 0>for increasing the resiliency of your application in

00:29:09.205 --> 00:29:10.725
<v Speaker 0>a Kubernetes environment.

00:29:11.125 --> 00:29:12.965
<v Speaker 0>With the caveat that you have to be

00:29:12.965 --> 00:29:13.684
<v Speaker 0>aware

00:29:13.845 --> 00:29:16.245
<v Speaker 0>of those scheduling constraints and the size of

00:29:16.245 --> 00:29:17.125
<v Speaker 0>your cluster.

00:29:17.525 --> 00:29:19.125
<v Speaker 0>And a three node setup like I'm working

00:29:19.125 --> 00:29:21.045
<v Speaker 0>with right now, it's gonna be very difficult

00:29:20.850 --> 00:29:23.009
<v Speaker 0>to roll out updates to your application

00:29:23.250 --> 00:29:25.970
<v Speaker 0>because there may not be enough nodes to

00:29:25.970 --> 00:29:28.049
<v Speaker 0>satisfy those constraints.

00:29:28.770 --> 00:29:31.010
<v Speaker 0>So let's tweak this example a little bit

00:29:31.010 --> 00:29:31.570
<v Speaker 0>more.

00:29:32.770 --> 00:29:35.090
<v Speaker 0>Now we have just two deployments,

00:29:35.169 --> 00:29:37.784
<v Speaker 0>a critical web app, which we need three

00:29:37.784 --> 00:29:40.424
<v Speaker 0>of, and it has an anti affinity against

00:29:40.424 --> 00:29:41.304
<v Speaker 0>itself

00:29:41.465 --> 00:29:42.984
<v Speaker 0>and the database.

00:29:43.465 --> 00:29:44.664
<v Speaker 0>Why the database?

00:29:44.904 --> 00:29:46.745
<v Speaker 0>Well, in the case of node failure, we

00:29:46.745 --> 00:29:48.585
<v Speaker 0>don't want our database and our critical web

00:29:48.585 --> 00:29:49.465
<v Speaker 0>app to be unavailable.

00:29:50.970 --> 00:29:52.890
<v Speaker 0>We then have our database, which has an

00:29:52.890 --> 00:29:55.690
<v Speaker 0>anti affinity on itself because we need resiliency

00:29:55.690 --> 00:29:57.210
<v Speaker 0>at the database layer.

00:29:58.650 --> 00:30:01.690
<v Speaker 0>When applied together in this configuration,

00:30:01.930 --> 00:30:02.890
<v Speaker 0>we end up with this.

00:30:04.205 --> 00:30:06.765
<v Speaker 0>We have the three database pods scheduled and

00:30:06.765 --> 00:30:07.404
<v Speaker 0>running.

00:30:07.805 --> 00:30:09.565
<v Speaker 0>However, we're unable

00:30:09.565 --> 00:30:11.965
<v Speaker 0>to schedule a critical application.

00:30:12.365 --> 00:30:15.645
<v Speaker 0>Why? Because the database anti affinity with itself

00:30:15.645 --> 00:30:17.405
<v Speaker 0>spreads out across all the nodes in our

00:30:17.405 --> 00:30:17.725
<v Speaker 0>cluster.

00:30:18.430 --> 00:30:20.350
<v Speaker 0>The critical app has an anti affinity of

00:30:20.350 --> 00:30:22.590
<v Speaker 0>the database, so now it can't schedule anywhere

00:30:22.590 --> 00:30:24.510
<v Speaker 0>where the database is running, which just happens

00:30:24.510 --> 00:30:25.150
<v Speaker 0>to be

00:30:26.030 --> 00:30:26.750
<v Speaker 0>everywhere.

00:30:28.030 --> 00:30:29.710
<v Speaker 0>And we have a tool for this too

00:30:29.710 --> 00:30:31.150
<v Speaker 0>through priority

00:30:31.150 --> 00:30:31.550
<v Speaker 0>classes.

00:30:32.675 --> 00:30:34.995
<v Speaker 0>Let's jump back over to our YAML and

00:30:35.555 --> 00:30:37.554
<v Speaker 0>create a new YAML document.

00:30:40.115 --> 00:30:42.675
<v Speaker 0>We can copy an example from a documentation

00:30:42.675 --> 00:30:43.554
<v Speaker 0>like so.

00:30:44.355 --> 00:30:46.995
<v Speaker 0>Here, we're using the scheduling Kubernetes IOV one

00:30:46.995 --> 00:30:47.555
<v Speaker 0>API.

00:30:48.169 --> 00:30:50.730
<v Speaker 0>We're going to create a new priority class,

00:30:51.210 --> 00:30:52.489
<v Speaker 0>and we'll call this

00:30:52.650 --> 00:30:53.369
<v Speaker 0>critical.

00:30:55.289 --> 00:30:56.649
<v Speaker 0>We set this value

00:30:56.890 --> 00:30:58.970
<v Speaker 0>very, very high because it has to rank

00:30:58.970 --> 00:31:01.049
<v Speaker 0>higher than the other priority classes within the

00:31:01.049 --> 00:31:01.289
<v Speaker 0>system.

00:31:02.425 --> 00:31:03.705
<v Speaker 0>We do not want to use this as

00:31:03.705 --> 00:31:06.505
<v Speaker 0>a global default. All our applications are not

00:31:06.505 --> 00:31:07.225
<v Speaker 0>critical.

00:31:07.705 --> 00:31:09.305
<v Speaker 0>And then you can add a description for

00:31:09.305 --> 00:31:11.465
<v Speaker 0>any operator that comes along as curious what

00:31:11.465 --> 00:31:13.065
<v Speaker 0>this priority class is for.

00:31:14.025 --> 00:31:16.745
<v Speaker 0>There's just one other flag on this resource

00:31:16.600 --> 00:31:18.919
<v Speaker 0>that isn't covered with this first example,

00:31:20.600 --> 00:31:22.519
<v Speaker 0>and that is the preemption policy.

00:31:23.480 --> 00:31:25.320
<v Speaker 0>Now I'm gonna set it to never,

00:31:25.799 --> 00:31:27.559
<v Speaker 0>but there is another option.

00:31:28.200 --> 00:31:29.639
<v Speaker 0>How do we know what these options are?

00:31:30.784 --> 00:31:32.625
<v Speaker 0>Well, we can rely on our good friend,

00:31:32.625 --> 00:31:33.984
<v Speaker 0>kubectl explain.

00:31:34.465 --> 00:31:36.865
<v Speaker 0>With kubectl explain, you can put in any

00:31:36.865 --> 00:31:40.385
<v Speaker 0>resource name and then any member of that

00:31:40.385 --> 00:31:41.184
<v Speaker 0>spec

00:31:41.424 --> 00:31:43.985
<v Speaker 0>to get detailed documentation about how it could

00:31:43.985 --> 00:31:44.544
<v Speaker 0>be configured.

00:31:46.130 --> 00:31:47.970
<v Speaker 0>Now we can see that the preemption policy

00:31:47.970 --> 00:31:50.130
<v Speaker 0>allows us to be never or

00:31:50.530 --> 00:31:52.530
<v Speaker 0>preempt lower priority.

00:31:53.730 --> 00:31:56.290
<v Speaker 0>Now all preemption means is that when new

00:31:56.290 --> 00:31:58.530
<v Speaker 0>pods come into the queue to be scheduled,

00:31:59.535 --> 00:32:02.575
<v Speaker 0>we can take a look at the value

00:32:02.575 --> 00:32:04.255
<v Speaker 0>of the priority class

00:32:04.655 --> 00:32:07.055
<v Speaker 0>to understand if they should skip the queue.

00:32:09.295 --> 00:32:12.175
<v Speaker 0>If we leave this as a default

00:32:11.950 --> 00:32:14.510
<v Speaker 0>when a critical pod needs to be scheduled,

00:32:14.670 --> 00:32:16.910
<v Speaker 0>plus there's 12 pods in front of it,

00:32:16.990 --> 00:32:19.630
<v Speaker 0>it's likely the scheduler will say, hey. I

00:32:19.630 --> 00:32:21.550
<v Speaker 0>have to go schedule this one first.

00:32:21.790 --> 00:32:23.550
<v Speaker 0>Kinda like being friends with a bouncer in

00:32:23.550 --> 00:32:24.110
<v Speaker 0>an eight clip.

00:32:25.455 --> 00:32:26.975
<v Speaker 0>And if you don't want this behavior in

00:32:26.975 --> 00:32:27.774
<v Speaker 0>your cluster,

00:32:28.495 --> 00:32:30.014
<v Speaker 0>set the policy to never,

00:32:30.415 --> 00:32:32.815
<v Speaker 0>and pods of this class will never skip

00:32:32.815 --> 00:32:33.615
<v Speaker 0>the queue.

00:32:36.735 --> 00:32:40.090
<v Speaker 0>So let's add this priority class to our

00:32:40.090 --> 00:32:40.969
<v Speaker 0>deployment.

00:32:41.529 --> 00:32:44.409
<v Speaker 0>We pop down to the pod spec where

00:32:46.330 --> 00:32:49.210
<v Speaker 0>we can set the priority class name to

00:32:49.210 --> 00:32:50.009
<v Speaker 0>critical.

00:32:50.649 --> 00:32:52.490
<v Speaker 0>So let's set up a scenario where we

00:32:52.490 --> 00:32:53.370
<v Speaker 0>can see that in action.

00:32:54.195 --> 00:32:55.715
<v Speaker 0>We'll take off the never so we can

00:32:55.715 --> 00:32:57.635
<v Speaker 0>get the default, which is to preempt the

00:32:57.635 --> 00:32:59.234
<v Speaker 0>pods ahead of the queue.

00:33:01.475 --> 00:33:04.114
<v Speaker 0>I'm going to comment out a critical app

00:33:06.115 --> 00:33:08.034
<v Speaker 0>and scale our database

00:33:08.995 --> 00:33:09.635
<v Speaker 0>to six.

00:33:10.250 --> 00:33:10.969
<v Speaker 0>Now

00:33:11.210 --> 00:33:12.969
<v Speaker 0>we can run get pods.

00:33:14.009 --> 00:33:16.169
<v Speaker 0>We'll see that we have three database pods

00:33:16.169 --> 00:33:19.049
<v Speaker 0>pending and three database pods running.

00:33:20.009 --> 00:33:22.730
<v Speaker 0>Currently, we have three pods and the queue

00:33:22.730 --> 00:33:23.610
<v Speaker 0>to be scheduled.

00:33:27.605 --> 00:33:29.924
<v Speaker 0>Let's bring back our priority class

00:33:30.485 --> 00:33:32.645
<v Speaker 0>and our critical application.

00:33:35.205 --> 00:33:38.245
<v Speaker 0>We make sure that our priority class name

00:33:38.245 --> 00:33:40.960
<v Speaker 0>is set on our deployment object.

00:33:44.720 --> 00:33:47.040
<v Speaker 0>Now when we apply these and run get

00:33:47.040 --> 00:33:47.759
<v Speaker 0>pods,

00:33:49.040 --> 00:33:50.879
<v Speaker 0>we'll see that our critical application

00:33:52.000 --> 00:33:53.440
<v Speaker 0>didn't just skip the queue,

00:33:54.615 --> 00:33:57.975
<v Speaker 0>It evicted the other database pods.

00:33:59.655 --> 00:34:01.655
<v Speaker 0>Priority classes give you a lot of power

00:34:01.655 --> 00:34:03.655
<v Speaker 0>when it comes to scheduling workloads within your

00:34:03.655 --> 00:34:04.375
<v Speaker 0>cluster.

00:34:05.015 --> 00:34:06.935
<v Speaker 0>Just be careful you don't blow away other

00:34:06.935 --> 00:34:08.614
<v Speaker 0>important aspects at the same time.

00:34:09.869 --> 00:34:10.590
<v Speaker 0>Alright.

00:34:10.750 --> 00:34:13.549
<v Speaker 0>Let's kick things up a little bit and

00:34:13.549 --> 00:34:16.989
<v Speaker 0>start playing with some more advanced scheduling techniques.

00:34:17.710 --> 00:34:20.190
<v Speaker 0>So there's a relatively new feature in Kubernetes

00:34:20.190 --> 00:34:22.029
<v Speaker 0>called topology spread constraints.

00:34:22.914 --> 00:34:24.994
<v Speaker 0>Try saying that three times fast.

00:34:26.194 --> 00:34:28.835
<v Speaker 0>Topology spread constraints allow us

00:34:29.074 --> 00:34:30.755
<v Speaker 0>to scale up our application

00:34:31.554 --> 00:34:33.714
<v Speaker 0>while maintaining even distribution

00:34:33.795 --> 00:34:35.234
<v Speaker 0>across a dimension.

00:34:36.270 --> 00:34:39.550
<v Speaker 0>That dimension could be region, host, architecture,

00:34:39.550 --> 00:34:40.750
<v Speaker 0>whatever you want,

00:34:41.230 --> 00:34:43.790
<v Speaker 0>and we have options to configure the maximum

00:34:43.790 --> 00:34:45.950
<v Speaker 0>skew for that even distribution.

00:34:46.429 --> 00:34:49.070
<v Speaker 0>Mean and allow meaning that we allow

00:34:49.150 --> 00:34:50.190
<v Speaker 0>a certain deviation

00:34:50.864 --> 00:34:53.425
<v Speaker 0>of that spread across the dimension

00:34:53.824 --> 00:34:55.505
<v Speaker 0>based on what we configure.

00:34:56.304 --> 00:34:57.905
<v Speaker 0>Now I know that all sounds like blah

00:34:57.905 --> 00:34:59.345
<v Speaker 0>blah blah blah blah,

00:34:59.825 --> 00:35:01.744
<v Speaker 0>so we're gonna walk through an example of

00:35:01.744 --> 00:35:04.545
<v Speaker 0>that step by step to really help solidify

00:35:04.545 --> 00:35:07.460
<v Speaker 0>what spread topology constraints are.

00:35:09.140 --> 00:35:11.220
<v Speaker 0>So I set up a situation

00:35:13.700 --> 00:35:16.900
<v Speaker 0>where we have three database pods and three

00:35:16.900 --> 00:35:18.260
<v Speaker 0>critical application pods.

00:35:18.755 --> 00:35:21.235
<v Speaker 0>We still have the anti affinity, so the

00:35:21.235 --> 00:35:23.235
<v Speaker 0>critical app cannot be scheduled to anywhere there

00:35:23.235 --> 00:35:24.995
<v Speaker 0>is a database pod.

00:35:26.035 --> 00:35:27.875
<v Speaker 0>If we take a look at the YAML,

00:35:28.915 --> 00:35:31.395
<v Speaker 0>we've added our topology spread constraint.

00:35:32.120 --> 00:35:33.800
<v Speaker 0>Now we're gonna come back to that in

00:35:33.800 --> 00:35:34.680
<v Speaker 0>just a minute.

00:35:34.920 --> 00:35:36.760
<v Speaker 0>The first thing I want to do is

00:35:36.760 --> 00:35:38.280
<v Speaker 0>scale down our database,

00:35:38.360 --> 00:35:40.360
<v Speaker 0>giving us a little bit of room to

00:35:40.360 --> 00:35:41.240
<v Speaker 0>deploy

00:35:41.240 --> 00:35:42.840
<v Speaker 0>a critical application

00:35:42.840 --> 00:35:43.320
<v Speaker 0>pod.

00:35:47.345 --> 00:35:49.425
<v Speaker 0>Now what's important to hear,

00:35:50.224 --> 00:35:52.305
<v Speaker 0>there is no anti affinity

00:35:53.025 --> 00:35:55.265
<v Speaker 0>on a critical application.

00:35:55.984 --> 00:35:59.424
<v Speaker 0>We only specify anti affinity against the database.

00:36:00.190 --> 00:36:02.510
<v Speaker 0>So you'd maybe expect that we're able to

00:36:02.510 --> 00:36:05.150
<v Speaker 0>deploy all three critical application pods

00:36:05.390 --> 00:36:06.110
<v Speaker 0>to

00:36:06.430 --> 00:36:09.070
<v Speaker 0>the one zone which we're using as a

00:36:09.070 --> 00:36:10.110
<v Speaker 0>topology key.

00:36:10.670 --> 00:36:13.070
<v Speaker 0>Now because we have the spread topology constraint,

00:36:13.505 --> 00:36:15.745
<v Speaker 0>this actually cannot be scheduled,

00:36:15.825 --> 00:36:16.865
<v Speaker 0>specifically

00:36:17.185 --> 00:36:19.745
<v Speaker 0>because we say that if we cannot satisfy

00:36:19.745 --> 00:36:20.785
<v Speaker 0>this constraint,

00:36:20.865 --> 00:36:24.065
<v Speaker 0>we don't want to schedule any new pods.

00:36:25.265 --> 00:36:26.625
<v Speaker 0>So let's work through

00:36:27.105 --> 00:36:28.785
<v Speaker 0>the topology spread configuration.

00:36:29.930 --> 00:36:31.450
<v Speaker 0>The first thing we do is we apply

00:36:31.450 --> 00:36:32.730
<v Speaker 0>a label selector.

00:36:32.970 --> 00:36:35.610
<v Speaker 0>So you could use this to enforce even

00:36:35.610 --> 00:36:36.490
<v Speaker 0>distribution

00:36:36.810 --> 00:36:40.330
<v Speaker 0>of multiple deployments, replica sets, stateful sets, etcetera,

00:36:40.330 --> 00:36:43.610
<v Speaker 0>any pod from any higher level component

00:36:45.055 --> 00:36:47.375
<v Speaker 0>if you need to enforce even distribution.

00:36:48.495 --> 00:36:50.735
<v Speaker 0>You can set the topology key.

00:36:51.455 --> 00:36:53.295
<v Speaker 0>Here, we're using host name.

00:36:53.695 --> 00:36:55.295
<v Speaker 0>So what I'm saying is that when I

00:36:55.295 --> 00:36:58.580
<v Speaker 0>schedule pods with app critical app a, I

00:36:58.580 --> 00:37:01.780
<v Speaker 0>want even distribution with every node across every

00:37:01.780 --> 00:37:03.220
<v Speaker 0>node in my cluster.

00:37:03.700 --> 00:37:05.620
<v Speaker 0>And the max skew of one means I'm

00:37:05.620 --> 00:37:06.980
<v Speaker 0>only willing to accept

00:37:07.220 --> 00:37:08.340
<v Speaker 0>one host,

00:37:08.340 --> 00:37:09.380
<v Speaker 0>one node,

00:37:09.620 --> 00:37:10.900
<v Speaker 0>and one pod

00:37:11.540 --> 00:37:13.220
<v Speaker 0>above the even distribution.

00:37:15.305 --> 00:37:16.505
<v Speaker 0>Does that make sense?

00:37:17.865 --> 00:37:19.465
<v Speaker 0>Let's break it down a bit more.

00:37:22.105 --> 00:37:24.424
<v Speaker 0>Let's set the max skew to two.

00:37:29.860 --> 00:37:32.500
<v Speaker 0>When we run get pods, we'll see that

00:37:32.500 --> 00:37:35.860
<v Speaker 0>we have two critical apps now being scheduled,

00:37:36.180 --> 00:37:37.860
<v Speaker 0>and they're both going to be on the

00:37:37.860 --> 00:37:39.140
<v Speaker 0>same machine,

00:37:39.540 --> 00:37:42.485
<v Speaker 0>f zed f, the only node that doesn't

00:37:42.485 --> 00:37:44.805
<v Speaker 0>have a database pod and doesn't violate the

00:37:44.805 --> 00:37:45.845
<v Speaker 0>anti affinity.

00:37:46.885 --> 00:37:49.365
<v Speaker 0>Now we don't have even distribution because we

00:37:49.365 --> 00:37:50.565
<v Speaker 0>have three nodes,

00:37:50.565 --> 00:37:52.645
<v Speaker 0>but we have two pods on one node.

00:37:52.965 --> 00:37:56.140
<v Speaker 0>Even distribution would mean one pod or one

00:37:56.140 --> 00:37:57.500
<v Speaker 0>pod on each node.

00:37:57.740 --> 00:37:59.340
<v Speaker 0>And with a skew of one, meaning maybe

00:37:59.340 --> 00:38:01.660
<v Speaker 0>one node could push to two.

00:38:03.660 --> 00:38:05.900
<v Speaker 0>I know it can be a little complicated.

00:38:07.260 --> 00:38:09.580
<v Speaker 0>So let's set the skew back to one

00:38:11.255 --> 00:38:13.575
<v Speaker 0>and drop our database to one.

00:38:20.455 --> 00:38:21.895
<v Speaker 0>Now when we run get pods,

00:38:23.220 --> 00:38:24.660
<v Speaker 0>we have two

00:38:25.620 --> 00:38:26.820
<v Speaker 0>being scheduled,

00:38:27.540 --> 00:38:29.860
<v Speaker 0>and even distribution will be maintained.

00:38:30.100 --> 00:38:31.940
<v Speaker 0>We're going to see one of these on

00:38:31.940 --> 00:38:33.300
<v Speaker 0>each pod,

00:38:33.460 --> 00:38:35.300
<v Speaker 0>each node that is available that doesn't have

00:38:35.300 --> 00:38:36.180
<v Speaker 0>a database.

00:38:37.300 --> 00:38:40.355
<v Speaker 0>So let's take the database out altogether now

00:38:44.115 --> 00:38:45.235
<v Speaker 0>and apply.

00:38:48.435 --> 00:38:49.075
<v Speaker 0>Now

00:38:49.235 --> 00:38:50.755
<v Speaker 0>we have one critical application

00:38:52.089 --> 00:38:54.490
<v Speaker 0>across each node. We have complete

00:38:54.569 --> 00:38:57.610
<v Speaker 0>even distribution right now, meaning

00:38:57.690 --> 00:39:00.010
<v Speaker 0>one on each of the dimension, the host

00:39:00.010 --> 00:39:00.650
<v Speaker 0>name.

00:39:01.210 --> 00:39:03.530
<v Speaker 0>With a skew of one, we can actually

00:39:03.530 --> 00:39:04.730
<v Speaker 0>scale this

00:39:05.290 --> 00:39:05.930
<v Speaker 0>to four,

00:39:09.155 --> 00:39:11.315
<v Speaker 0>and that will be okay. And in fact,

00:39:11.315 --> 00:39:13.155
<v Speaker 0>we can push this to 20

00:39:14.755 --> 00:39:15.875
<v Speaker 0>and apply.

00:39:17.395 --> 00:39:19.155
<v Speaker 0>And this will continue to work

00:39:19.850 --> 00:39:22.490
<v Speaker 0>because as long as the scheduler can have

00:39:22.490 --> 00:39:23.770
<v Speaker 0>even distribution,

00:39:24.890 --> 00:39:26.490
<v Speaker 0>meaning it can schedule

00:39:27.210 --> 00:39:29.450
<v Speaker 0>ubiquitously across the dimension,

00:39:30.090 --> 00:39:31.210
<v Speaker 0>we have no problems.

00:39:33.285 --> 00:39:34.965
<v Speaker 0>So let's bring back

00:39:35.444 --> 00:39:36.965
<v Speaker 0>our database node

00:39:38.885 --> 00:39:39.845
<v Speaker 0>with one.

00:39:45.605 --> 00:39:47.605
<v Speaker 0>We're going to delete our deployment.

00:39:53.580 --> 00:39:54.460
<v Speaker 0>Like so.

00:39:55.500 --> 00:39:57.260
<v Speaker 0>This puts us back in a position

00:39:57.500 --> 00:39:59.500
<v Speaker 0>where we have one database

00:39:59.500 --> 00:40:01.020
<v Speaker 0>once it finally schedules.

00:40:02.385 --> 00:40:03.345
<v Speaker 0>There we go.

00:40:04.465 --> 00:40:06.225
<v Speaker 0>And now we're gonna bring back

00:40:07.025 --> 00:40:09.345
<v Speaker 0>our topology spread aware deployment.

00:40:12.385 --> 00:40:14.225
<v Speaker 0>So we'll set this to three

00:40:15.345 --> 00:40:16.065
<v Speaker 0>and apply.

00:40:22.040 --> 00:40:24.200
<v Speaker 0>You'll see that one fails to schedule.

00:40:26.120 --> 00:40:27.880
<v Speaker 0>Why does it work this way?

00:40:28.520 --> 00:40:31.240
<v Speaker 0>Well, if we're if we're able to schedule

00:40:31.240 --> 00:40:33.800
<v Speaker 0>evenly across the dimension, which is host name

00:40:33.800 --> 00:40:34.040
<v Speaker 0>here,

00:40:34.655 --> 00:40:36.255
<v Speaker 0>and we have the ability to schedule on

00:40:36.255 --> 00:40:37.535
<v Speaker 0>all the host names,

00:40:39.535 --> 00:40:41.215
<v Speaker 0>then the skew never

00:40:41.215 --> 00:40:43.295
<v Speaker 0>goes above zero and you can scale the

00:40:43.295 --> 00:40:44.895
<v Speaker 0>workload as much as you want.

00:40:45.935 --> 00:40:47.135
<v Speaker 0>When you lose access

00:40:47.900 --> 00:40:49.580
<v Speaker 0>to one of the nodes on the dimension

00:40:49.580 --> 00:40:51.740
<v Speaker 0>or the node in this dimension,

00:40:52.140 --> 00:40:54.540
<v Speaker 0>we're going to then have uneven distribution

00:40:54.540 --> 00:40:56.220
<v Speaker 0>across the dimension,

00:40:57.420 --> 00:41:00.060
<v Speaker 0>which means this skew becomes very important.

00:41:01.164 --> 00:41:02.605
<v Speaker 0>Now, of course, I could say that I

00:41:02.605 --> 00:41:03.964
<v Speaker 0>want a skew of 10

00:41:04.204 --> 00:41:06.045
<v Speaker 0>and I can scale this deployment back up

00:41:06.045 --> 00:41:07.805
<v Speaker 0>as much as I want because we can

00:41:07.805 --> 00:41:11.244
<v Speaker 0>accept 10 nodes across breaking the deviation.

00:41:12.525 --> 00:41:14.684
<v Speaker 0>But it's likely that you want even distribution

00:41:14.684 --> 00:41:15.404
<v Speaker 0>for a reason.

00:41:16.580 --> 00:41:18.340
<v Speaker 0>We'll apply it back to one.

00:41:19.860 --> 00:41:22.580
<v Speaker 0>So when should you use topology spread constraint?

00:41:22.580 --> 00:41:25.220
<v Speaker 0>Well, it's kind of an evolution of anti

00:41:25.220 --> 00:41:25.940
<v Speaker 0>affinity.

00:41:27.300 --> 00:41:29.140
<v Speaker 0>Let's assume that you want to decrease the

00:41:29.140 --> 00:41:29.700
<v Speaker 0>latency

00:41:30.555 --> 00:41:33.835
<v Speaker 0>or minimize egress traffic across zones and regions

00:41:33.835 --> 00:41:35.035
<v Speaker 0>within your cluster

00:41:35.515 --> 00:41:37.435
<v Speaker 0>or latency to your customers.

00:41:38.954 --> 00:41:41.355
<v Speaker 0>You could use anti affinity to ensure that

00:41:41.355 --> 00:41:44.075
<v Speaker 0>you schedule the pods in different regions and

00:41:44.075 --> 00:41:44.474
<v Speaker 0>zones,

00:41:46.440 --> 00:41:49.000
<v Speaker 0>But the the anti affinity means that you

00:41:49.000 --> 00:41:52.200
<v Speaker 0>can't then continue to scale up those pods

00:41:52.200 --> 00:41:55.240
<v Speaker 0>within that region without adding new nodes.

00:41:57.320 --> 00:41:59.080
<v Speaker 0>Spread topology constraints

00:41:59.495 --> 00:42:01.255
<v Speaker 0>mean that we can continue to scale up

00:42:01.255 --> 00:42:02.215
<v Speaker 0>a workload

00:42:02.455 --> 00:42:04.615
<v Speaker 0>while ensuring the even distribution.

00:42:06.055 --> 00:42:07.415
<v Speaker 0>We could be in a position where we

00:42:07.415 --> 00:42:10.295
<v Speaker 0>want 20 or 50 replicas of a pod.

00:42:12.300 --> 00:42:15.100
<v Speaker 0>This could all be scheduled on one machine.

00:42:15.820 --> 00:42:17.900
<v Speaker 0>So we add the anti affinity, and now

00:42:17.900 --> 00:42:19.340
<v Speaker 0>we need 50 machines.

00:42:19.660 --> 00:42:21.660
<v Speaker 0>But, actually, what we really want to do

00:42:21.740 --> 00:42:23.820
<v Speaker 0>is just ensure that we have enough

00:42:23.900 --> 00:42:24.860
<v Speaker 0>resiliency,

00:42:24.860 --> 00:42:26.940
<v Speaker 0>redundancy, and performance benefit

00:42:27.215 --> 00:42:30.335
<v Speaker 0>by maintaining even distribution across the zones and

00:42:30.335 --> 00:42:31.215
<v Speaker 0>the regions.

00:42:32.175 --> 00:42:33.615
<v Speaker 0>So we have the ability

00:42:35.695 --> 00:42:38.975
<v Speaker 0>to do anti affinity in a controlled manner

00:42:39.630 --> 00:42:41.070
<v Speaker 0>without the ceiling

00:42:41.549 --> 00:42:44.589
<v Speaker 0>on the scale or without our scale being

00:42:44.589 --> 00:42:46.670
<v Speaker 0>bound by the number of nodes,

00:42:47.390 --> 00:42:48.750
<v Speaker 0>and that's pretty cool.

00:42:49.069 --> 00:42:49.790
<v Speaker 0>Alright.

00:42:50.430 --> 00:42:52.805
<v Speaker 0>Let's kick things up again.

00:42:53.125 --> 00:42:56.885
<v Speaker 0>This time, we're venturing so far into new.

00:42:58.244 --> 00:43:00.885
<v Speaker 0>This stuff is one twenty six alpha. However,

00:43:01.765 --> 00:43:05.045
<v Speaker 0>this is some new, cool scheduling features coming

00:43:05.045 --> 00:43:06.005
<v Speaker 0>to Kubernetes

00:43:06.005 --> 00:43:06.565
<v Speaker 0>soon.

00:43:07.980 --> 00:43:11.260
<v Speaker 0>Now I can't use my shiny managed GKE

00:43:11.260 --> 00:43:14.300
<v Speaker 0>cluster for this demo because we do need

00:43:14.300 --> 00:43:15.100
<v Speaker 0>to enable

00:43:16.060 --> 00:43:18.540
<v Speaker 0>a feature gate on the Kube API server.

00:43:19.815 --> 00:43:22.454
<v Speaker 0>If we go into manifest Kube API server,

00:43:22.454 --> 00:43:24.775
<v Speaker 0>you'll see that on the feature gate line

00:43:25.175 --> 00:43:25.895
<v Speaker 0>here,

00:43:26.694 --> 00:43:28.935
<v Speaker 0>I have enabled a feature gate called pod

00:43:28.935 --> 00:43:29.895
<v Speaker 0>scheduling

00:43:29.895 --> 00:43:30.695
<v Speaker 0>readiness.

00:43:31.895 --> 00:43:33.095
<v Speaker 0>What does this mean?

00:43:33.575 --> 00:43:34.935
<v Speaker 0>Well, it means I can have a pod

00:43:34.935 --> 00:43:35.335
<v Speaker 0>spec,

00:43:36.369 --> 00:43:38.690
<v Speaker 0>which has scheduling gates.

00:43:39.730 --> 00:43:41.250
<v Speaker 0>This is for situations

00:43:41.250 --> 00:43:44.530
<v Speaker 0>where you want to create pods,

00:43:44.609 --> 00:43:46.930
<v Speaker 0>but they're not ready to be scheduled yet.

00:43:48.275 --> 00:43:50.835
<v Speaker 0>This could be very useful for blue green

00:43:50.835 --> 00:43:52.755
<v Speaker 0>and canary style deployments,

00:43:52.995 --> 00:43:55.475
<v Speaker 0>where we wanna start pushing resources around, but

00:43:55.475 --> 00:43:57.395
<v Speaker 0>we need to get their scheduling.

00:43:57.955 --> 00:43:58.595
<v Speaker 0>Why?

00:43:58.995 --> 00:43:59.715
<v Speaker 0>Well,

00:43:59.875 --> 00:44:02.755
<v Speaker 0>when you have unschedulable pods for whatever reason,

00:44:03.530 --> 00:44:06.250
<v Speaker 0>that can trigger your cluster autoscaler,

00:44:06.410 --> 00:44:07.849
<v Speaker 0>which can spin up a whole bunch of

00:44:07.849 --> 00:44:09.610
<v Speaker 0>new nodes unnecessarily

00:44:09.690 --> 00:44:11.690
<v Speaker 0>just because you created a pod that could

00:44:11.690 --> 00:44:12.890
<v Speaker 0>never be scheduled,

00:44:12.970 --> 00:44:14.570
<v Speaker 0>and it can't be scheduled because it's not

00:44:14.570 --> 00:44:15.849
<v Speaker 0>ready to be scheduled yet.

00:44:16.565 --> 00:44:18.725
<v Speaker 0>This is where scheduling gates come in.

00:44:19.845 --> 00:44:21.285
<v Speaker 0>So let's apply

00:44:21.285 --> 00:44:22.885
<v Speaker 0>this to our cluster

00:44:23.205 --> 00:44:24.885
<v Speaker 0>and run get pods.

00:44:26.805 --> 00:44:29.285
<v Speaker 0>We'll see here that our pod is pending,

00:44:30.510 --> 00:44:32.510
<v Speaker 0>and we can describe our pod

00:44:36.430 --> 00:44:37.870
<v Speaker 0>and we'll see a nice little message at

00:44:37.870 --> 00:44:39.870
<v Speaker 0>the bottom saying that we can't schedule this

00:44:39.870 --> 00:44:42.990
<v Speaker 0>because we have a non empty scheduling gates.

00:44:45.275 --> 00:44:46.555
<v Speaker 0>We can come in

00:44:47.595 --> 00:44:48.955
<v Speaker 0>and remove a gate.

00:44:52.474 --> 00:44:53.435
<v Speaker 0>Wow.

00:44:53.755 --> 00:44:55.355
<v Speaker 0>You just see me edit a pod.

00:44:56.420 --> 00:44:57.780
<v Speaker 0>Typically, pods

00:44:57.780 --> 00:44:59.540
<v Speaker 0>are extremely immutable.

00:45:00.020 --> 00:45:02.500
<v Speaker 0>However, with scheduling gates, we do have some

00:45:02.500 --> 00:45:04.180
<v Speaker 0>mutability on the spec.

00:45:04.900 --> 00:45:08.100
<v Speaker 0>Though it's not ever really a situation where

00:45:08.100 --> 00:45:09.620
<v Speaker 0>you should be manually

00:45:09.700 --> 00:45:11.620
<v Speaker 0>modifying these scheduling gates,

00:45:12.795 --> 00:45:15.355
<v Speaker 0>This should be a controller within your cluster.

00:45:16.395 --> 00:45:18.474
<v Speaker 0>You should be pushing these pods or these

00:45:18.474 --> 00:45:21.035
<v Speaker 0>deployments replica sets should be creating the pods

00:45:21.035 --> 00:45:22.715
<v Speaker 0>with the scheduling gates.

00:45:22.875 --> 00:45:25.194
<v Speaker 0>And your custom controller, whether that be for

00:45:25.194 --> 00:45:28.550
<v Speaker 0>blue green, canary, perhaps it's a phased rollout

00:45:28.550 --> 00:45:30.470
<v Speaker 0>across new zones or regions,

00:45:30.710 --> 00:45:31.510
<v Speaker 0>whatever

00:45:33.190 --> 00:45:34.630
<v Speaker 0>your reasoning is,

00:45:35.110 --> 00:45:36.230
<v Speaker 0>your controller

00:45:36.870 --> 00:45:38.550
<v Speaker 0>can monitor those gates

00:45:38.630 --> 00:45:40.310
<v Speaker 0>and remove them as required.

00:45:40.790 --> 00:45:42.674
<v Speaker 0>So then that we've removed the scheduling gates,

00:45:42.674 --> 00:45:44.595
<v Speaker 0>we can reapply the pod and it will

00:45:44.595 --> 00:45:45.474
<v Speaker 0>be scheduled

00:45:46.115 --> 00:45:47.075
<v Speaker 0>as normal.

00:45:48.755 --> 00:45:51.395
<v Speaker 0>So this isn't a world changing feature,

00:45:51.795 --> 00:45:53.795
<v Speaker 0>but it does open up new levels of

00:45:53.795 --> 00:45:55.954
<v Speaker 0>automation in cluster web controllers,

00:45:56.910 --> 00:45:59.150
<v Speaker 0>and I'm very excited to see what tooling

00:45:59.150 --> 00:46:00.830
<v Speaker 0>does with this new feature.

00:46:01.630 --> 00:46:02.430
<v Speaker 0>Alright.

00:46:02.910 --> 00:46:04.910
<v Speaker 0>We're gonna kick it up one more time,

00:46:05.549 --> 00:46:07.630
<v Speaker 0>and we're gonna straight back in to alpha

00:46:07.630 --> 00:46:09.390
<v Speaker 0>features of 01/26.

00:46:10.255 --> 00:46:12.815
<v Speaker 0>This is dynamic resource allocation.

00:46:13.615 --> 00:46:15.615
<v Speaker 0>What is dynamic resource allocation?

00:46:15.695 --> 00:46:16.335
<v Speaker 0>Well,

00:46:16.974 --> 00:46:19.375
<v Speaker 0>let's understand everything we've covered so far.

00:46:20.175 --> 00:46:21.135
<v Speaker 0>Scheduler,

00:46:21.135 --> 00:46:23.055
<v Speaker 0>at least in the earliest days, only allows

00:46:23.055 --> 00:46:25.750
<v Speaker 0>you only allowed you to schedule based on

00:46:25.750 --> 00:46:27.110
<v Speaker 0>CPU and memory.

00:46:28.150 --> 00:46:29.990
<v Speaker 0>Over time, that wasn't good enough for people

00:46:29.990 --> 00:46:31.430
<v Speaker 0>deploying to Kubernetes,

00:46:31.910 --> 00:46:35.190
<v Speaker 0>and new options were added. The ability to

00:46:35.190 --> 00:46:36.950
<v Speaker 0>schedule based on the hardware available,

00:46:37.505 --> 00:46:38.945
<v Speaker 0>network topologies,

00:46:39.185 --> 00:46:40.145
<v Speaker 0>availability

00:46:40.785 --> 00:46:42.625
<v Speaker 0>of volumes, and so forth.

00:46:43.505 --> 00:46:45.825
<v Speaker 0>Well, these are all baked pretty much straight

00:46:45.825 --> 00:46:47.425
<v Speaker 0>into Kubernetes itself.

00:46:48.705 --> 00:46:51.025
<v Speaker 0>Dynamic resource allocation

00:46:51.540 --> 00:46:54.740
<v Speaker 0>allows us to dynamically define new things that

00:46:54.740 --> 00:46:56.340
<v Speaker 0>can be claimed.

00:46:57.780 --> 00:46:59.860
<v Speaker 0>Now the examples focus on

00:47:00.340 --> 00:47:01.300
<v Speaker 0>hardware

00:47:01.380 --> 00:47:02.820
<v Speaker 0>as part of the node,

00:47:03.300 --> 00:47:04.180
<v Speaker 0>like a GPU.

00:47:05.315 --> 00:47:07.555
<v Speaker 0>I actually think the use case for this

00:47:07.555 --> 00:47:08.915
<v Speaker 0>will go much further,

00:47:09.315 --> 00:47:11.235
<v Speaker 0>potentially out to claiming

00:47:11.715 --> 00:47:12.835
<v Speaker 0>cloud resources

00:47:12.835 --> 00:47:15.875
<v Speaker 0>or maybe locks on topics on Red Panda.

00:47:17.395 --> 00:47:20.099
<v Speaker 0>Now it's very early, but I'm gonna show

00:47:20.099 --> 00:47:21.780
<v Speaker 0>you what I've been able to put together

00:47:22.099 --> 00:47:24.180
<v Speaker 0>for dynamic resource allocation.

00:47:26.579 --> 00:47:29.060
<v Speaker 0>Now, of course, when you're venturing in

00:47:29.619 --> 00:47:32.125
<v Speaker 0>to the wilds of alpha features, we have

00:47:32.125 --> 00:47:34.445
<v Speaker 0>a feature gate called dynamic resource allocation as

00:47:34.445 --> 00:47:37.165
<v Speaker 0>well as a runtime config to enable a

00:47:37.165 --> 00:47:40.285
<v Speaker 0>new API group of resource.kates.io.

00:47:41.405 --> 00:47:43.645
<v Speaker 0>For this example, I did have to provide

00:47:43.645 --> 00:47:45.245
<v Speaker 0>my own CRD,

00:47:46.080 --> 00:47:48.000
<v Speaker 0>And I'm kind of leaning on the example

00:47:48.000 --> 00:47:50.160
<v Speaker 0>from the KEP here, which talks about claiming

00:47:50.160 --> 00:47:50.960
<v Speaker 0>cats,

00:47:51.760 --> 00:47:53.600
<v Speaker 0>only there wasn't enough code in the KEP

00:47:53.600 --> 00:47:55.360
<v Speaker 0>for it to actually deploy,

00:47:55.440 --> 00:47:56.800
<v Speaker 0>so I had to fix that.

00:47:57.440 --> 00:47:59.280
<v Speaker 0>So here we have a custom resource definition

00:47:59.280 --> 00:48:00.880
<v Speaker 0>that describes a cat, and we have a

00:48:00.880 --> 00:48:02.484
<v Speaker 0>couple of properties, one properties. One being the

00:48:02.484 --> 00:48:04.484
<v Speaker 0>size of the cat and the color of

00:48:04.484 --> 00:48:05.285
<v Speaker 0>the cat.

00:48:05.845 --> 00:48:07.445
<v Speaker 0>And this is now available

00:48:07.445 --> 00:48:08.885
<v Speaker 0>within our cluster

00:48:09.685 --> 00:48:10.885
<v Speaker 0>of a grep

00:48:11.285 --> 00:48:12.245
<v Speaker 0>for cat.

00:48:13.045 --> 00:48:13.525
<v Speaker 0>Ta da.

00:48:16.120 --> 00:48:17.000
<v Speaker 0>Next up,

00:48:17.160 --> 00:48:17.880
<v Speaker 0>we have

00:48:18.360 --> 00:48:19.720
<v Speaker 0>resource.YAML.

00:48:20.120 --> 00:48:21.480
<v Speaker 0>Now the first thing you need to do

00:48:21.480 --> 00:48:22.920
<v Speaker 0>is provide a driver.

00:48:23.000 --> 00:48:25.000
<v Speaker 0>This is something that is responsible

00:48:25.000 --> 00:48:28.440
<v Speaker 0>for providing or allocating the claim

00:48:28.385 --> 00:48:30.305
<v Speaker 0>and then removing it when a claim is

00:48:30.305 --> 00:48:31.345
<v Speaker 0>no longer needed.

00:48:31.984 --> 00:48:34.145
<v Speaker 0>So here, we're just making up a driver

00:48:34.145 --> 00:48:36.545
<v Speaker 0>name of resourcedriver.example.com,

00:48:36.545 --> 00:48:38.145
<v Speaker 0>and we have a resource class to back

00:48:38.145 --> 00:48:38.625
<v Speaker 0>them.

00:48:40.545 --> 00:48:41.984
<v Speaker 0>Next, I'm creating a cat.

00:48:42.590 --> 00:48:44.990
<v Speaker 0>So I'm using the custom resource definition I

00:48:44.990 --> 00:48:47.150
<v Speaker 0>put together to describe a cat, which is

00:48:47.150 --> 00:48:48.590
<v Speaker 0>large and black.

00:48:50.110 --> 00:48:52.830
<v Speaker 0>Then we have a resource claim template.

00:48:53.550 --> 00:48:54.990
<v Speaker 0>Much like a PVC,

00:48:55.070 --> 00:48:56.910
<v Speaker 0>here I'm saying that I want to be

00:48:56.910 --> 00:48:57.630
<v Speaker 0>able to claim

00:48:58.245 --> 00:49:01.205
<v Speaker 0>something of this resource class name with these

00:49:01.205 --> 00:49:02.085
<v Speaker 0>parameters.

00:49:02.645 --> 00:49:04.965
<v Speaker 0>So I'm providing a template for someone who

00:49:04.965 --> 00:49:07.125
<v Speaker 0>wants to claim my cat.

00:49:08.645 --> 00:49:10.885
<v Speaker 0>So we can now apply this to my

00:49:10.885 --> 00:49:12.085
<v Speaker 0>cluster like so.

00:49:13.619 --> 00:49:15.940
<v Speaker 0>Now I can run kubectl get

00:49:16.099 --> 00:49:16.980
<v Speaker 0>cats,

00:49:17.059 --> 00:49:19.220
<v Speaker 0>and I'll see my large black cat.

00:49:19.619 --> 00:49:21.619
<v Speaker 0>I can also run resource

00:49:21.619 --> 00:49:22.580
<v Speaker 0>claim

00:49:23.299 --> 00:49:24.260
<v Speaker 0>template.

00:49:25.140 --> 00:49:26.900
<v Speaker 0>My cat can be claimed,

00:49:27.525 --> 00:49:30.405
<v Speaker 0>and we're just waiting for the first consumer

00:49:30.645 --> 00:49:32.085
<v Speaker 0>who wants my cat.

00:49:32.565 --> 00:49:34.484
<v Speaker 0>So let's go back to our pod spec.

00:49:34.805 --> 00:49:36.645
<v Speaker 0>Here, we have an NGINX pod,

00:49:37.125 --> 00:49:38.085
<v Speaker 0>and that's it.

00:49:38.484 --> 00:49:41.605
<v Speaker 0>So let's actually consume our new resource claim.

00:49:42.330 --> 00:49:44.490
<v Speaker 0>Much like a PVC, we have to start

00:49:44.490 --> 00:49:46.650
<v Speaker 0>by adding our resource claims

00:49:46.730 --> 00:49:48.730
<v Speaker 0>to the top of our pod spec.

00:49:50.170 --> 00:49:51.530
<v Speaker 0>We can give it a name

00:49:52.010 --> 00:49:53.610
<v Speaker 0>and say that that's a Salem,

00:49:54.204 --> 00:49:55.805
<v Speaker 0>the name of the cat from my favorite

00:49:55.805 --> 00:49:57.885
<v Speaker 0>TV show as a kid, Sabrina.

00:49:59.085 --> 00:50:01.405
<v Speaker 0>We can say the source of this claim

00:50:01.724 --> 00:50:03.325
<v Speaker 0>is our resource

00:50:03.325 --> 00:50:04.204
<v Speaker 0>claim

00:50:04.204 --> 00:50:05.165
<v Speaker 0>template

00:50:05.165 --> 00:50:05.965
<v Speaker 0>name,

00:50:06.684 --> 00:50:08.525
<v Speaker 0>which is large black

00:50:08.525 --> 00:50:10.444
<v Speaker 0>cat claim

00:50:09.880 --> 00:50:10.840
<v Speaker 0>template.

00:50:12.599 --> 00:50:14.920
<v Speaker 0>Now that we have this available,

00:50:14.920 --> 00:50:16.680
<v Speaker 0>we can go to a container.

00:50:16.920 --> 00:50:18.599
<v Speaker 0>And you see here where we have resources

00:50:18.599 --> 00:50:21.480
<v Speaker 0>and limits and CPU already for memory.

00:50:21.560 --> 00:50:22.920
<v Speaker 0>We're going to add

00:50:23.115 --> 00:50:24.475
<v Speaker 0>that we want

00:50:25.115 --> 00:50:26.155
<v Speaker 0>our claims,

00:50:28.075 --> 00:50:30.315
<v Speaker 0>and we want our name Salem.

00:50:31.994 --> 00:50:34.315
<v Speaker 0>Now we can come to pod.yaml,

00:50:35.835 --> 00:50:36.475
<v Speaker 0>drop it in,

00:50:39.130 --> 00:50:40.170
<v Speaker 0>and apply.

00:50:42.090 --> 00:50:45.530
<v Speaker 0>Now there's no real driver piloting this claim,

00:50:46.010 --> 00:50:47.850
<v Speaker 0>so we could run resource

00:50:48.010 --> 00:50:48.970
<v Speaker 0>claim

00:50:49.450 --> 00:50:50.090
<v Speaker 0>template,

00:50:50.865 --> 00:50:52.625
<v Speaker 0>and we'd see that it's still waiting for

00:50:52.625 --> 00:50:53.905
<v Speaker 0>the first consumer.

00:50:55.825 --> 00:50:58.225
<v Speaker 0>But I hope that shows you how we

00:50:58.225 --> 00:51:00.945
<v Speaker 0>can start to bring in new hardware like

00:51:00.945 --> 00:51:02.385
<v Speaker 0>the KEP proposes,

00:51:02.785 --> 00:51:04.625
<v Speaker 0>but also claims on cloud resources.

00:51:06.250 --> 00:51:07.609
<v Speaker 0>And I can't wait to see where this

00:51:07.609 --> 00:51:09.770
<v Speaker 0>functionality goes over the next year.

00:51:11.049 --> 00:51:13.290
<v Speaker 0>So that's dynamic resource allocation.

00:51:16.009 --> 00:51:18.089
<v Speaker 0>So that's it for our deep dive into

00:51:18.089 --> 00:51:19.049
<v Speaker 0>Kubernetes scheduling.

00:51:19.965 --> 00:51:22.205
<v Speaker 0>We took a look at the basics, scheduling

00:51:22.205 --> 00:51:24.365
<v Speaker 0>the node by setting the node name yourself.

00:51:25.325 --> 00:51:27.565
<v Speaker 0>We then progressed into node selectors,

00:51:27.645 --> 00:51:28.605
<v Speaker 0>affinities,

00:51:28.685 --> 00:51:29.805
<v Speaker 0>anti affinities,

00:51:30.045 --> 00:51:32.045
<v Speaker 0>topology spread constraints,

00:51:33.005 --> 00:51:35.085
<v Speaker 0>and some very fresh out of the oven

00:51:35.085 --> 00:51:37.830
<v Speaker 0>features with with dynamic resource allocation

00:51:38.470 --> 00:51:39.990
<v Speaker 0>and scheduling games.

00:51:40.630 --> 00:51:42.150
<v Speaker 0>So I hope this video was useful to

00:51:42.150 --> 00:51:44.630
<v Speaker 0>you. Remember to subscribe and click the thumb

00:51:44.630 --> 00:51:46.790
<v Speaker 0>button to thumb up, please.

00:51:47.349 --> 00:51:49.205
<v Speaker 0>And I will see you next time. Thank

00:51:49.205 --> 00:51:51.525
<v Speaker 0>you again to Commodore for sponsoring my time

00:51:51.525 --> 00:51:52.965
<v Speaker 0>to make this video.

00:51:53.925 --> 00:51:55.605
<v Speaker 0>I'll see you all next time. Have a

00:51:55.605 --> 00:51:56.005
<v Speaker 0>great day.
