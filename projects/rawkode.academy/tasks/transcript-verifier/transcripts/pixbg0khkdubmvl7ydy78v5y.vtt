WEBVTT

NOTE
Transcription provided by Deepgram
Request Id: ed4c303b-27d8-411e-8fc9-e38eebf1bcc7
Created: 2025-04-29T13:25:18.793Z
Duration: 1226.0085
Channels: 1

00:00:00.000 --> 00:00:03.360
<v Speaker 0>Hello and welcome back to the Rawkode Academy.

00:00:03.520 --> 00:00:06.000
<v Speaker 0>I'm your host, David Flanagan. And today, we

00:00:06.000 --> 00:00:08.720
<v Speaker 0>kick off a new course, the complete guide

00:00:08.720 --> 00:00:09.759
<v Speaker 0>to Parca.

00:00:10.000 --> 00:00:12.639
<v Speaker 0>Before I tell you what Parca is, let's

00:00:12.639 --> 00:00:14.525
<v Speaker 0>set the scene. So So if you've been

00:00:14.525 --> 00:00:16.125
<v Speaker 0>working in the infrastructure,

00:00:16.125 --> 00:00:17.005
<v Speaker 0>operations,

00:00:17.085 --> 00:00:19.565
<v Speaker 0>bare metal, cloud computing, DevOps,

00:00:19.725 --> 00:00:21.165
<v Speaker 0>any of these professions,

00:00:21.245 --> 00:00:22.925
<v Speaker 0>then the likelihood is you've spent a lot

00:00:22.925 --> 00:00:25.485
<v Speaker 0>of your time looking at graphs like this.

00:00:25.565 --> 00:00:27.885
<v Speaker 0>Now this is an extreme situation which has

00:00:27.885 --> 00:00:30.330
<v Speaker 0>been contrived to walk you through an introduction

00:00:30.330 --> 00:00:32.650
<v Speaker 0>to Parca. This is a graph that shows

00:00:32.650 --> 00:00:35.770
<v Speaker 0>the CPU consumption across our infrastructure.

00:00:36.090 --> 00:00:38.410
<v Speaker 0>And as you can see something happened within

00:00:38.410 --> 00:00:39.449
<v Speaker 0>the last hour.

00:00:39.690 --> 00:00:43.385
<v Speaker 0>Our CPU has spiked from under one utilization,

00:00:43.385 --> 00:00:45.625
<v Speaker 0>one core being consumed at all time to

00:00:45.625 --> 00:00:47.465
<v Speaker 0>now approaching four.

00:00:47.625 --> 00:00:49.785
<v Speaker 0>Now because this is a contrived situation, I

00:00:49.785 --> 00:00:51.705
<v Speaker 0>know that this is a new deployment with

00:00:51.705 --> 00:00:53.730
<v Speaker 0>a new version of our software and I

00:00:53.730 --> 00:00:55.410
<v Speaker 0>want to show you how you can take

00:00:55.410 --> 00:00:58.370
<v Speaker 0>a signal like this and use Parca to

00:00:58.370 --> 00:01:00.930
<v Speaker 0>understand what changed within your application

00:01:01.090 --> 00:01:03.330
<v Speaker 0>and help you lower that mean time to

00:01:03.330 --> 00:01:04.209
<v Speaker 0>resolution.

00:01:04.450 --> 00:01:06.530
<v Speaker 0>So let's learn about Parca.

00:01:06.530 --> 00:01:08.290
<v Speaker 0>Parca is a continuous

00:01:08.665 --> 00:01:09.465
<v Speaker 0>profiler.

00:01:09.625 --> 00:01:11.545
<v Speaker 0>If you're not aware or you haven't done

00:01:11.545 --> 00:01:14.424
<v Speaker 0>profiling before, this is a tool that monitors

00:01:14.424 --> 00:01:16.505
<v Speaker 0>for your application's profiles.

00:01:16.505 --> 00:01:19.704
<v Speaker 0>Typically for Go applications we expose a PPROF

00:01:19.704 --> 00:01:20.424
<v Speaker 0>endpoint

00:01:20.505 --> 00:01:23.180
<v Speaker 0>that prints out a whole bunch of diagnostic

00:01:23.180 --> 00:01:25.979
<v Speaker 0>information that you can consume to understand the

00:01:25.979 --> 00:01:27.500
<v Speaker 0>Go runtime itself.

00:01:27.740 --> 00:01:29.900
<v Speaker 0>Parca takes that to the next degree.

00:01:30.220 --> 00:01:33.340
<v Speaker 0>Parca runs full time non stop

00:01:33.420 --> 00:01:34.460
<v Speaker 0>consistently

00:01:34.635 --> 00:01:37.355
<v Speaker 0>inside of your Kubernetes cluster or on bare

00:01:37.355 --> 00:01:38.475
<v Speaker 0>metal if you wish

00:01:38.795 --> 00:01:40.555
<v Speaker 0>and monitors your application.

00:01:40.715 --> 00:01:43.675
<v Speaker 0>However, it doesn't require you to have a

00:01:43.675 --> 00:01:44.955
<v Speaker 0>pprof endpoint

00:01:44.955 --> 00:01:47.755
<v Speaker 0>or to use Go. Parca is quite sophisticated.

00:01:48.210 --> 00:01:50.290
<v Speaker 0>It uses this new shiny technology.

00:01:50.370 --> 00:01:53.490
<v Speaker 0>I say new but recent shiny technology called

00:01:53.490 --> 00:01:54.770
<v Speaker 0>eBPF.

00:01:54.770 --> 00:01:56.850
<v Speaker 0>This allows it to hook into the kernel

00:01:56.850 --> 00:01:58.130
<v Speaker 0>using probes

00:01:58.130 --> 00:02:00.930
<v Speaker 0>to understand what's actually happening within your application

00:02:01.545 --> 00:02:04.345
<v Speaker 0>without you having to instrument anything yourself.

00:02:04.505 --> 00:02:06.185
<v Speaker 0>This means it can track the memory, the

00:02:06.185 --> 00:02:08.264
<v Speaker 0>CPU, the IO and even the network and

00:02:08.264 --> 00:02:09.225
<v Speaker 0>utilization

00:02:09.225 --> 00:02:12.505
<v Speaker 0>down to the function call of your application

00:02:12.585 --> 00:02:14.985
<v Speaker 0>and show you what's actually happening under the

00:02:14.985 --> 00:02:17.810
<v Speaker 0>hood. So when we see graphs with extreme

00:02:17.810 --> 00:02:20.290
<v Speaker 0>deviation on it, we can rely on Parca

00:02:20.290 --> 00:02:22.610
<v Speaker 0>to tell us what is going on. So

00:02:22.610 --> 00:02:24.290
<v Speaker 0>let's dive in and take a look at

00:02:24.290 --> 00:02:27.890
<v Speaker 0>using Parca to resolve our current CPU utilization

00:02:28.055 --> 00:02:29.655
<v Speaker 0>problem. So the first thing we want to

00:02:29.655 --> 00:02:31.975
<v Speaker 0>do is get Parca installed to our cluster.

00:02:32.295 --> 00:02:33.895
<v Speaker 0>To do so, you can click the quick

00:02:33.895 --> 00:02:35.895
<v Speaker 0>start guide here, which takes you to the

00:02:35.895 --> 00:02:37.255
<v Speaker 0>Parca documentation.

00:02:37.495 --> 00:02:39.495
<v Speaker 0>You can, if you wish, curl and run

00:02:39.495 --> 00:02:41.530
<v Speaker 0>a binary on your machine. The first thing

00:02:41.530 --> 00:02:43.690
<v Speaker 0>I'll cover is that Parca comes in two

00:02:43.690 --> 00:02:44.410
<v Speaker 0>parts.

00:02:44.490 --> 00:02:46.330
<v Speaker 0>One, the Parca server.

00:02:46.410 --> 00:02:48.410
<v Speaker 0>This is a database that will store all

00:02:48.410 --> 00:02:50.170
<v Speaker 0>of the profiling information,

00:02:50.250 --> 00:02:52.490
<v Speaker 0>provide a simple UI that will allow you

00:02:52.490 --> 00:02:55.530
<v Speaker 0>to explore this and understand what's happening with

00:02:55.530 --> 00:02:56.490
<v Speaker 0>your applications.

00:02:56.945 --> 00:02:58.865
<v Speaker 0>The second part is the agent.

00:02:59.185 --> 00:03:01.505
<v Speaker 0>Again, can download this as a binary and

00:03:01.505 --> 00:03:02.945
<v Speaker 0>run it in all of the machines within

00:03:02.945 --> 00:03:03.985
<v Speaker 0>your infrastructure

00:03:04.145 --> 00:03:06.785
<v Speaker 0>or you can deploy it to your Kubernetes

00:03:06.785 --> 00:03:09.425
<v Speaker 0>cluster with some Kubernetes YAML. If you scroll

00:03:09.425 --> 00:03:11.105
<v Speaker 0>to the top and select Kubernetes,

00:03:11.740 --> 00:03:13.260
<v Speaker 0>you'll see that we can now create the

00:03:13.260 --> 00:03:14.540
<v Speaker 0>Parca namespace,

00:03:14.780 --> 00:03:17.100
<v Speaker 0>deploy the server which comes with the API

00:03:17.100 --> 00:03:18.620
<v Speaker 0>and the user interface

00:03:18.700 --> 00:03:20.940
<v Speaker 0>and then the agent which gets deployed as

00:03:20.940 --> 00:03:23.180
<v Speaker 0>a daemon set to every node within your

00:03:23.180 --> 00:03:25.945
<v Speaker 0>cluster. It's really simple to get Parca deployed

00:03:25.945 --> 00:03:28.505
<v Speaker 0>and running within your infrastructure especially

00:03:28.505 --> 00:03:30.185
<v Speaker 0>if you're using Kubernetes.

00:03:30.345 --> 00:03:32.505
<v Speaker 0>So I've already installed Parca, so let's head

00:03:32.505 --> 00:03:34.425
<v Speaker 0>over to my terminal where we can run

00:03:34.425 --> 00:03:35.305
<v Speaker 0>kubectl,

00:03:35.465 --> 00:03:37.705
<v Speaker 0>filter to the Parca namespace

00:03:37.865 --> 00:03:38.665
<v Speaker 0>and get pods.

00:03:39.190 --> 00:03:41.510
<v Speaker 0>Here we have the Parca server running, that's

00:03:41.510 --> 00:03:43.510
<v Speaker 0>the first pod on the list followed by

00:03:43.510 --> 00:03:44.550
<v Speaker 0>a agent

00:03:44.630 --> 00:03:46.950
<v Speaker 0>which is deployed as a daemon set. Because

00:03:46.950 --> 00:03:48.550
<v Speaker 0>it's deployed as a daemon set and because

00:03:48.550 --> 00:03:50.470
<v Speaker 0>I had three nodes in my cluster, we

00:03:50.470 --> 00:03:52.815
<v Speaker 0>get three copies of the agent. These agents

00:03:52.815 --> 00:03:54.655
<v Speaker 0>are the ones that are responsible for deploying

00:03:54.655 --> 00:03:56.175
<v Speaker 0>the eBPF probes

00:03:56.175 --> 00:03:58.735
<v Speaker 0>to the kernel to allow us to understand

00:03:58.735 --> 00:04:00.895
<v Speaker 0>all of the processes running on each of

00:04:00.895 --> 00:04:01.935
<v Speaker 0>these machines.

00:04:02.175 --> 00:04:03.055
<v Speaker 0>From here,

00:04:03.295 --> 00:04:04.975
<v Speaker 0>we can run kubectl

00:04:05.455 --> 00:04:06.335
<v Speaker 0>arca

00:04:06.495 --> 00:04:07.535
<v Speaker 0>get service

00:04:07.940 --> 00:04:09.700
<v Speaker 0>where you see we have a cluster IP

00:04:09.700 --> 00:04:12.100
<v Speaker 0>service running on port seventy seventy.

00:04:12.500 --> 00:04:14.820
<v Speaker 0>When you port forward to this, you get

00:04:14.820 --> 00:04:16.580
<v Speaker 0>the Parca UI.

00:04:16.660 --> 00:04:18.420
<v Speaker 0>So let's jump back to our browser

00:04:18.820 --> 00:04:21.955
<v Speaker 0>and see the Parca UI. From here, you

00:04:21.955 --> 00:04:23.955
<v Speaker 0>can select targets and you'll see that the

00:04:23.955 --> 00:04:26.435
<v Speaker 0>agent is deployed to all three nodes within

00:04:26.435 --> 00:04:27.395
<v Speaker 0>my cluster.

00:04:27.715 --> 00:04:29.635
<v Speaker 0>We can see when it last scraped and

00:04:29.635 --> 00:04:32.355
<v Speaker 0>how long the scrape itself took. If we

00:04:32.355 --> 00:04:33.795
<v Speaker 0>head back to profiles,

00:04:34.140 --> 00:04:36.460
<v Speaker 0>we can now get CPU samples

00:04:36.460 --> 00:04:38.060
<v Speaker 0>and begin to filter

00:04:38.380 --> 00:04:41.180
<v Speaker 0>on any dimension that we wish, at least

00:04:41.180 --> 00:04:43.740
<v Speaker 0>any dimension that is available and scraped by

00:04:43.740 --> 00:04:45.100
<v Speaker 0>the Parca agent.

00:04:45.420 --> 00:04:46.060
<v Speaker 0>However,

00:04:46.300 --> 00:04:48.460
<v Speaker 0>you can also just click search and get

00:04:48.460 --> 00:04:49.020
<v Speaker 0>visibility

00:04:49.455 --> 00:04:52.014
<v Speaker 0>into the CPU consumption of all the processes

00:04:52.014 --> 00:04:53.935
<v Speaker 0>on your host. And as we can see

00:04:53.935 --> 00:04:55.775
<v Speaker 0>here, we have a bit of a runaway

00:04:55.775 --> 00:04:58.014
<v Speaker 0>problem with one of the pods within our

00:04:58.014 --> 00:04:58.735
<v Speaker 0>cluster.

00:04:58.974 --> 00:05:01.294
<v Speaker 0>Now consuming over six CPUs.

00:05:01.849 --> 00:05:04.490
<v Speaker 0>So let's hover on one of the points

00:05:04.490 --> 00:05:07.690
<v Speaker 0>within the runaway process. From here, we can

00:05:07.690 --> 00:05:10.889
<v Speaker 0>see the CPU value and this point exactly

00:05:10.889 --> 00:05:12.570
<v Speaker 0>is 5.37.

00:05:12.729 --> 00:05:14.729
<v Speaker 0>We can see the duration and we see

00:05:14.729 --> 00:05:17.530
<v Speaker 0>the labels or dimensions applied to this process.

00:05:17.824 --> 00:05:20.385
<v Speaker 0>This actually allows us to understand the executable

00:05:20.385 --> 00:05:22.305
<v Speaker 0>that was run and the pod that it

00:05:22.305 --> 00:05:24.305
<v Speaker 0>was running. Here we can see that the

00:05:24.305 --> 00:05:26.145
<v Speaker 0>executable was app.

00:05:26.305 --> 00:05:28.544
<v Speaker 0>Now if these are third party pods, you

00:05:28.544 --> 00:05:30.305
<v Speaker 0>may not understand or be able to use

00:05:30.305 --> 00:05:32.700
<v Speaker 0>the executable to understand and even more about

00:05:32.700 --> 00:05:35.340
<v Speaker 0>this process. However, the pod name should give

00:05:35.340 --> 00:05:36.940
<v Speaker 0>you everything else that you require.

00:05:37.340 --> 00:05:39.180
<v Speaker 0>Here, we can see that the pod that

00:05:39.180 --> 00:05:42.060
<v Speaker 0>is running away with the CPU utilization of

00:05:42.060 --> 00:05:44.940
<v Speaker 0>this machine is a million dollar app pod.

00:05:45.445 --> 00:05:47.525
<v Speaker 0>This is my one application that I have

00:05:47.525 --> 00:05:50.085
<v Speaker 0>deployed to the cluster and I should understand

00:05:50.085 --> 00:05:52.965
<v Speaker 0>exactly what is happening in my code. Right?

00:05:53.365 --> 00:05:54.485
<v Speaker 0>Well, wrong.

00:05:54.885 --> 00:05:55.765
<v Speaker 0>Unfortunately,

00:05:56.165 --> 00:05:57.285
<v Speaker 0>in 2023

00:05:57.285 --> 00:05:59.045
<v Speaker 0>and so and at least for the last

00:05:59.045 --> 00:06:02.360
<v Speaker 0>ten years, Applications are comprised primarily of open

00:06:02.360 --> 00:06:03.560
<v Speaker 0>source libraries

00:06:03.640 --> 00:06:06.120
<v Speaker 0>and then a small bit probably written by

00:06:06.120 --> 00:06:08.520
<v Speaker 0>you and your team or your organization.

00:06:08.680 --> 00:06:11.000
<v Speaker 0>We don't really have great visibility into the

00:06:11.000 --> 00:06:13.915
<v Speaker 0>dependencies our application use. Think of the last

00:06:13.915 --> 00:06:16.955
<v Speaker 0>time you ran a GoGet, an NPM install,

00:06:17.195 --> 00:06:18.395
<v Speaker 0>a PEP install,

00:06:18.635 --> 00:06:21.435
<v Speaker 0>a gem bundle and so forth. Did you

00:06:21.435 --> 00:06:23.195
<v Speaker 0>look at the source code of all of

00:06:23.195 --> 00:06:25.115
<v Speaker 0>the dependencies that you're pulling in to make

00:06:25.115 --> 00:06:25.835
<v Speaker 0>your life easier?

00:06:26.430 --> 00:06:28.430
<v Speaker 0>The chances are no. So what do you

00:06:28.430 --> 00:06:30.910
<v Speaker 0>do if one of those dependencies

00:06:30.910 --> 00:06:34.750
<v Speaker 0>is starting to consume more memory, more network,

00:06:34.750 --> 00:06:37.150
<v Speaker 0>more disk IO or more CPU?

00:06:37.230 --> 00:06:38.830
<v Speaker 0>Well, we need more visibility

00:06:39.194 --> 00:06:41.354
<v Speaker 0>and this is where Parca shines.

00:06:41.514 --> 00:06:43.595
<v Speaker 0>Let's go to our CPU samples where we

00:06:43.595 --> 00:06:45.354
<v Speaker 0>can see filter on pod

00:06:45.835 --> 00:06:48.235
<v Speaker 0>and we're going to see million dollar pod.

00:06:48.235 --> 00:06:49.754
<v Speaker 0>And if we get the pod ID from

00:06:49.754 --> 00:06:53.160
<v Speaker 0>here, we can see that it's BCLCC.

00:06:53.160 --> 00:06:55.639
<v Speaker 0>We can click search to only reveal this

00:06:55.639 --> 00:06:57.479
<v Speaker 0>one unique series.

00:06:57.479 --> 00:06:59.880
<v Speaker 0>Now what's really nice about Parca, which is

00:06:59.880 --> 00:07:01.319
<v Speaker 0>kind of hidden by the zoomed in rate

00:07:01.319 --> 00:07:02.120
<v Speaker 0>at the moment,

00:07:02.440 --> 00:07:05.735
<v Speaker 0>is that we get an icicle graph below.

00:07:05.735 --> 00:07:07.815
<v Speaker 0>This shows you the functions where the CPU

00:07:07.815 --> 00:07:10.535
<v Speaker 0>is spending the most time. The longer the

00:07:10.535 --> 00:07:13.175
<v Speaker 0>bar, the more time the CPU has spent

00:07:13.175 --> 00:07:13.815
<v Speaker 0>there.

00:07:14.135 --> 00:07:16.855
<v Speaker 0>And it drills down like an icicle so

00:07:16.855 --> 00:07:19.310
<v Speaker 0>that you can follow the CPU consumption from

00:07:19.310 --> 00:07:22.430
<v Speaker 0>function to function across your stack. And actually,

00:07:22.430 --> 00:07:24.750
<v Speaker 0>we'll zoom back in and if we scroll

00:07:24.750 --> 00:07:27.389
<v Speaker 0>down, we'll see that if we hover on

00:07:27.389 --> 00:07:28.190
<v Speaker 0>the root,

00:07:28.349 --> 00:07:30.910
<v Speaker 0>a % of our time is spent there.

00:07:31.205 --> 00:07:33.685
<v Speaker 0>Of course, because everything else within our application

00:07:33.685 --> 00:07:35.845
<v Speaker 0>is a child or descending of the root

00:07:35.845 --> 00:07:37.925
<v Speaker 0>function. If we work our way down, we

00:07:37.925 --> 00:07:39.605
<v Speaker 0>can see the number start to get lower

00:07:39.605 --> 00:07:40.405
<v Speaker 0>and lower.

00:07:41.045 --> 00:07:42.405
<v Speaker 0>And if we look down here, we can

00:07:42.405 --> 00:07:43.765
<v Speaker 0>see it's substantially lower.

00:07:44.360 --> 00:07:46.920
<v Speaker 0>What we're looking for is that inflection point

00:07:46.920 --> 00:07:49.240
<v Speaker 0>where we go from the most CPU

00:07:49.240 --> 00:07:51.560
<v Speaker 0>down to distributed CPU cycles.

00:07:51.960 --> 00:07:53.480
<v Speaker 0>And as we can see here,

00:07:53.720 --> 00:07:55.480
<v Speaker 0>the million dollar dependency

00:07:55.480 --> 00:07:57.240
<v Speaker 0>is calling two functions.

00:07:58.065 --> 00:08:00.625
<v Speaker 0>One, Feb and one, addition.

00:08:01.104 --> 00:08:03.185
<v Speaker 0>These are both consuming 86%

00:08:03.185 --> 00:08:05.345
<v Speaker 0>of our CPU. And if and below this

00:08:05.345 --> 00:08:08.865
<v Speaker 0>is substantially less, meaning the root cause or

00:08:08.865 --> 00:08:11.185
<v Speaker 0>where most of the consumption is happening is

00:08:11.185 --> 00:08:13.670
<v Speaker 0>probably within these two functions.

00:08:14.070 --> 00:08:16.470
<v Speaker 0>What I love about this icicle graph

00:08:16.550 --> 00:08:19.110
<v Speaker 0>is that visually we can identify the bad

00:08:19.110 --> 00:08:21.590
<v Speaker 0>actors or the bad functions or the not

00:08:21.590 --> 00:08:23.110
<v Speaker 0>necessarily malicious,

00:08:23.350 --> 00:08:24.950
<v Speaker 0>but the code that we need to optimize

00:08:25.465 --> 00:08:28.425
<v Speaker 0>very very quickly. And the icicle graph isn't

00:08:28.425 --> 00:08:30.505
<v Speaker 0>the only way to present this information.

00:08:30.745 --> 00:08:32.664
<v Speaker 0>If you really want, you can filter by

00:08:32.664 --> 00:08:34.745
<v Speaker 0>table. So when we filter by table, we

00:08:34.745 --> 00:08:36.664
<v Speaker 0>still get the cumulative sum for how much

00:08:36.664 --> 00:08:38.585
<v Speaker 0>time is spent in each of the functions.

00:08:38.904 --> 00:08:41.225
<v Speaker 0>And we can see a large drop off

00:08:41.760 --> 00:08:43.680
<v Speaker 0>between the addition function

00:08:43.840 --> 00:08:45.920
<v Speaker 0>and then any other function below it going

00:08:45.920 --> 00:08:47.840
<v Speaker 0>from 83,000

00:08:47.840 --> 00:08:49.600
<v Speaker 0>down to 41,000.

00:08:49.840 --> 00:08:50.480
<v Speaker 0>Again,

00:08:50.720 --> 00:08:51.600
<v Speaker 0>signifying

00:08:51.600 --> 00:08:53.120
<v Speaker 0>that the addition function

00:08:53.375 --> 00:08:56.095
<v Speaker 0>with the largest deviation between the next caller

00:08:56.095 --> 00:08:58.815
<v Speaker 0>probably has the problem that we want to

00:08:58.815 --> 00:09:01.135
<v Speaker 0>fix. So I think it's about time we

00:09:01.135 --> 00:09:03.135
<v Speaker 0>take a look at some code. Let's pop

00:09:03.135 --> 00:09:04.735
<v Speaker 0>over to Versus Code

00:09:04.975 --> 00:09:07.250
<v Speaker 0>and open our main dot go. This is

00:09:07.250 --> 00:09:08.770
<v Speaker 0>our million dollar application.

00:09:09.090 --> 00:09:12.610
<v Speaker 0>It is a simple HTTP handler that calls

00:09:12.610 --> 00:09:14.290
<v Speaker 0>some dependencies

00:09:14.290 --> 00:09:16.130
<v Speaker 0>function called addition.

00:09:16.210 --> 00:09:19.330
<v Speaker 0>That's all this application does. Again, everything else

00:09:19.330 --> 00:09:21.495
<v Speaker 0>is open source and by open source, it's

00:09:21.495 --> 00:09:23.415
<v Speaker 0>my own dependency that I've stuck in place.

00:09:23.415 --> 00:09:24.855
<v Speaker 0>However, remember,

00:09:24.855 --> 00:09:26.375
<v Speaker 0>when's the last time you looked at the

00:09:26.375 --> 00:09:28.695
<v Speaker 0>source code of any of your open source

00:09:28.695 --> 00:09:31.655
<v Speaker 0>dependencies you consume within your application? I bet

00:09:31.655 --> 00:09:33.850
<v Speaker 0>the answer is never. So let's pop open

00:09:33.850 --> 00:09:37.050
<v Speaker 0>dApp because we know that this addition function

00:09:37.050 --> 00:09:39.930
<v Speaker 0>is the root cause or potential root cause

00:09:39.930 --> 00:09:41.610
<v Speaker 0>of whatever is happening

00:09:41.769 --> 00:09:44.730
<v Speaker 0>to degrade the performance of our application. And

00:09:44.730 --> 00:09:46.410
<v Speaker 0>when we open this, we see some very

00:09:46.410 --> 00:09:49.375
<v Speaker 0>simple code. It's using the HTTP request object

00:09:49.375 --> 00:09:52.015
<v Speaker 0>to grab I and j and convert them

00:09:52.015 --> 00:09:54.335
<v Speaker 0>to an integer. And if we scroll down,

00:09:54.415 --> 00:09:57.135
<v Speaker 0>we then see some rather interesting code. For

00:09:57.135 --> 00:09:58.495
<v Speaker 0>some weird reason,

00:09:58.735 --> 00:10:01.135
<v Speaker 0>this open source dependency that we're consuming has

00:10:01.135 --> 00:10:03.769
<v Speaker 0>decided to open a loop for a very

00:10:03.769 --> 00:10:06.570
<v Speaker 0>large number to calculate the Fibonacci sequence.

00:10:06.889 --> 00:10:08.410
<v Speaker 0>As you can see, there's also the Fib

00:10:08.410 --> 00:10:09.050
<v Speaker 0>function.

00:10:09.529 --> 00:10:11.449
<v Speaker 0>These are the two things we've seen in

00:10:11.449 --> 00:10:12.889
<v Speaker 0>our CPU profiles.

00:10:12.889 --> 00:10:14.970
<v Speaker 0>We've seen the call to addition was consuming

00:10:14.970 --> 00:10:16.945
<v Speaker 0>a lot of CPU. We also seen the

00:10:16.945 --> 00:10:19.745
<v Speaker 0>feb call consuming just as much CPU,

00:10:19.905 --> 00:10:21.665
<v Speaker 0>which now makes sense that we're looking at

00:10:21.665 --> 00:10:23.985
<v Speaker 0>the code. We can see the loop. We

00:10:23.985 --> 00:10:26.385
<v Speaker 0>see the function call. We got all of

00:10:26.385 --> 00:10:28.705
<v Speaker 0>this from an uninstrumented

00:10:28.705 --> 00:10:29.665
<v Speaker 0>application.

00:10:29.825 --> 00:10:31.105
<v Speaker 0>Let's just clarify that again.

00:10:31.959 --> 00:10:34.519
<v Speaker 0>This is four lines of Go code with

00:10:34.519 --> 00:10:36.360
<v Speaker 0>some imports and package metadata.

00:10:37.240 --> 00:10:41.240
<v Speaker 0>We haven't instrumented this with metrics, logs, tracing,

00:10:41.320 --> 00:10:44.200
<v Speaker 0>nothing. Yet Parca using eBPF

00:10:44.200 --> 00:10:45.399
<v Speaker 0>is able to understand

00:10:46.154 --> 00:10:49.035
<v Speaker 0>the stack pointers within our Go application and

00:10:49.035 --> 00:10:51.755
<v Speaker 0>give us so much visibility for free. Now

00:10:51.755 --> 00:10:54.555
<v Speaker 0>this application only has one single dependency which

00:10:54.555 --> 00:10:56.555
<v Speaker 0>is a million dollar debt with this addition

00:10:56.555 --> 00:10:58.875
<v Speaker 0>function. However, your application

00:10:59.195 --> 00:11:01.170
<v Speaker 0>likely has dozens,

00:11:01.170 --> 00:11:04.610
<v Speaker 0>hundreds or even thousands of dependencies calling dozens,

00:11:04.690 --> 00:11:07.330
<v Speaker 0>hundreds or thousands of dependent functions.

00:11:07.410 --> 00:11:08.690
<v Speaker 0>Using Parca,

00:11:08.690 --> 00:11:11.570
<v Speaker 0>you can understand the CPU profile of all

00:11:11.570 --> 00:11:14.690
<v Speaker 0>of that without changing a single line of

00:11:14.690 --> 00:11:18.204
<v Speaker 0>code and that is ridiculously powerful. So let's

00:11:18.204 --> 00:11:21.165
<v Speaker 0>deploy effects removing our terrible

00:11:21.485 --> 00:11:22.445
<v Speaker 0>dependency.

00:11:22.605 --> 00:11:24.764
<v Speaker 0>We delete the Fibonacci call, we delete the

00:11:24.764 --> 00:11:25.964
<v Speaker 0>Fibonacci function,

00:11:26.204 --> 00:11:27.964
<v Speaker 0>we can rebuild this image and deploy it

00:11:27.964 --> 00:11:29.670
<v Speaker 0>to a cluster. I'm going to spare you

00:11:29.670 --> 00:11:31.270
<v Speaker 0>that little bit of boring typing in the

00:11:31.270 --> 00:11:33.350
<v Speaker 0>terminal and fast forward directly

00:11:33.430 --> 00:11:35.990
<v Speaker 0>to Parca again where we look at v

00:11:35.990 --> 00:11:39.190
<v Speaker 0>two of our application. So here we are.

00:11:39.190 --> 00:11:41.510
<v Speaker 0>We can still see the old CPU consuming

00:11:41.510 --> 00:11:44.264
<v Speaker 0>workload which hasn't been running for, the last

00:11:44.264 --> 00:11:45.945
<v Speaker 0>five minutes. However,

00:11:46.185 --> 00:11:48.584
<v Speaker 0>we can't actually see the new version of

00:11:48.584 --> 00:11:52.425
<v Speaker 0>our deployment because the utilization is so normal.

00:11:52.584 --> 00:11:54.425
<v Speaker 0>It blends in with everything else within the

00:11:54.425 --> 00:11:56.345
<v Speaker 0>cluster. But, of course, we can use the

00:11:56.345 --> 00:11:59.870
<v Speaker 0>compare button. So let's grab our million dollar

00:11:59.870 --> 00:12:00.750
<v Speaker 0>application

00:12:00.910 --> 00:12:02.430
<v Speaker 0>with the broken version

00:12:02.670 --> 00:12:05.310
<v Speaker 0>and click search. Here, we can grab the

00:12:05.310 --> 00:12:08.030
<v Speaker 0>million dollar application with the correct version and

00:12:08.030 --> 00:12:10.510
<v Speaker 0>hit search. Now we can see the two

00:12:10.510 --> 00:12:13.045
<v Speaker 0>lines side by side. We can see over

00:12:13.045 --> 00:12:16.005
<v Speaker 0>here where it fluctuated from, like, anywhere from

00:12:16.085 --> 00:12:17.845
<v Speaker 0>six to 6.5.

00:12:17.925 --> 00:12:20.005
<v Speaker 0>And over here, it's going from zero to

00:12:20.005 --> 00:12:21.125
<v Speaker 0>0.1.

00:12:21.125 --> 00:12:23.365
<v Speaker 0>With the comparison, we can actually scroll down

00:12:23.365 --> 00:12:25.205
<v Speaker 0>to take a look at the icicle graph

00:12:25.230 --> 00:12:27.709
<v Speaker 0>where we can see that everything below the

00:12:27.709 --> 00:12:30.430
<v Speaker 0>serve function was worse with the older version

00:12:30.430 --> 00:12:33.149
<v Speaker 0>and everything is now better with the new

00:12:33.149 --> 00:12:34.670
<v Speaker 0>version. Perfect.

00:12:34.750 --> 00:12:37.550
<v Speaker 0>If this deploy went the other way and

00:12:37.550 --> 00:12:39.070
<v Speaker 0>we wanted to see what changed

00:12:39.515 --> 00:12:41.755
<v Speaker 0>from the old version to the new version,

00:12:42.555 --> 00:12:45.195
<v Speaker 0>we can of course pop over and switch

00:12:45.195 --> 00:12:48.395
<v Speaker 0>these around. From here, we can see search

00:12:48.395 --> 00:12:50.395
<v Speaker 0>and search. And now we can see that

00:12:50.395 --> 00:12:54.620
<v Speaker 0>the application is worse on every single layer.

00:12:54.700 --> 00:12:57.820
<v Speaker 0>And again, we can identify fib and addition

00:12:57.820 --> 00:13:00.620
<v Speaker 0>as a potential cause of this degradation.

00:13:00.780 --> 00:13:04.140
<v Speaker 0>So that's Parca. Parca instruments your code and

00:13:04.140 --> 00:13:06.855
<v Speaker 0>allows you to understand what's happening down to

00:13:06.855 --> 00:13:09.415
<v Speaker 0>the function level with no changes on your

00:13:09.415 --> 00:13:11.334
<v Speaker 0>part. But if we pop open our drop

00:13:11.334 --> 00:13:13.175
<v Speaker 0>down here, we see the only option is

00:13:13.175 --> 00:13:14.375
<v Speaker 0>CPU samples.

00:13:14.375 --> 00:13:15.894
<v Speaker 0>And I said something at the start of

00:13:15.894 --> 00:13:18.134
<v Speaker 0>this video, which is that Parker can help

00:13:18.134 --> 00:13:20.920
<v Speaker 0>you not just with CPU, but with memory,

00:13:20.920 --> 00:13:23.400
<v Speaker 0>disk IO and even network. So where are

00:13:23.400 --> 00:13:25.640
<v Speaker 0>these settings? Well, at the moment, with the

00:13:25.640 --> 00:13:27.880
<v Speaker 0>auto instrumentation with eBPF,

00:13:28.120 --> 00:13:30.040
<v Speaker 0>only CPU samples are supported,

00:13:30.360 --> 00:13:32.440
<v Speaker 0>but more will be coming soon. If you

00:13:32.440 --> 00:13:35.415
<v Speaker 0>want to have access to the extra instrumentation,

00:13:35.495 --> 00:13:38.135
<v Speaker 0>you need to provide a pay professor endpoint.

00:13:38.375 --> 00:13:39.735
<v Speaker 0>So let's see how to do that for

00:13:39.735 --> 00:13:40.775
<v Speaker 0>a Go application

00:13:41.015 --> 00:13:41.655
<v Speaker 0>now.

00:13:41.975 --> 00:13:43.735
<v Speaker 0>Let's head back to Versus Code where we

00:13:43.735 --> 00:13:45.335
<v Speaker 0>have the easiest,

00:13:45.415 --> 00:13:47.820
<v Speaker 0>simplest Go code in the world. We could

00:13:47.820 --> 00:13:49.980
<v Speaker 0>scroll down to our imports where we import

00:13:49.980 --> 00:13:51.580
<v Speaker 0>net HTTP

00:13:51.580 --> 00:13:52.460
<v Speaker 0>pprof.

00:13:52.780 --> 00:13:55.020
<v Speaker 0>That's quite hard to say. And guess what?

00:13:55.020 --> 00:13:57.740
<v Speaker 0>We're done. Let's rebuild this and push v

00:13:57.740 --> 00:14:00.700
<v Speaker 0>three to our cluster. So now that our

00:14:00.700 --> 00:14:03.260
<v Speaker 0>application has been instrumented

00:14:02.815 --> 00:14:03.615
<v Speaker 0>using

00:14:03.775 --> 00:14:06.895
<v Speaker 0>the default HTTP handler with Go and importing

00:14:06.895 --> 00:14:08.975
<v Speaker 0>the PPROF package, we can then go to

00:14:08.975 --> 00:14:12.095
<v Speaker 0>Parca to consume those extra profiles.

00:14:12.175 --> 00:14:13.295
<v Speaker 0>So if we click on the drop down

00:14:13.295 --> 00:14:15.215
<v Speaker 0>here for select profile where it used to

00:14:15.215 --> 00:14:17.520
<v Speaker 0>be just CPU samples, we now have access

00:14:17.520 --> 00:14:19.920
<v Speaker 0>to the number of Go routines created,

00:14:20.080 --> 00:14:23.600
<v Speaker 0>memory allocated objects and bytes and memory and

00:14:23.600 --> 00:14:26.880
<v Speaker 0>use object and bytes. Now because we're doing

00:14:26.880 --> 00:14:29.279
<v Speaker 0>this via the Parca agent and via the

00:14:29.279 --> 00:14:32.080
<v Speaker 0>pprof endpoint within our Go application, we're going

00:14:32.080 --> 00:14:34.535
<v Speaker 0>to end up with the original CPU samples

00:14:34.774 --> 00:14:37.495
<v Speaker 0>but then a duplicate of process CPU samples

00:14:37.495 --> 00:14:39.095
<v Speaker 0>and process CPU

00:14:39.095 --> 00:14:40.295
<v Speaker 0>nanoseconds.

00:14:40.295 --> 00:14:42.295
<v Speaker 0>Now there are ways to configure the Parca

00:14:42.295 --> 00:14:45.334
<v Speaker 0>agent to avoid certain workloads within your cluster,

00:14:45.415 --> 00:14:48.320
<v Speaker 0>meaning when you instrument your own applications with

00:14:48.320 --> 00:14:50.960
<v Speaker 0>PPROF endpoints, you can attach labels to the

00:14:50.960 --> 00:14:53.280
<v Speaker 0>deployments and tell the Parca agent not to

00:14:53.280 --> 00:14:56.480
<v Speaker 0>scrape or not to use eBPF instrumentation

00:14:56.640 --> 00:14:59.245
<v Speaker 0>on those workloads. In a video later on

00:14:59.245 --> 00:15:00.685
<v Speaker 0>in this course, we'll be taking a look

00:15:00.685 --> 00:15:03.165
<v Speaker 0>at the more advanced configuration techniques for such

00:15:03.165 --> 00:15:04.045
<v Speaker 0>deployments.

00:15:04.125 --> 00:15:05.725
<v Speaker 0>But for right now, let's just do a

00:15:05.725 --> 00:15:08.445
<v Speaker 0>simple exploration of the go routines, the memory

00:15:08.445 --> 00:15:11.380
<v Speaker 0>allocated, and the memory in use. First, click

00:15:11.380 --> 00:15:14.420
<v Speaker 0>go routine created total and click search.

00:15:14.580 --> 00:15:17.700
<v Speaker 0>Now we've already seen the code for my

00:15:17.700 --> 00:15:18.500
<v Speaker 0>application.

00:15:18.660 --> 00:15:21.300
<v Speaker 0>It isn't actually spinning off any go routines

00:15:21.300 --> 00:15:23.940
<v Speaker 0>for us to see any fluctuation in this

00:15:23.940 --> 00:15:26.065
<v Speaker 0>graph at all. So you can see that

00:15:26.065 --> 00:15:28.865
<v Speaker 0>it's pretty static at six nonstop.

00:15:28.865 --> 00:15:32.225
<v Speaker 0>However, your application probably does use go routines

00:15:32.225 --> 00:15:33.985
<v Speaker 0>if you're writing in Go. So you'll be

00:15:33.985 --> 00:15:36.305
<v Speaker 0>able to visualize that in much more detail.

00:15:36.385 --> 00:15:38.065
<v Speaker 0>Moving down, we get to see the memory

00:15:38.065 --> 00:15:39.345
<v Speaker 0>allocated objects

00:15:39.530 --> 00:15:40.410
<v Speaker 0>total.

00:15:40.490 --> 00:15:42.090
<v Speaker 0>Now when I click search here, what we

00:15:42.090 --> 00:15:44.330
<v Speaker 0>should actually see is an incremental

00:15:44.330 --> 00:15:46.810
<v Speaker 0>linear line, hopefully linear

00:15:47.370 --> 00:15:49.210
<v Speaker 0>and if we click search, there we go.

00:15:49.210 --> 00:15:51.850
<v Speaker 0>Now this is the total allocated objects across

00:15:51.850 --> 00:15:55.025
<v Speaker 0>the entire lifespan of this pod, so we

00:15:55.025 --> 00:15:57.665
<v Speaker 0>would expect that to only go up or

00:15:57.665 --> 00:16:00.465
<v Speaker 0>at least stop if there were no traffic,

00:16:00.625 --> 00:16:02.225
<v Speaker 0>it wasn't being consumed,

00:16:02.465 --> 00:16:04.305
<v Speaker 0>it wasn't doing anything useful.

00:16:04.705 --> 00:16:07.105
<v Speaker 0>Perhaps idle time. We can then see the

00:16:07.105 --> 00:16:08.225
<v Speaker 0>same for the bates total.

00:16:08.750 --> 00:16:11.550
<v Speaker 0>So while we have lots of allocated objects,

00:16:11.630 --> 00:16:13.870
<v Speaker 0>the base total may be slightly different depending

00:16:13.870 --> 00:16:15.950
<v Speaker 0>on what those objects looks like within your

00:16:15.950 --> 00:16:18.510
<v Speaker 0>code and it's vital to understand both the

00:16:18.510 --> 00:16:21.630
<v Speaker 0>shape and the size of the thing happening

00:16:21.630 --> 00:16:24.535
<v Speaker 0>within your application. Now that's the total values

00:16:24.535 --> 00:16:26.135
<v Speaker 0>over all time but what if you want

00:16:26.135 --> 00:16:28.135
<v Speaker 0>to see what your application looks like right

00:16:28.135 --> 00:16:29.815
<v Speaker 0>now as a snapshot?

00:16:29.895 --> 00:16:31.735
<v Speaker 0>Well, we can use n use bytes and

00:16:31.735 --> 00:16:34.535
<v Speaker 0>n use objects. We click search and we'll

00:16:34.535 --> 00:16:36.055
<v Speaker 0>see that this will go up and down

00:16:36.055 --> 00:16:37.975
<v Speaker 0>depending on what's happening with our application.

00:16:38.900 --> 00:16:42.180
<v Speaker 0>So while this looks relatively flat right now,

00:16:42.340 --> 00:16:45.860
<v Speaker 0>let's trigger some workloads, some requests against our

00:16:45.860 --> 00:16:46.660
<v Speaker 0>application.

00:16:46.980 --> 00:16:48.820
<v Speaker 0>So if we head over to the terminal,

00:16:49.140 --> 00:16:51.060
<v Speaker 0>I have a k six script.

00:16:51.735 --> 00:16:53.255
<v Speaker 0>If we pop this open,

00:16:54.935 --> 00:16:57.095
<v Speaker 0>you'll see that it just makes some requests

00:16:57.095 --> 00:16:59.654
<v Speaker 0>calling our addition function with a couple of

00:16:59.654 --> 00:17:02.535
<v Speaker 0>values, nothing particularly fancy.

00:17:03.015 --> 00:17:05.015
<v Speaker 0>But we can run this with 10 virtual

00:17:05.015 --> 00:17:07.359
<v Speaker 0>users for a duration of nine hundred seconds

00:17:07.359 --> 00:17:08.799
<v Speaker 0>or fifteen minutes,

00:17:09.119 --> 00:17:10.960
<v Speaker 0>like so. So this is going to send

00:17:10.960 --> 00:17:13.520
<v Speaker 0>a whole bunch of requests every single second

00:17:13.520 --> 00:17:15.760
<v Speaker 0>to our pod, which if we give it

00:17:15.760 --> 00:17:17.520
<v Speaker 0>just a few moments, we should be able

00:17:17.520 --> 00:17:19.524
<v Speaker 0>to head back on over to Parca and

00:17:19.685 --> 00:17:21.285
<v Speaker 0>see our memory utilization

00:17:21.285 --> 00:17:23.605
<v Speaker 0>both total and end use

00:17:23.765 --> 00:17:24.644
<v Speaker 0>change

00:17:24.885 --> 00:17:26.885
<v Speaker 0>from what we've had over the last five

00:17:26.885 --> 00:17:28.244
<v Speaker 0>to ten minutes.

00:17:28.405 --> 00:17:30.085
<v Speaker 0>Okay. So that's been running for a couple

00:17:30.085 --> 00:17:32.180
<v Speaker 0>of minutes now, so let's head back over

00:17:32.180 --> 00:17:33.940
<v Speaker 0>to Parca. Now if we go to the

00:17:33.940 --> 00:17:36.020
<v Speaker 0>go routine created total, what we should see

00:17:36.020 --> 00:17:38.660
<v Speaker 0>is well, even though our application doesn't use

00:17:38.660 --> 00:17:40.900
<v Speaker 0>a lot of go routines, the HTTP server

00:17:40.900 --> 00:17:43.860
<v Speaker 0>does depending on how many requests come in

00:17:43.860 --> 00:17:46.580
<v Speaker 0>and the number of processors available on the

00:17:46.580 --> 00:17:48.495
<v Speaker 0>machine. So if we click search,

00:17:49.135 --> 00:17:50.895
<v Speaker 0>as we can see when we started

00:17:50.975 --> 00:17:53.935
<v Speaker 0>the k six kind of load stressing process,

00:17:54.255 --> 00:17:56.415
<v Speaker 0>the number of goroutines did rise from six

00:17:56.415 --> 00:17:59.055
<v Speaker 0>to 16 where it has continued. So let's

00:17:59.055 --> 00:18:00.495
<v Speaker 0>go across and kill that,

00:18:01.030 --> 00:18:02.790
<v Speaker 0>like so. And we should see that our

00:18:02.790 --> 00:18:05.430
<v Speaker 0>go routines actually drop back down as we

00:18:05.430 --> 00:18:07.750
<v Speaker 0>go back into an idle state. Now our

00:18:07.750 --> 00:18:09.990
<v Speaker 0>h b endpoints are also very very simple,

00:18:09.990 --> 00:18:12.470
<v Speaker 0>so we're not gonna see huge fluctuations in

00:18:12.470 --> 00:18:14.630
<v Speaker 0>memory. However, if we go to the objects

00:18:14.630 --> 00:18:17.504
<v Speaker 0>total, we'll see that this continues to rise,

00:18:17.825 --> 00:18:20.304
<v Speaker 0>maybe just increasing a little bit with that

00:18:20.304 --> 00:18:22.864
<v Speaker 0>stress and load test tool running against it.

00:18:23.265 --> 00:18:24.705
<v Speaker 0>And the same for the baits, I would

00:18:24.705 --> 00:18:25.264
<v Speaker 0>imagine,

00:18:25.585 --> 00:18:27.424
<v Speaker 0>maybe slightly more consistent.

00:18:27.505 --> 00:18:29.025
<v Speaker 0>If we go to the end user's memory

00:18:29.025 --> 00:18:31.429
<v Speaker 0>and the objects and the baits here, these

00:18:31.429 --> 00:18:33.910
<v Speaker 0>will probably still continue to fluctuate again because

00:18:33.910 --> 00:18:36.390
<v Speaker 0>the http endpoints aren't allocating anything to the

00:18:36.390 --> 00:18:39.110
<v Speaker 0>heap. So we're just going to trust that

00:18:39.110 --> 00:18:41.670
<v Speaker 0>our profile information is doing exactly what we

00:18:41.670 --> 00:18:43.990
<v Speaker 0>want. Let's head back over to go routines

00:18:43.990 --> 00:18:46.309
<v Speaker 0>and see if we've reached our idle state

00:18:46.309 --> 00:18:46.710
<v Speaker 0>again.

00:18:47.605 --> 00:18:50.085
<v Speaker 0>Perfect. There we have it. I hope that

00:18:50.085 --> 00:18:51.924
<v Speaker 0>gives you a taste for what you can

00:18:51.924 --> 00:18:52.884
<v Speaker 0>start to do,

00:18:53.365 --> 00:18:56.804
<v Speaker 0>understand and achieve by deploying Parca to your

00:18:56.804 --> 00:18:57.604
<v Speaker 0>infrastructure.

00:18:57.605 --> 00:18:59.605
<v Speaker 0>It's a phenomenal tool that gives us a

00:18:59.605 --> 00:19:01.365
<v Speaker 0>new level of observability

00:19:01.365 --> 00:19:03.289
<v Speaker 0>that we haven't previously had

00:19:03.530 --> 00:19:06.250
<v Speaker 0>with so little instrumentation to be done to

00:19:06.250 --> 00:19:08.169
<v Speaker 0>the point of none. Alright?

00:19:08.410 --> 00:19:10.650
<v Speaker 0>If you just want to get Parca running

00:19:10.650 --> 00:19:11.850
<v Speaker 0>on your infrastructure,

00:19:12.010 --> 00:19:15.290
<v Speaker 0>deploy the agent, rely on the eBPF

00:19:15.290 --> 00:19:16.170
<v Speaker 0>instrumentation,

00:19:16.735 --> 00:19:18.414
<v Speaker 0>you can do that now. Don't need to

00:19:18.414 --> 00:19:21.134
<v Speaker 0>modify your applications and you'll begin to understand

00:19:21.294 --> 00:19:24.975
<v Speaker 0>at the function level how your application behaved

00:19:24.975 --> 00:19:26.094
<v Speaker 0>from release

00:19:26.095 --> 00:19:29.294
<v Speaker 0>to release and that is really important. Continuous

00:19:29.294 --> 00:19:31.294
<v Speaker 0>profiling is there to help you understand how

00:19:31.294 --> 00:19:33.220
<v Speaker 0>your software changes over time.

00:19:33.380 --> 00:19:35.380
<v Speaker 0>So in an upcoming video, we're gonna take

00:19:35.380 --> 00:19:37.940
<v Speaker 0>a look at a real production application

00:19:37.940 --> 00:19:41.059
<v Speaker 0>triggering the deployments from version to version using

00:19:41.059 --> 00:19:42.580
<v Speaker 0>profiling to understand

00:19:42.820 --> 00:19:45.299
<v Speaker 0>how the code that we commit affects the

00:19:45.299 --> 00:19:47.545
<v Speaker 0>performance of our application. And then if you

00:19:47.545 --> 00:19:50.345
<v Speaker 0>do want to go beyond CPU samples,

00:19:50.665 --> 00:19:53.225
<v Speaker 0>you can add PPROF endpoints to your application

00:19:53.305 --> 00:19:55.625
<v Speaker 0>to understand the memory, the network and the

00:19:55.625 --> 00:19:58.985
<v Speaker 0>disk IO consumption or patterns of your applications

00:19:58.985 --> 00:20:01.370
<v Speaker 0>too. Join us for the next video as

00:20:01.370 --> 00:20:03.210
<v Speaker 0>we take a look at taking all of

00:20:03.210 --> 00:20:05.450
<v Speaker 0>this new understanding and observability

00:20:05.450 --> 00:20:07.210
<v Speaker 0>and putting it in to the home of

00:20:07.210 --> 00:20:08.890
<v Speaker 0>the rest of our observability

00:20:08.890 --> 00:20:09.529
<v Speaker 0>data.

00:20:09.930 --> 00:20:13.290
<v Speaker 0>Namely, Grafana. Parca has fantastic support for working

00:20:13.290 --> 00:20:15.674
<v Speaker 0>with the Grafana ecosystem and we'll see that

00:20:15.674 --> 00:20:18.475
<v Speaker 0>in the next video. Until then, have fun

00:20:18.475 --> 00:20:20.075
<v Speaker 0>playing with Parca and we'll see you all

00:20:20.075 --> 00:20:21.434
<v Speaker 0>next time. Have a great day.
