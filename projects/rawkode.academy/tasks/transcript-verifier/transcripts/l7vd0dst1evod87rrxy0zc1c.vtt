WEBVTT

NOTE
Transcription provided by Deepgram
Request Id: 531f53ec-a73f-41e5-b1c6-14ffd8e0540b
Created: 2025-04-29T14:25:51.522Z
Duration: 1578.7312
Channels: 1

00:00:05.040 --> 00:00:05.760
<v Speaker 0>Hello,

00:00:06.000 --> 00:00:08.639
<v Speaker 0>and welcome back to the Rawkode Academy.

00:00:09.120 --> 00:00:11.280
<v Speaker 0>I'm your host, David Flanagan.

00:00:11.360 --> 00:00:12.880
<v Speaker 0>Although you may know me from across the

00:00:12.880 --> 00:00:14.000
<v Speaker 0>Internet as Rawkode.

00:00:14.535 --> 00:00:17.895
<v Speaker 0>Today, I'm going to guide you through Komodor.

00:00:18.855 --> 00:00:20.935
<v Speaker 0>If you're not familiar with Komodor,

00:00:21.255 --> 00:00:23.335
<v Speaker 0>it is a SaaS based product to help

00:00:23.335 --> 00:00:26.455
<v Speaker 0>you troubleshoot and debug your Kubernetes clusters,

00:00:27.349 --> 00:00:28.070
<v Speaker 0>Something

00:00:28.630 --> 00:00:30.390
<v Speaker 0>I have a few opinions about.

00:00:31.189 --> 00:00:34.230
<v Speaker 0>And I don't often cover SaaS based products.

00:00:34.469 --> 00:00:37.110
<v Speaker 0>However, Komodor just this week

00:00:37.670 --> 00:00:38.470
<v Speaker 0>announced

00:00:38.470 --> 00:00:41.815
<v Speaker 0>their new free tier, meaning you don't need

00:00:41.815 --> 00:00:44.054
<v Speaker 0>to pay to get started with Komodor.

00:00:44.774 --> 00:00:47.175
<v Speaker 0>Not only that, Komodor sponsored my time to

00:00:47.175 --> 00:00:49.734
<v Speaker 0>produce the advanced scheduling demo

00:00:50.055 --> 00:00:51.574
<v Speaker 0>and lastly,

00:00:52.135 --> 00:00:54.774
<v Speaker 0>they've also committed to being on an episode

00:00:54.774 --> 00:00:56.670
<v Speaker 0>of clustered I'm going to put them through

00:00:56.670 --> 00:00:58.750
<v Speaker 0>the tests. I'm going to give them a

00:00:58.750 --> 00:00:59.790
<v Speaker 0>broken cluster

00:00:59.950 --> 00:01:02.030
<v Speaker 0>and they're convinced they can use Komodor to

00:01:02.030 --> 00:01:02.830
<v Speaker 0>fix it.

00:01:04.750 --> 00:01:07.390
<v Speaker 0>So thank you Komodor for sponsoring the advanced

00:01:07.390 --> 00:01:08.270
<v Speaker 0>scheduling video.

00:01:09.314 --> 00:01:11.875
<v Speaker 0>I'm sorry and thank you for joining me

00:01:11.875 --> 00:01:13.475
<v Speaker 0>on clustered very, soon.

00:01:15.314 --> 00:01:17.795
<v Speaker 0>But today let's focus on the tutorial and

00:01:17.795 --> 00:01:20.195
<v Speaker 0>show you how you can get started with

00:01:20.195 --> 00:01:20.755
<v Speaker 0>Komodor.

00:01:21.440 --> 00:01:23.440
<v Speaker 0>We're going to start from the beginning

00:01:23.760 --> 00:01:26.720
<v Speaker 0>but also showcase some advanced use cases for

00:01:26.720 --> 00:01:27.600
<v Speaker 0>Komodor.

00:01:28.080 --> 00:01:29.360
<v Speaker 0>But the first thing we need to do

00:01:29.360 --> 00:01:31.520
<v Speaker 0>is go to commodore.com.

00:01:34.000 --> 00:01:36.080
<v Speaker 0>From here you can feel free to read

00:01:36.080 --> 00:01:38.375
<v Speaker 0>the marketing material, go to resources,

00:01:38.375 --> 00:01:39.735
<v Speaker 0>pricing, documentation,

00:01:39.975 --> 00:01:40.935
<v Speaker 0>whatever you want.

00:01:41.415 --> 00:01:44.055
<v Speaker 0>I'm gonna start by logging in

00:01:45.335 --> 00:01:47.095
<v Speaker 0>which I use my Google account.

00:01:49.575 --> 00:01:52.534
<v Speaker 0>So immediately we're presented with the service list.

00:01:52.950 --> 00:01:54.550
<v Speaker 0>This is a list of all of the

00:01:54.550 --> 00:01:55.990
<v Speaker 0>micro services or

00:01:56.230 --> 00:01:59.190
<v Speaker 0>maybe huge services, who knows, deployed to your

00:01:59.190 --> 00:02:00.470
<v Speaker 0>Kubernetes cluster.

00:02:01.030 --> 00:02:02.630
<v Speaker 0>Now you won't see this list straight away,

00:02:02.630 --> 00:02:05.030
<v Speaker 0>this is because I've already added my first

00:02:05.030 --> 00:02:06.310
<v Speaker 0>Kubernetes cluster.

00:02:06.630 --> 00:02:07.910
<v Speaker 0>But let me walk you through the process

00:02:07.910 --> 00:02:09.155
<v Speaker 0>for doing that yourself.

00:02:09.955 --> 00:02:11.555
<v Speaker 0>Down at the bottom left you will see

00:02:11.555 --> 00:02:12.995
<v Speaker 0>the integrations button.

00:02:13.635 --> 00:02:15.474
<v Speaker 0>When you select this you can click on

00:02:15.474 --> 00:02:16.755
<v Speaker 0>add a cluster.

00:02:18.034 --> 00:02:20.275
<v Speaker 0>You can give it whatever name you want

00:02:20.275 --> 00:02:21.155
<v Speaker 0>and hit next.

00:02:22.790 --> 00:02:24.470
<v Speaker 0>This will give you the command that you

00:02:24.470 --> 00:02:27.270
<v Speaker 0>copy and run-in your terminal. It will add

00:02:27.270 --> 00:02:28.470
<v Speaker 0>a Helm repository,

00:02:29.510 --> 00:02:32.470
<v Speaker 0>deploy the Helm chart with a Komodor agent,

00:02:33.110 --> 00:02:34.950
<v Speaker 0>and then you click next where it will

00:02:34.950 --> 00:02:36.975
<v Speaker 0>wait for the connection and confirm it.

00:02:37.775 --> 00:02:40.095
<v Speaker 0>From there go back to the home page

00:02:40.095 --> 00:02:42.974
<v Speaker 0>and you'll see your Kubernetes services from your

00:02:42.974 --> 00:02:43.775
<v Speaker 0>cluster.

00:02:44.415 --> 00:02:46.334
<v Speaker 0>Now there's a couple of nice things on

00:02:46.334 --> 00:02:48.415
<v Speaker 0>this page right off the bat. First,

00:02:48.974 --> 00:02:50.174
<v Speaker 0>all my services are healthy.

00:02:52.060 --> 00:02:53.180
<v Speaker 0>But secondly,

00:02:53.739 --> 00:02:55.739
<v Speaker 0>we get a good overview of the workloads

00:02:55.739 --> 00:02:57.020
<v Speaker 0>running in this cluster.

00:02:57.500 --> 00:02:58.700
<v Speaker 0>You can see I have a bunch of

00:02:58.700 --> 00:02:59.980
<v Speaker 0>Prometheus stuff.

00:03:00.379 --> 00:03:02.140
<v Speaker 0>I've got one password connect,

00:03:02.140 --> 00:03:03.340
<v Speaker 0>Weave GitOps,

00:03:03.500 --> 00:03:05.755
<v Speaker 0>cert manager, shop at I,

00:03:06.314 --> 00:03:07.515
<v Speaker 0>lots of cool stuff.

00:03:08.314 --> 00:03:11.034
<v Speaker 0>Now if things weren't all healthy, we could

00:03:11.034 --> 00:03:12.314
<v Speaker 0>either exclude

00:03:12.314 --> 00:03:13.435
<v Speaker 0>the healthies

00:03:13.674 --> 00:03:15.675
<v Speaker 0>or we could filter on the healthies.

00:03:17.194 --> 00:03:18.795
<v Speaker 0>If you have more than one cluster,

00:03:19.970 --> 00:03:21.730
<v Speaker 0>you can filter by that too.

00:03:22.450 --> 00:03:23.730
<v Speaker 0>And if you only want to take a

00:03:23.730 --> 00:03:25.730
<v Speaker 0>look at particular namespace,

00:03:26.690 --> 00:03:28.450
<v Speaker 0>in my case, let's just take a look

00:03:28.450 --> 00:03:30.290
<v Speaker 0>at my community namespace.

00:03:30.690 --> 00:03:32.450
<v Speaker 0>You'll see that I'm only running a single

00:03:32.450 --> 00:03:32.849
<v Speaker 0>service.

00:03:33.485 --> 00:03:35.245
<v Speaker 0>If I want to view the platform and

00:03:35.245 --> 00:03:37.405
<v Speaker 0>the community namespace, I can do so as

00:03:37.405 --> 00:03:37.885
<v Speaker 0>well.

00:03:41.325 --> 00:03:43.805
<v Speaker 0>If you want to filter by workload type,

00:03:43.885 --> 00:03:45.165
<v Speaker 0>we can click on daemon set and see

00:03:45.165 --> 00:03:45.805
<v Speaker 0>daemon sets.

00:03:46.790 --> 00:03:48.630
<v Speaker 0>Just the basic settings that you would expect

00:03:48.630 --> 00:03:50.790
<v Speaker 0>from a service overview of your cluster.

00:03:51.350 --> 00:03:52.790
<v Speaker 0>The last thing I'll point out on this

00:03:52.790 --> 00:03:54.390
<v Speaker 0>page is at the top right.

00:03:54.870 --> 00:03:56.630
<v Speaker 0>Here we can sort by a few options.

00:03:57.030 --> 00:03:59.430
<v Speaker 0>By default is on health which makes sense,

00:03:59.430 --> 00:04:01.110
<v Speaker 0>if there's something that is unhealthy in your

00:04:01.110 --> 00:04:03.005
<v Speaker 0>cluster you want to see that first. The

00:04:03.005 --> 00:04:04.765
<v Speaker 0>other view that I've been enjoying over the

00:04:04.765 --> 00:04:06.525
<v Speaker 0>last few days is namespace.

00:04:06.845 --> 00:04:08.045
<v Speaker 0>This is a good way to break it

00:04:08.045 --> 00:04:10.205
<v Speaker 0>down by namespace without specifically filtering on a

00:04:10.205 --> 00:04:11.325
<v Speaker 0>namespace itself.

00:04:12.525 --> 00:04:14.125
<v Speaker 0>And if you're only worried about things that

00:04:14.125 --> 00:04:16.765
<v Speaker 0>have changed recently, go to last modified and

00:04:16.765 --> 00:04:18.550
<v Speaker 0>you'll see the most recent resources that have

00:04:18.550 --> 00:04:20.149
<v Speaker 0>been modified within your cluster.

00:04:20.550 --> 00:04:23.270
<v Speaker 0>I deployed Prometheus today, so we can see

00:04:23.270 --> 00:04:24.070
<v Speaker 0>Prometheus

00:04:24.070 --> 00:04:25.190
<v Speaker 0>front and center.

00:04:25.750 --> 00:04:28.470
<v Speaker 0>And that's the service overview. It's not life

00:04:28.470 --> 00:04:29.110
<v Speaker 0>changing,

00:04:29.669 --> 00:04:30.949
<v Speaker 0>but it's very valuable

00:04:31.785 --> 00:04:33.785
<v Speaker 0>with just enough functionality

00:04:34.745 --> 00:04:36.585
<v Speaker 0>to maybe pry cube control out of your

00:04:36.585 --> 00:04:38.105
<v Speaker 0>hands when things go wrong.

00:04:39.145 --> 00:04:41.145
<v Speaker 0>So let's see what else we can do

00:04:41.145 --> 00:04:42.185
<v Speaker 0>with Komodor.

00:04:42.665 --> 00:04:44.905
<v Speaker 0>So we also have the jobs option on

00:04:44.905 --> 00:04:46.819
<v Speaker 0>the left. Although I have no jobs on

00:04:46.819 --> 00:04:48.900
<v Speaker 0>my cluster, however this is just the same

00:04:48.900 --> 00:04:50.900
<v Speaker 0>as services, if you are using the job

00:04:50.900 --> 00:04:52.979
<v Speaker 0>object or the cron job object you will

00:04:52.979 --> 00:04:54.500
<v Speaker 0>see them listed here.

00:04:55.860 --> 00:04:57.380
<v Speaker 0>Next we have the events,

00:04:57.539 --> 00:04:59.505
<v Speaker 0>this will show you all the events from

00:04:59.505 --> 00:05:01.105
<v Speaker 0>your Kubernetes cluster.

00:05:01.425 --> 00:05:03.185
<v Speaker 0>Now this is something that can be typically

00:05:03.185 --> 00:05:04.625
<v Speaker 0>quite overwhelming

00:05:04.625 --> 00:05:07.025
<v Speaker 0>to do from the kubectl command line

00:05:07.745 --> 00:05:08.945
<v Speaker 0>because events

00:05:08.945 --> 00:05:11.585
<v Speaker 0>come fast and furious in the Kubernetes cluster.

00:05:14.140 --> 00:05:16.700
<v Speaker 0>And when we have an abundance of information,

00:05:17.980 --> 00:05:19.740
<v Speaker 0>bring in a visual layer

00:05:20.380 --> 00:05:21.740
<v Speaker 0>to that information

00:05:21.980 --> 00:05:24.140
<v Speaker 0>is how we develop understanding.

00:05:25.100 --> 00:05:27.475
<v Speaker 0>So let's see how we can understand the

00:05:27.475 --> 00:05:30.275
<v Speaker 0>events within a Kubernetes cluster with Komodor.

00:05:31.955 --> 00:05:33.555
<v Speaker 0>Much like the service page, we have the

00:05:33.555 --> 00:05:36.115
<v Speaker 0>ability to filter these events on cluster and

00:05:36.115 --> 00:05:37.075
<v Speaker 0>namespace.

00:05:37.315 --> 00:05:40.035
<v Speaker 0>However, now we can filter by individual service.

00:05:40.720 --> 00:05:42.720
<v Speaker 0>We can filter by the event type.

00:05:43.199 --> 00:05:44.879
<v Speaker 0>We have the ability to filter on the

00:05:44.879 --> 00:05:47.199
<v Speaker 0>status of the event as well as deploy

00:05:47.199 --> 00:05:49.440
<v Speaker 0>details and availability reasons.

00:05:49.759 --> 00:05:51.199
<v Speaker 0>And we'll get into more of these in

00:05:51.199 --> 00:05:52.000
<v Speaker 0>just a moment.

00:05:53.120 --> 00:05:54.719
<v Speaker 0>But first, let's take a look at my

00:05:54.719 --> 00:05:55.759
<v Speaker 0>platform namespace.

00:05:56.525 --> 00:05:58.044
<v Speaker 0>Now here we can see all the events

00:05:58.044 --> 00:06:00.525
<v Speaker 0>as Komodor was deployed to my cluster and

00:06:00.525 --> 00:06:03.245
<v Speaker 0>went through the discovery phase. That is discovering

00:06:03.245 --> 00:06:05.885
<v Speaker 0>all the workloads and resources within my cluster.

00:06:06.285 --> 00:06:08.445
<v Speaker 0>From here, we can click on a service

00:06:08.445 --> 00:06:08.925
<v Speaker 0>name.

00:06:09.560 --> 00:06:11.400
<v Speaker 0>So that slides out at a nice kind

00:06:11.400 --> 00:06:14.120
<v Speaker 0>of popover model dialogue meaning that we don't

00:06:14.120 --> 00:06:16.520
<v Speaker 0>really lose our original context when we're debugging

00:06:16.520 --> 00:06:18.680
<v Speaker 0>which I think is really important for debugging

00:06:18.680 --> 00:06:21.400
<v Speaker 0>tool. So very nice addition.

00:06:22.760 --> 00:06:25.160
<v Speaker 0>We have the service name, the health status,

00:06:25.435 --> 00:06:27.595
<v Speaker 0>can see all the events for the service,

00:06:27.995 --> 00:06:29.595
<v Speaker 0>as well as the pods,

00:06:29.995 --> 00:06:31.595
<v Speaker 0>the notes are scheduled on,

00:06:31.835 --> 00:06:33.995
<v Speaker 0>and some additional information which gives us access

00:06:33.995 --> 00:06:36.315
<v Speaker 0>to the labels and annotations on

00:06:36.794 --> 00:06:37.435
<v Speaker 0>the service.

00:06:38.590 --> 00:06:41.230
<v Speaker 0>Okay. Let's pop in to the monitor namespace

00:06:41.630 --> 00:06:44.910
<v Speaker 0>and we'll select our Grafana service.

00:06:46.190 --> 00:06:49.070
<v Speaker 0>Currently, we only see information about Grafana,

00:06:49.230 --> 00:06:50.430
<v Speaker 0>which we'd expect.

00:06:51.295 --> 00:06:52.815
<v Speaker 0>We can see the events,

00:06:53.055 --> 00:06:54.575
<v Speaker 0>again the nodes,

00:06:54.815 --> 00:06:55.695
<v Speaker 0>pods

00:06:56.415 --> 00:06:58.655
<v Speaker 0>and our labels and annotations.

00:06:59.375 --> 00:07:00.735
<v Speaker 0>Now before we take a look at the

00:07:00.735 --> 00:07:03.135
<v Speaker 0>best practice recommendations, let's pop back over to

00:07:03.135 --> 00:07:03.615
<v Speaker 0>events

00:07:04.220 --> 00:07:07.100
<v Speaker 0>and see here we have the related resources

00:07:07.100 --> 00:07:07.820
<v Speaker 0>button.

00:07:08.380 --> 00:07:10.620
<v Speaker 0>Now this is quite nice because it allows

00:07:10.620 --> 00:07:13.340
<v Speaker 0>us to select other resources within the same

00:07:13.340 --> 00:07:14.300
<v Speaker 0>namespace

00:07:14.620 --> 00:07:16.220
<v Speaker 0>and we want to be able to collectively

00:07:16.220 --> 00:07:18.380
<v Speaker 0>group them and view their events together.

00:07:19.705 --> 00:07:22.105
<v Speaker 0>So I'll pick on Kubernetes services,

00:07:22.185 --> 00:07:24.425
<v Speaker 0>and I'll mark this as related.

00:07:24.905 --> 00:07:27.145
<v Speaker 0>I'll pop over to config maps, where I'll

00:07:27.145 --> 00:07:29.625
<v Speaker 0>select the API server one, and I'll pick

00:07:29.625 --> 00:07:31.545
<v Speaker 0>one more, which is to pop over to

00:07:31.545 --> 00:07:33.625
<v Speaker 0>secret and sacred fan out config.

00:07:34.440 --> 00:07:36.600
<v Speaker 0>We apply the selection and now you'll see

00:07:36.600 --> 00:07:39.240
<v Speaker 0>that the events listed for this resource include

00:07:39.240 --> 00:07:40.920
<v Speaker 0>their related resources.

00:07:41.240 --> 00:07:43.080
<v Speaker 0>Now I think this feature could be improved.

00:07:43.080 --> 00:07:43.640
<v Speaker 0>I'd love to see

00:07:44.440 --> 00:07:46.840
<v Speaker 0>scan the YAML for

00:07:46.840 --> 00:07:50.495
<v Speaker 0>reference config maps, secrets and and services with

00:07:50.495 --> 00:07:51.775
<v Speaker 0>matching selectors

00:07:52.175 --> 00:07:54.735
<v Speaker 0>and hook this up for me. However, doing

00:07:54.735 --> 00:07:56.735
<v Speaker 0>it manually for those few resources that I

00:07:56.735 --> 00:07:58.655
<v Speaker 0>do want to group collectively

00:07:58.655 --> 00:08:00.575
<v Speaker 0>isn't exactly at the end of the world.

00:08:01.129 --> 00:08:03.129
<v Speaker 0>It's a cool feature and one that could

00:08:03.129 --> 00:08:05.770
<v Speaker 0>have some really interesting improvements over time.

00:08:07.210 --> 00:08:09.210
<v Speaker 0>So let's go back to the information screen

00:08:09.210 --> 00:08:11.289
<v Speaker 0>and we'll see these best practice warnings.

00:08:13.395 --> 00:08:15.475
<v Speaker 0>So when we click this, we have a

00:08:15.475 --> 00:08:17.475
<v Speaker 0>bunch of checks and here we can see

00:08:17.475 --> 00:08:19.635
<v Speaker 0>that our deployment has one replica.

00:08:19.875 --> 00:08:22.195
<v Speaker 0>Now this is a warning just because if

00:08:22.195 --> 00:08:24.595
<v Speaker 0>we lose that replica, we've lost our service.

00:08:24.675 --> 00:08:27.075
<v Speaker 0>So maybe you want to run two or

00:08:27.075 --> 00:08:27.315
<v Speaker 0>three.

00:08:27.890 --> 00:08:29.970
<v Speaker 0>However, you know your services

00:08:30.130 --> 00:08:32.530
<v Speaker 0>better than any tool can. So feel free

00:08:32.530 --> 00:08:33.970
<v Speaker 0>to use the ignore button.

00:08:34.370 --> 00:08:36.450
<v Speaker 0>For Grafana, maybe we determine that we do

00:08:36.450 --> 00:08:38.130
<v Speaker 0>only ever want one and we're happy for

00:08:38.130 --> 00:08:40.414
<v Speaker 0>that to be offline if something goes wrong.

00:08:41.294 --> 00:08:43.855
<v Speaker 0>We can just say ignore for fourteen days,

00:08:43.855 --> 00:08:46.095
<v Speaker 0>thirty days, ninety days or forever.

00:08:46.735 --> 00:08:48.415
<v Speaker 0>So perhaps I'm not ready to make a

00:08:48.415 --> 00:08:50.095
<v Speaker 0>decision on whether this is good or bad

00:08:50.095 --> 00:08:51.615
<v Speaker 0>yet and I'll ignore it for a couple

00:08:51.615 --> 00:08:51.935
<v Speaker 0>of weeks.

00:08:54.140 --> 00:08:56.220
<v Speaker 0>Next we have a critical warning telling us

00:08:56.220 --> 00:08:59.420
<v Speaker 0>that this workload has no liveness probe.

00:08:59.980 --> 00:09:01.900
<v Speaker 0>If we expand it, it tells us

00:09:02.620 --> 00:09:04.940
<v Speaker 0>that liveness probes are sustained to ensure that

00:09:04.940 --> 00:09:07.180
<v Speaker 0>an application stays on a healthy state. When

00:09:07.180 --> 00:09:09.074
<v Speaker 0>a liveness probe fails, the pod will be

00:09:09.074 --> 00:09:09.875
<v Speaker 0>restarted.

00:09:10.675 --> 00:09:13.074
<v Speaker 0>This is a pretty neat behavior, the Kubelet

00:09:13.074 --> 00:09:15.635
<v Speaker 0>monitors our workloads and if it needs to

00:09:15.635 --> 00:09:17.555
<v Speaker 0>kick them, it kicks them.

00:09:17.954 --> 00:09:19.475
<v Speaker 0>So you should always try and have some

00:09:19.475 --> 00:09:22.115
<v Speaker 0>of these best practices whenever possible and Komodor

00:09:22.115 --> 00:09:23.555
<v Speaker 0>brings up front and center.

00:09:23.954 --> 00:09:25.329
<v Speaker 0>So I'm not going to ignore that one

00:09:25.329 --> 00:09:26.769
<v Speaker 0>because you know what, I should have a

00:09:26.769 --> 00:09:28.209
<v Speaker 0>liveness probe in this workload.

00:09:29.490 --> 00:09:32.050
<v Speaker 0>Now we've got some past ones here where

00:09:32.050 --> 00:09:34.370
<v Speaker 0>we have a readiness probe, we have CPU

00:09:34.370 --> 00:09:35.730
<v Speaker 0>and memory constraints

00:09:37.730 --> 00:09:39.410
<v Speaker 0>and the last warning is just the pull

00:09:39.410 --> 00:09:39.889
<v Speaker 0>policy.

00:09:40.345 --> 00:09:42.505
<v Speaker 0>It's not good practice to have an image

00:09:42.505 --> 00:09:43.865
<v Speaker 0>pull policy of always.

00:09:45.145 --> 00:09:47.065
<v Speaker 0>Usually preferred to set it to if not

00:09:47.065 --> 00:09:49.065
<v Speaker 0>present, it just means when a work remotely

00:09:49.065 --> 00:09:50.505
<v Speaker 0>starts, you don't need to go to an

00:09:50.505 --> 00:09:52.025
<v Speaker 0>image registry and see if it can be

00:09:52.025 --> 00:09:54.505
<v Speaker 0>pulled down and it usually means you're using

00:09:54.505 --> 00:09:56.450
<v Speaker 0>some sort of alias tag system.

00:09:56.770 --> 00:09:58.210
<v Speaker 0>Again, we want to kind of get away

00:09:58.210 --> 00:09:59.650
<v Speaker 0>from that as much as possible.

00:09:59.970 --> 00:10:01.410
<v Speaker 0>So it's not a critical but it is

00:10:01.410 --> 00:10:03.090
<v Speaker 0>a warning that you maybe you need to

00:10:03.090 --> 00:10:04.130
<v Speaker 0>update this.

00:10:05.010 --> 00:10:06.450
<v Speaker 0>And I think this is a nice way

00:10:06.450 --> 00:10:08.865
<v Speaker 0>to gain more insights and understanding of the

00:10:08.865 --> 00:10:10.465
<v Speaker 0>services within our cluster.

00:10:10.945 --> 00:10:12.385
<v Speaker 0>So let's see what else we can do

00:10:12.385 --> 00:10:13.745
<v Speaker 0>from the events page.

00:10:15.185 --> 00:10:17.425
<v Speaker 0>So I'm not gonna filter on an individual

00:10:17.425 --> 00:10:20.145
<v Speaker 0>service. I don't think that shows anything new,

00:10:20.545 --> 00:10:22.225
<v Speaker 0>but if we scroll down we could filter

00:10:22.225 --> 00:10:23.585
<v Speaker 0>on the event type.

00:10:24.790 --> 00:10:26.629
<v Speaker 0>So let's filter by one of these event

00:10:26.629 --> 00:10:29.029
<v Speaker 0>types and see what information we get back.

00:10:29.670 --> 00:10:31.190
<v Speaker 0>Let's start with one of the most common

00:10:31.190 --> 00:10:32.949
<v Speaker 0>ones which is config change.

00:10:33.269 --> 00:10:34.550
<v Speaker 0>This is going to tell you when a

00:10:34.550 --> 00:10:37.110
<v Speaker 0>conflict map is created, modified or deleted within

00:10:37.110 --> 00:10:37.670
<v Speaker 0>your cluster.

00:10:38.985 --> 00:10:39.625
<v Speaker 0>So

00:10:39.945 --> 00:10:41.065
<v Speaker 0>let's create one.

00:10:41.545 --> 00:10:44.105
<v Speaker 0>Here I have cm. Yml, which is a

00:10:44.105 --> 00:10:45.865
<v Speaker 0>ConfigMap called Rawkode.

00:10:46.745 --> 00:10:48.665
<v Speaker 0>When we go to the terminal,

00:10:49.385 --> 00:10:52.185
<v Speaker 0>we can apply this to our monitoring namespace.

00:10:55.860 --> 00:10:58.100
<v Speaker 0>Let's make a quick change to this conflict

00:10:58.100 --> 00:10:59.860
<v Speaker 0>map and say that we no longer want

00:10:59.860 --> 00:11:01.460
<v Speaker 0>key value, instead

00:11:01.620 --> 00:11:02.500
<v Speaker 0>we want

00:11:04.020 --> 00:11:04.660
<v Speaker 0>name

00:11:05.220 --> 00:11:05.780
<v Speaker 0>David.

00:11:07.125 --> 00:11:08.405
<v Speaker 0>Go to our terminal,

00:11:08.805 --> 00:11:10.405
<v Speaker 0>apply this one more time,

00:11:10.645 --> 00:11:12.885
<v Speaker 0>and let's go visualize this with Komodor.

00:11:14.485 --> 00:11:16.325
<v Speaker 0>So right away, we can see

00:11:16.885 --> 00:11:19.765
<v Speaker 0>that a ConfigMap was created in the monitor

00:11:19.765 --> 00:11:21.205
<v Speaker 0>namespace called Rawkode.

00:11:22.110 --> 00:11:23.790
<v Speaker 0>We click on this, we have all green

00:11:23.790 --> 00:11:25.709
<v Speaker 0>because it was the first time this ConfigMap

00:11:25.709 --> 00:11:26.510
<v Speaker 0>was created.

00:11:27.230 --> 00:11:29.470
<v Speaker 0>We then have our change. And this time

00:11:29.470 --> 00:11:31.790
<v Speaker 0>we can see that the key value was

00:11:31.790 --> 00:11:34.510
<v Speaker 0>removed and name David was added.

00:11:35.149 --> 00:11:36.485
<v Speaker 0>And if you want to view this in

00:11:36.485 --> 00:11:39.125
<v Speaker 0>more details, you can expand the depth.

00:11:39.285 --> 00:11:41.125
<v Speaker 0>We can see the data changed

00:11:42.725 --> 00:11:45.285
<v Speaker 0>along with some metadata about the resource as

00:11:45.285 --> 00:11:45.845
<v Speaker 0>well.

00:11:47.925 --> 00:11:50.870
<v Speaker 0>This is one of these really simple but

00:11:50.870 --> 00:11:52.230
<v Speaker 0>very valuable features.

00:11:52.630 --> 00:11:55.029
<v Speaker 0>When things go wrong on a Kubernetes cluster,

00:11:55.029 --> 00:11:57.510
<v Speaker 0>it's not because the resources haven't changed, it's

00:11:57.510 --> 00:11:59.750
<v Speaker 0>because of our changes that things sometimes go

00:11:59.750 --> 00:12:02.310
<v Speaker 0>wrong. Human error is probably still the biggest

00:12:02.310 --> 00:12:04.550
<v Speaker 0>cause of problems on a Kubernetes cluster.

00:12:05.495 --> 00:12:08.295
<v Speaker 0>So it's crucial that you understand

00:12:08.295 --> 00:12:10.615
<v Speaker 0>when config changes in your cluster and how

00:12:10.615 --> 00:12:12.455
<v Speaker 0>that can have a cascading effect on the

00:12:12.455 --> 00:12:14.295
<v Speaker 0>workloads within your cluster.

00:12:15.575 --> 00:12:17.895
<v Speaker 0>And your ability to see those changes as

00:12:17.895 --> 00:12:18.695
<v Speaker 0>they happen

00:12:19.575 --> 00:12:22.610
<v Speaker 0>will substantially lower your mean time to recovery.

00:12:24.450 --> 00:12:26.370
<v Speaker 0>So beyond config change,

00:12:27.890 --> 00:12:29.090
<v Speaker 0>let's filter

00:12:29.170 --> 00:12:31.010
<v Speaker 0>on availability issues.

00:12:31.730 --> 00:12:35.250
<v Speaker 0>Now availability issues give us an understanding of

00:12:35.250 --> 00:12:37.605
<v Speaker 0>when a workload was unavailable.

00:12:39.365 --> 00:12:41.605
<v Speaker 0>Perhaps because the pod was being restarted or

00:12:41.605 --> 00:12:43.125
<v Speaker 0>the probes were failing.

00:12:43.925 --> 00:12:45.925
<v Speaker 0>If we take a look at the Grafana

00:12:45.925 --> 00:12:46.725
<v Speaker 0>one here,

00:12:47.045 --> 00:12:49.470
<v Speaker 0>you can see that pod was unhealthy and

00:12:49.470 --> 00:12:51.470
<v Speaker 0>why was unhealthy? Well,

00:12:52.110 --> 00:12:53.950
<v Speaker 0>it was container creating.

00:12:54.190 --> 00:12:55.790
<v Speaker 0>Of course it's not healthy if it's just

00:12:55.790 --> 00:12:56.430
<v Speaker 0>creating.

00:12:57.470 --> 00:12:59.149
<v Speaker 0>Also what's nice here is it shows you

00:12:59.149 --> 00:13:00.990
<v Speaker 0>each of the containers and the status for

00:13:00.990 --> 00:13:01.709
<v Speaker 0>them too.

00:13:03.175 --> 00:13:04.855
<v Speaker 0>If you want, you can click the live

00:13:04.855 --> 00:13:06.615
<v Speaker 0>pods and logs button.

00:13:09.175 --> 00:13:11.575
<v Speaker 0>This will show us the current pod and

00:13:11.575 --> 00:13:13.495
<v Speaker 0>our cluster for that selector

00:13:14.040 --> 00:13:16.120
<v Speaker 0>where we could pop it open and go

00:13:16.120 --> 00:13:18.680
<v Speaker 0>to logs. So it's nice having our logs

00:13:18.680 --> 00:13:20.920
<v Speaker 0>right front and center when required.

00:13:21.480 --> 00:13:23.160
<v Speaker 0>If we pop back to details,

00:13:23.240 --> 00:13:24.759
<v Speaker 0>we can see the conditions

00:13:26.214 --> 00:13:28.215
<v Speaker 0>that tells if our pod is healthy.

00:13:29.095 --> 00:13:31.495
<v Speaker 0>We have the containers, the images are running,

00:13:31.495 --> 00:13:33.335
<v Speaker 0>the pull policy, the ports, the mounts and

00:13:33.335 --> 00:13:34.214
<v Speaker 0>arguments,

00:13:34.214 --> 00:13:36.215
<v Speaker 0>all the useful information that you need.

00:13:36.615 --> 00:13:38.775
<v Speaker 0>We have the ability to see the tolerations,

00:13:39.735 --> 00:13:40.535
<v Speaker 0>the volumes

00:13:42.350 --> 00:13:44.670
<v Speaker 0>and of course the events associated with this

00:13:44.670 --> 00:13:45.390
<v Speaker 0>workload.

00:13:46.190 --> 00:13:48.110
<v Speaker 0>If you're a fan of the kubectl describe

00:13:48.110 --> 00:13:50.830
<v Speaker 0>command, you can click describe and get the

00:13:50.830 --> 00:13:52.830
<v Speaker 0>exact output on the screen as so.

00:13:54.845 --> 00:13:57.725
<v Speaker 0>So to reiterate from the events page, we've

00:13:57.725 --> 00:13:59.725
<v Speaker 0>seen a pod with availability issues.

00:13:59.805 --> 00:14:01.245
<v Speaker 0>We went to the current instance of this

00:14:01.245 --> 00:14:03.324
<v Speaker 0>pod, we've seen there was no problem and

00:14:03.324 --> 00:14:04.925
<v Speaker 0>we had all the information we need to

00:14:04.925 --> 00:14:05.964
<v Speaker 0>debug a problem.

00:14:06.830 --> 00:14:09.470
<v Speaker 0>To debug if there was something wrong.

00:14:10.030 --> 00:14:12.350
<v Speaker 0>Now the rest of the Komodor UI is

00:14:12.350 --> 00:14:14.030
<v Speaker 0>pretty much more of the same.

00:14:15.230 --> 00:14:17.790
<v Speaker 0>We can break down all of the resources.

00:14:17.870 --> 00:14:19.630
<v Speaker 0>We can see nodes. We can click on

00:14:19.630 --> 00:14:20.110
<v Speaker 0>a node.

00:14:21.115 --> 00:14:22.955
<v Speaker 0>We can see all of its conditions,

00:14:23.355 --> 00:14:24.475
<v Speaker 0>the capacity,

00:14:26.075 --> 00:14:28.235
<v Speaker 0>and allocatable resources

00:14:28.235 --> 00:14:31.035
<v Speaker 0>across CPU, memory, storage, etcetera.

00:14:31.915 --> 00:14:33.995
<v Speaker 0>We have the ability to cordon and drain

00:14:33.995 --> 00:14:35.115
<v Speaker 0>a node if we wish.

00:14:36.280 --> 00:14:39.160
<v Speaker 0>For workloads, it's the same. We have deployments.

00:14:39.160 --> 00:14:40.840
<v Speaker 0>We can click. We can edit. We can

00:14:40.840 --> 00:14:42.440
<v Speaker 0>scale. We can restart.

00:14:43.320 --> 00:14:44.840
<v Speaker 0>You can do this for most of the

00:14:44.840 --> 00:14:46.520
<v Speaker 0>resources within your cluster.

00:14:47.000 --> 00:14:48.840
<v Speaker 0>For storage, we can see storage classes.

00:14:49.405 --> 00:14:51.805
<v Speaker 0>For config, we go to config maps.

00:14:56.605 --> 00:14:57.885
<v Speaker 0>And we can see them.

00:14:58.765 --> 00:15:01.885
<v Speaker 0>Pretty much, you're getting a visual representation

00:15:01.965 --> 00:15:04.685
<v Speaker 0>of everything you can do with the kubect

00:15:03.930 --> 00:15:05.290
<v Speaker 0>ectl command.

00:15:06.970 --> 00:15:09.529
<v Speaker 0>We can even list the custom resource definitions

00:15:09.529 --> 00:15:11.050
<v Speaker 0>within our cluster

00:15:13.050 --> 00:15:14.090
<v Speaker 0>and search.

00:15:16.330 --> 00:15:18.410
<v Speaker 0>So I'm not going to spend any more

00:15:18.410 --> 00:15:19.770
<v Speaker 0>time going through this

00:15:20.115 --> 00:15:22.115
<v Speaker 0>because these webpages are dashboards

00:15:22.275 --> 00:15:24.515
<v Speaker 0>and as we all know dashboards are not

00:15:24.515 --> 00:15:27.475
<v Speaker 0>to be looked at until something goes wrong.

00:15:29.075 --> 00:15:31.875
<v Speaker 0>So how do we get information from Komodor

00:15:31.875 --> 00:15:34.355
<v Speaker 0>to give us alerts when our attention is

00:15:34.355 --> 00:15:34.675
<v Speaker 0>needed?

00:15:35.810 --> 00:15:38.210
<v Speaker 0>And for that we have monitors.

00:15:39.570 --> 00:15:41.410
<v Speaker 0>We can expand our cluster

00:15:42.050 --> 00:15:43.650
<v Speaker 0>and we can see that we have some

00:15:43.650 --> 00:15:46.210
<v Speaker 0>rules already in place. These are shipped by

00:15:46.210 --> 00:15:48.050
<v Speaker 0>default by Komodor.

00:15:48.130 --> 00:15:50.545
<v Speaker 0>We have an availability monitor. This will let

00:15:50.545 --> 00:15:52.144
<v Speaker 0>us know if any of our workloads are

00:15:52.144 --> 00:15:54.545
<v Speaker 0>less than 80% for more than ten seconds.

00:15:54.865 --> 00:15:57.105
<v Speaker 0>If we need 10 pods on a deployment

00:15:57.345 --> 00:15:59.264
<v Speaker 0>and for more than ten seconds, we have

00:15:59.264 --> 00:16:01.745
<v Speaker 0>seven or less, we'll get an alert.

00:16:02.945 --> 00:16:04.865
<v Speaker 0>If our cron jobs are failing, we get

00:16:04.865 --> 00:16:05.345
<v Speaker 0>an alert.

00:16:06.760 --> 00:16:08.840
<v Speaker 0>We can get alerts for when deployments are

00:16:08.840 --> 00:16:09.560
<v Speaker 0>updated

00:16:12.120 --> 00:16:13.960
<v Speaker 0>and we can also get alerts for when

00:16:13.960 --> 00:16:15.400
<v Speaker 0>our nodes are not healthy.

00:16:17.160 --> 00:16:18.600
<v Speaker 0>So let's take a look at one of

00:16:18.600 --> 00:16:20.955
<v Speaker 0>these alerts and then configure our own.

00:16:23.035 --> 00:16:25.755
<v Speaker 0>Here is the deployment alert. This is going

00:16:25.755 --> 00:16:28.235
<v Speaker 0>to let us know whenever a workload is

00:16:28.235 --> 00:16:29.835
<v Speaker 0>modified within our cluster.

00:16:30.475 --> 00:16:32.475
<v Speaker 0>Using the integrations that you have configured with

00:16:32.475 --> 00:16:33.195
<v Speaker 0>Komodor,

00:16:33.355 --> 00:16:35.870
<v Speaker 0>you can use these as destinations. We

00:16:36.670 --> 00:16:38.910
<v Speaker 0>can use a standard webhook or publish a

00:16:38.910 --> 00:16:40.030
<v Speaker 0>message to Slack.

00:16:40.510 --> 00:16:42.510
<v Speaker 0>I have a channel called SRE

00:16:43.390 --> 00:16:44.990
<v Speaker 0>and I'm going to click save.

00:16:45.870 --> 00:16:48.270
<v Speaker 0>So now if we modify a deployment, we

00:16:48.270 --> 00:16:49.470
<v Speaker 0>should get a notification

00:16:49.755 --> 00:16:51.355
<v Speaker 0>to my Slack channel.

00:16:52.395 --> 00:16:54.795
<v Speaker 0>So let's test it. I have my Slack

00:16:54.795 --> 00:16:57.755
<v Speaker 0>here, but first we need a deployment change.

00:16:57.995 --> 00:17:00.075
<v Speaker 0>So I'm just going to do this through

00:17:00.155 --> 00:17:01.355
<v Speaker 0>the Komodor UI.

00:17:02.500 --> 00:17:04.099
<v Speaker 0>I'm gonna go to deployments,

00:17:05.059 --> 00:17:07.700
<v Speaker 0>and we'll modify the cert manager deployment.

00:17:08.900 --> 00:17:10.659
<v Speaker 0>We can click on edit YAML,

00:17:14.579 --> 00:17:16.020
<v Speaker 0>where I'm just going to add a new

00:17:16.020 --> 00:17:16.500
<v Speaker 0>label.

00:17:18.044 --> 00:17:19.325
<v Speaker 0>We hit apply,

00:17:21.005 --> 00:17:22.845
<v Speaker 0>we can see that we have new events

00:17:23.484 --> 00:17:25.405
<v Speaker 0>and we can see that our manual action

00:17:25.405 --> 00:17:26.924
<v Speaker 0>to edit a deployment,

00:17:27.325 --> 00:17:29.404
<v Speaker 0>we click on it, we see the change.

00:17:29.725 --> 00:17:31.405
<v Speaker 0>So even though we can see the deployment

00:17:31.405 --> 00:17:35.320
<v Speaker 0>changed here, this will not trigger a Slack

00:17:35.320 --> 00:17:36.200
<v Speaker 0>notification

00:17:37.400 --> 00:17:41.080
<v Speaker 0>because it uses the resources generation

00:17:41.560 --> 00:17:43.559
<v Speaker 0>rather than a resource version.

00:17:44.520 --> 00:17:46.365
<v Speaker 0>Which is good because

00:17:46.845 --> 00:17:49.165
<v Speaker 0>that we really need a notification that a

00:17:49.165 --> 00:17:51.085
<v Speaker 0>label changed on a workload when the workload

00:17:51.085 --> 00:17:54.285
<v Speaker 0>itself is not restarted, rescheduled, or modified.

00:17:55.245 --> 00:17:57.005
<v Speaker 0>Now let's make one more change to our

00:17:57.005 --> 00:17:57.405
<v Speaker 0>resource.

00:17:58.130 --> 00:18:00.370
<v Speaker 0>This time we're going to add an environment

00:18:00.370 --> 00:18:01.169
<v Speaker 0>variable,

00:18:05.809 --> 00:18:07.809
<v Speaker 0>which is going to have a value

00:18:08.610 --> 00:18:09.250
<v Speaker 0>of high.

00:18:10.654 --> 00:18:12.414
<v Speaker 0>Now this will trigger

00:18:13.054 --> 00:18:14.815
<v Speaker 0>a new generation.

00:18:15.375 --> 00:18:17.134
<v Speaker 0>And if we pop over to Slack,

00:18:17.615 --> 00:18:20.414
<v Speaker 0>we'll see the notification and the SRE channel.

00:18:21.534 --> 00:18:23.690
<v Speaker 0>And if we click on this, it takes

00:18:23.690 --> 00:18:24.730
<v Speaker 0>us directly

00:18:24.810 --> 00:18:26.890
<v Speaker 0>to the event with the change.

00:18:27.930 --> 00:18:30.490
<v Speaker 0>We can see that the revision and generation

00:18:30.490 --> 00:18:31.210
<v Speaker 0>of this

00:18:31.690 --> 00:18:34.730
<v Speaker 0>resource went from one to two because of

00:18:34.730 --> 00:18:36.330
<v Speaker 0>an environment variable addition.

00:18:37.165 --> 00:18:38.445
<v Speaker 0>This workload

00:18:38.845 --> 00:18:41.805
<v Speaker 0>change is also denoted here by the new

00:18:41.805 --> 00:18:44.125
<v Speaker 0>deployment which tells us that the image doesn't

00:18:44.125 --> 00:18:45.325
<v Speaker 0>change but

00:18:45.565 --> 00:18:47.245
<v Speaker 0>other aspects did.

00:18:47.565 --> 00:18:50.925
<v Speaker 0>So given that we have a pretty sophisticated

00:18:51.150 --> 00:18:53.470
<v Speaker 0>troubleshooting and debugging tool here,

00:18:54.110 --> 00:18:56.990
<v Speaker 0>it's also worth noting that Komodor has

00:18:57.950 --> 00:18:59.470
<v Speaker 0>pretty elevated

00:18:59.470 --> 00:19:02.030
<v Speaker 0>privileged access to your cluster

00:19:02.750 --> 00:19:04.430
<v Speaker 0>and as such we need to be able

00:19:04.430 --> 00:19:05.150
<v Speaker 0>to trust it.

00:19:06.745 --> 00:19:09.144
<v Speaker 0>Luckily, Komodor provided the ability

00:19:09.465 --> 00:19:12.424
<v Speaker 0>to protect some of our more sensitive information

00:19:12.424 --> 00:19:15.065
<v Speaker 0>from being leaked through the Komodor UI.

00:19:18.025 --> 00:19:19.784
<v Speaker 0>Let's go to resources,

00:19:19.785 --> 00:19:21.065
<v Speaker 0>workloads, and pods.

00:19:21.750 --> 00:19:24.070
<v Speaker 0>If I select the default namespace,

00:19:24.310 --> 00:19:27.750
<v Speaker 0>I have a new super secret workload.

00:19:28.630 --> 00:19:30.150
<v Speaker 0>If we click on this, there's not a

00:19:30.150 --> 00:19:31.670
<v Speaker 0>lot to see here, but if we go

00:19:31.670 --> 00:19:33.669
<v Speaker 0>to the logs, we have

00:19:34.470 --> 00:19:35.190
<v Speaker 0>a password.

00:19:36.315 --> 00:19:39.355
<v Speaker 0>How do we prevent Komodor from leaking such

00:19:39.355 --> 00:19:40.075
<v Speaker 0>information?

00:19:40.315 --> 00:19:42.635
<v Speaker 0>Now it doesn't have to be just an

00:19:42.635 --> 00:19:45.755
<v Speaker 0>or standard out logging although when applications crash

00:19:45.755 --> 00:19:48.460
<v Speaker 0>sometimes they do dump the environment revealing very

00:19:48.460 --> 00:19:49.580
<v Speaker 0>sensitive information

00:19:50.060 --> 00:19:53.100
<v Speaker 0>but also how do we redact this from

00:19:53.100 --> 00:19:56.220
<v Speaker 0>config maps and other sources of sensitive information.

00:19:56.540 --> 00:19:59.660
<v Speaker 0>Fortunately by default Komodor hashes all the information

00:19:59.660 --> 00:20:02.140
<v Speaker 0>that it pulls from secret resources

00:20:02.975 --> 00:20:04.575
<v Speaker 0>But we do need to put in a

00:20:04.575 --> 00:20:06.815
<v Speaker 0>couple of extra steps to protect standard IO

00:20:06.815 --> 00:20:09.535
<v Speaker 0>and our config maps. Although I hope you're

00:20:09.535 --> 00:20:11.535
<v Speaker 0>not storing too much sensitive information in a

00:20:11.535 --> 00:20:12.495
<v Speaker 0>config map.

00:20:13.855 --> 00:20:15.769
<v Speaker 0>And I'm gonna configure this

00:20:16.250 --> 00:20:18.009
<v Speaker 0>through the Commodore UI.

00:20:18.809 --> 00:20:20.010
<v Speaker 0>So the first thing we want to do

00:20:20.010 --> 00:20:22.410
<v Speaker 0>is to go to configuration and config maps

00:20:22.410 --> 00:20:25.129
<v Speaker 0>where I can select the Commodore namespace.

00:20:25.290 --> 00:20:28.169
<v Speaker 0>Here we have the Kubernetes watcher config

00:20:28.490 --> 00:20:30.010
<v Speaker 0>and I'm going to go straight to the

00:20:30.010 --> 00:20:30.650
<v Speaker 0>edit page.

00:20:32.784 --> 00:20:34.625
<v Speaker 0>You'll see we have two settings here on

00:20:34.625 --> 00:20:36.705
<v Speaker 0>lines twenty and twenty one called redact and

00:20:36.705 --> 00:20:37.984
<v Speaker 0>redact logs.

00:20:40.385 --> 00:20:43.265
<v Speaker 0>These take a list of expressions to redact

00:20:43.265 --> 00:20:46.700
<v Speaker 0>from Kubernetes resources and from log data.

00:20:47.500 --> 00:20:49.580
<v Speaker 0>Now this accepts regex

00:20:49.580 --> 00:20:51.660
<v Speaker 0>pattern matching like you can do with any

00:20:51.660 --> 00:20:53.419
<v Speaker 0>sophisticated login library

00:20:53.500 --> 00:20:54.940
<v Speaker 0>but I'm going to keep it very simple

00:20:54.940 --> 00:20:55.980
<v Speaker 0>for this demo.

00:20:57.260 --> 00:20:59.260
<v Speaker 0>I'm going to explicitly say that I want

00:20:59.260 --> 00:21:01.544
<v Speaker 0>my password one two three omitted

00:21:02.585 --> 00:21:04.024
<v Speaker 0>and we'll add one more.

00:21:05.225 --> 00:21:07.385
<v Speaker 0>This time we'll do a regex match for

00:21:07.385 --> 00:21:09.945
<v Speaker 0>anything that looks like password equals

00:21:10.345 --> 00:21:13.465
<v Speaker 0>then we'll open a matcher to dot star

00:21:14.830 --> 00:21:17.149
<v Speaker 0>and we'll stick a space on the end.

00:21:18.510 --> 00:21:20.590
<v Speaker 0>So now we will have to kick the

00:21:20.590 --> 00:21:21.869
<v Speaker 0>Komodor agent

00:21:22.270 --> 00:21:24.270
<v Speaker 0>so that reloads its configuration.

00:21:26.794 --> 00:21:28.554
<v Speaker 0>We can just delete.

00:21:34.794 --> 00:21:36.235
<v Speaker 0>And, already, we can see we have a

00:21:36.235 --> 00:21:37.595
<v Speaker 0>new one running.

00:21:38.395 --> 00:21:41.115
<v Speaker 0>So let's go back to our pod and

00:21:41.115 --> 00:21:42.554
<v Speaker 0>the default namespace

00:21:45.649 --> 00:21:47.809
<v Speaker 0>where we have our super secret

00:21:47.970 --> 00:21:48.849
<v Speaker 0>workload.

00:21:51.090 --> 00:21:52.769
<v Speaker 0>And if we view the logs,

00:21:52.850 --> 00:21:55.730
<v Speaker 0>our password one two three has been redacted.

00:21:58.205 --> 00:22:00.044
<v Speaker 0>So let's modify this

00:22:00.365 --> 00:22:01.965
<v Speaker 0>at the deployment level

00:22:04.925 --> 00:22:06.684
<v Speaker 0>where we can say edit YAML.

00:22:08.845 --> 00:22:10.845
<v Speaker 0>And we have password one two three, but

00:22:10.845 --> 00:22:13.039
<v Speaker 0>let's also add password equals

00:22:13.760 --> 00:22:14.559
<v Speaker 0>hello

00:22:15.280 --> 00:22:17.519
<v Speaker 0>not secret like so.

00:22:19.600 --> 00:22:21.680
<v Speaker 0>We'll cause this pod, which we can see

00:22:21.680 --> 00:22:23.920
<v Speaker 0>here, to be terminated with a new one

00:22:23.920 --> 00:22:24.240
<v Speaker 0>running.

00:22:25.045 --> 00:22:26.645
<v Speaker 0>And if we pop open the logs for

00:22:26.645 --> 00:22:27.205
<v Speaker 0>here,

00:22:27.525 --> 00:22:29.285
<v Speaker 0>we can see that both values

00:22:29.445 --> 00:22:31.605
<v Speaker 0>have been properly redacted.

00:22:32.965 --> 00:22:35.044
<v Speaker 0>So this is a very important feature,

00:22:35.765 --> 00:22:38.245
<v Speaker 0>but also a very cumbersome feature

00:22:38.850 --> 00:22:41.570
<v Speaker 0>because security is hard. It's never easy.

00:22:42.130 --> 00:22:44.130
<v Speaker 0>It would probably be worthwhile for your team

00:22:44.130 --> 00:22:46.369
<v Speaker 0>or organization to have convention

00:22:47.170 --> 00:22:50.049
<v Speaker 0>to, well, not log sensitive values.

00:22:50.450 --> 00:22:52.290
<v Speaker 0>But if you do, always make sure there's

00:22:52.290 --> 00:22:53.410
<v Speaker 0>some marker in place

00:22:54.005 --> 00:22:56.325
<v Speaker 0>so you can configure tools like Komodor and

00:22:56.325 --> 00:22:57.845
<v Speaker 0>other logging systems

00:22:58.085 --> 00:23:00.725
<v Speaker 0>to redact that information as fast as possible.

00:23:01.445 --> 00:23:02.085
<v Speaker 0>And

00:23:02.325 --> 00:23:05.125
<v Speaker 0>you can even run the container locally to

00:23:05.125 --> 00:23:08.360
<v Speaker 0>test your redactions before pushing them to your

00:23:08.360 --> 00:23:09.720
<v Speaker 0>Kubernetes cluster.

00:23:10.360 --> 00:23:11.559
<v Speaker 0>Let's take a look.

00:23:13.400 --> 00:23:14.920
<v Speaker 0>If I go to my terminal,

00:23:15.080 --> 00:23:16.440
<v Speaker 0>I have a just fail.

00:23:17.320 --> 00:23:19.880
<v Speaker 0>It's like a make fail. However, it allows

00:23:19.880 --> 00:23:21.640
<v Speaker 0>positional arguments on targets

00:23:21.934 --> 00:23:23.775
<v Speaker 0>and is generally just a little bit nicer

00:23:23.775 --> 00:23:24.575
<v Speaker 0>to work with.

00:23:26.495 --> 00:23:28.735
<v Speaker 0>From here we can see redact and you

00:23:28.735 --> 00:23:30.655
<v Speaker 0>can already see the autocomplete here in the

00:23:30.655 --> 00:23:32.895
<v Speaker 0>documentation from the just file but we provide

00:23:32.895 --> 00:23:34.255
<v Speaker 0>a redaction phrase

00:23:34.440 --> 00:23:35.639
<v Speaker 0>and a logline.

00:23:37.000 --> 00:23:39.639
<v Speaker 0>I'm gonna say this redact raw code

00:23:39.960 --> 00:23:42.039
<v Speaker 0>and then the logline that I wanna test

00:23:42.039 --> 00:23:44.759
<v Speaker 0>is to say I am raw code

00:23:44.840 --> 00:23:46.519
<v Speaker 0>beer me type.

00:23:47.240 --> 00:23:48.919
<v Speaker 0>This pulls down the container image,

00:23:49.605 --> 00:23:51.365
<v Speaker 0>does a little bit of plumbing and then

00:23:51.365 --> 00:23:53.765
<v Speaker 0>shows you the input log and output and

00:23:53.765 --> 00:23:55.445
<v Speaker 0>you can see what it was before and

00:23:55.445 --> 00:23:57.205
<v Speaker 0>after the redaction.

00:23:59.205 --> 00:24:01.845
<v Speaker 0>This means that you can test your regex

00:24:01.845 --> 00:24:03.045
<v Speaker 0>patterns all you want.

00:24:03.960 --> 00:24:05.719
<v Speaker 0>Say you want to do password

00:24:05.960 --> 00:24:06.840
<v Speaker 0>equals

00:24:07.559 --> 00:24:08.679
<v Speaker 0>dot star

00:24:09.559 --> 00:24:10.679
<v Speaker 0>question mark

00:24:12.360 --> 00:24:14.679
<v Speaker 0>Rawkode because maybe we got it wrong

00:24:14.919 --> 00:24:16.519
<v Speaker 0>and then in our test

00:24:17.000 --> 00:24:18.120
<v Speaker 0>we'll do password

00:24:19.125 --> 00:24:20.164
<v Speaker 0>equals

00:24:20.965 --> 00:24:22.565
<v Speaker 0>blah raw

00:24:23.045 --> 00:24:24.485
<v Speaker 0>code without any.

00:24:25.205 --> 00:24:27.605
<v Speaker 0>Now this won't redact, but we have a

00:24:27.605 --> 00:24:29.044
<v Speaker 0>problem. We can fix it.

00:24:29.445 --> 00:24:31.045
<v Speaker 0>So let's run that again with the e

00:24:31.045 --> 00:24:31.765
<v Speaker 0>on the end

00:24:32.410 --> 00:24:34.330
<v Speaker 0>and oh shit, it's still broken.

00:24:34.650 --> 00:24:37.130
<v Speaker 0>Well clearly, don't know how to spell Rawkode.

00:24:39.530 --> 00:24:41.050
<v Speaker 0>There we go. We did do it right.

00:24:41.050 --> 00:24:41.769
<v Speaker 0>It works.

00:24:42.090 --> 00:24:44.650
<v Speaker 0>That string was redacted because we were able

00:24:44.650 --> 00:24:45.929
<v Speaker 0>to test the regex.

00:24:46.250 --> 00:24:48.465
<v Speaker 0>Well, that's it's really cool. You can actually

00:24:48.465 --> 00:24:50.705
<v Speaker 0>plumb in a shell script a whole bunch

00:24:50.705 --> 00:24:52.785
<v Speaker 0>of example log lines that you have from

00:24:52.785 --> 00:24:55.185
<v Speaker 0>your application with redactions that you know always

00:24:55.185 --> 00:24:56.544
<v Speaker 0>have to be satisfied

00:24:56.545 --> 00:24:59.105
<v Speaker 0>and hook this into your CI system and

00:24:59.105 --> 00:25:01.425
<v Speaker 0>that way you know right away whenever you've

00:25:01.425 --> 00:25:04.070
<v Speaker 0>got secrets leaking that should be redacted.

00:25:05.590 --> 00:25:08.070
<v Speaker 0>So that's a quick overview of Komodor.

00:25:08.149 --> 00:25:09.909
<v Speaker 0>There's an awful lot to love and it

00:25:09.909 --> 00:25:12.710
<v Speaker 0>gives you great visibility into the wonderfully complex

00:25:12.710 --> 00:25:15.830
<v Speaker 0>system that is Kubernetes running our wonderfully complex

00:25:15.830 --> 00:25:17.830
<v Speaker 0>applications which are our microservices.

00:25:18.995 --> 00:25:20.674
<v Speaker 0>This is just part one, there's going to

00:25:20.674 --> 00:25:22.434
<v Speaker 0>be a part two of this video dropping

00:25:22.434 --> 00:25:23.794
<v Speaker 0>early next week.

00:25:24.115 --> 00:25:25.794
<v Speaker 0>In part two we'll be taking a look

00:25:25.794 --> 00:25:27.955
<v Speaker 0>at more of the Komodor integrations.

00:25:28.434 --> 00:25:30.355
<v Speaker 0>We will see in how to integrate your

00:25:30.355 --> 00:25:31.875
<v Speaker 0>source control via GitHub.

00:25:32.290 --> 00:25:33.890
<v Speaker 0>We'll also take a look at hooking up

00:25:33.890 --> 00:25:35.890
<v Speaker 0>to Sentry for exception tracking

00:25:36.690 --> 00:25:39.090
<v Speaker 0>and Grafana and Alert Manager

00:25:39.170 --> 00:25:41.890
<v Speaker 0>giving you full visibility across all of your

00:25:41.890 --> 00:25:42.690
<v Speaker 0>observability

00:25:42.690 --> 00:25:43.170
<v Speaker 0>stack.

00:25:44.195 --> 00:25:45.795
<v Speaker 0>And then at the end of next week,

00:25:45.795 --> 00:25:47.795
<v Speaker 0>part three will drop where we take a

00:25:47.795 --> 00:25:50.515
<v Speaker 0>look at two final features. One, the humble

00:25:50.515 --> 00:25:52.915
<v Speaker 0>webhook and how we can get information from

00:25:52.915 --> 00:25:55.235
<v Speaker 0>Komodor to do whatever the hell we please.

00:25:55.554 --> 00:25:57.315
<v Speaker 0>And then one of my favorite features,

00:25:58.600 --> 00:26:00.360
<v Speaker 0>the vCluster integration.

00:26:01.240 --> 00:26:03.960
<v Speaker 0>Deploying Komodor to all your virtual clusters

00:26:04.200 --> 00:26:06.280
<v Speaker 0>and multitenant Kubernetes environments.

00:26:07.706 --> 00:26:09.226
<v Speaker 0>We'll be back next week with the next

00:26:09.226 --> 00:26:10.986
<v Speaker 0>video. Until then, have a wonderful day, and

00:26:10.986 --> 00:26:11.786
<v Speaker 0>I'll see you all soon.
