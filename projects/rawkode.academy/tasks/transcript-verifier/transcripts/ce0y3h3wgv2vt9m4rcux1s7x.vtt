WEBVTT

NOTE
Transcription provided by Deepgram
Request Id: 5c413dad-cf7c-4d3e-b314-b994857511b1
Created: 2025-04-29T12:51:00.935Z
Duration: 2590.8892
Channels: 1

00:00:00.000 --> 00:00:02.000
<v Speaker 0>Welcome to Cloud Native Compass,

00:00:02.240 --> 00:00:04.799
<v Speaker 0>a podcast to help you navigate the vast

00:00:04.799 --> 00:00:07.040
<v Speaker 0>landscape of the cloud native ecosystem.

00:00:07.040 --> 00:00:09.840
<v Speaker 1>We're your hosts. I'm David Flanagan, a technology

00:00:09.840 --> 00:00:12.320
<v Speaker 1>magpie that can't stop playing with new shiny

00:00:12.320 --> 00:00:15.434
<v Speaker 1>things. I'm Laura Santa Maria, a forever learner

00:00:15.434 --> 00:00:17.435
<v Speaker 0>who is constantly breaking production.

00:00:17.835 --> 00:00:20.635
<v Speaker 0>Alright. This episode of Cloud Native Compass is

00:00:20.635 --> 00:00:21.675
<v Speaker 0>all about AI.

00:00:21.994 --> 00:00:25.435
<v Speaker 0>So David decided to input a request into

00:00:25.435 --> 00:00:26.235
<v Speaker 0>ChatGPT

00:00:26.235 --> 00:00:28.154
<v Speaker 0>for a sonnet about Kubernetes.

00:00:28.800 --> 00:00:30.880
<v Speaker 0>David, why don't you start us out? What

00:00:30.880 --> 00:00:32.320
<v Speaker 0>the chat GPT say?

00:00:33.600 --> 00:00:37.280
<v Speaker 1>In digital realms where servers and sway, a

00:00:37.280 --> 00:00:39.600
<v Speaker 1>symphony of nodes and rhythmic dance,

00:00:39.840 --> 00:00:43.375
<v Speaker 1>there lies a master orchestrate and play Kubernetes,

00:00:43.375 --> 00:00:44.894
<v Speaker 1>the captain of advance.

00:00:45.295 --> 00:00:46.414
<v Speaker 0>Its containers,

00:00:46.495 --> 00:00:49.215
<v Speaker 0>like actors on a stage, hold microcosms of

00:00:49.215 --> 00:00:52.254
<v Speaker 0>code's vibrant art. With Docker's grace, they come

00:00:52.254 --> 00:00:53.695
<v Speaker 0>alive, engage.

00:00:53.934 --> 00:00:56.015
<v Speaker 0>A ballet where technology takes part.

00:00:56.910 --> 00:01:00.030
<v Speaker 1>Scaling with grace like stars and boundless skies,

00:01:00.270 --> 00:01:03.230
<v Speaker 1>replicas bloom whether at its will.

00:01:04.030 --> 00:01:06.510
<v Speaker 1>Resilient adheres the failure's cry,

00:01:06.910 --> 00:01:09.150
<v Speaker 1>yet presses on each challenge to fulfill.

00:01:09.784 --> 00:01:10.664
<v Speaker 0>Oh, Kubernetes,

00:01:11.064 --> 00:01:13.865
<v Speaker 0>conductor of the cloud, you orchestrate our dreams

00:01:13.865 --> 00:01:15.225
<v Speaker 0>forever proud.

00:01:16.344 --> 00:01:18.024
<v Speaker 0>I can't believe I can shred that.

00:01:21.064 --> 00:01:21.704
<v Speaker 0>So,

00:01:22.024 --> 00:01:24.780
<v Speaker 0>we have a guest on. So JJ, can

00:01:24.780 --> 00:01:26.620
<v Speaker 0>you tell us who you are and what

00:01:26.620 --> 00:01:27.900
<v Speaker 0>you do and

00:01:28.300 --> 00:01:29.020
<v Speaker 0>how

00:01:29.900 --> 00:01:32.220
<v Speaker 0>it all works, the whole thing, all of

00:01:32.220 --> 00:01:33.180
<v Speaker 0>it? Yeah.

00:01:34.380 --> 00:01:36.540
<v Speaker 2>Hi. JJ Asgar.

00:01:36.620 --> 00:01:39.260
<v Speaker 2>I'm a developer advocate for IBM now,

00:01:40.615 --> 00:01:42.215
<v Speaker 2>And that means a lot. I wear a

00:01:42.215 --> 00:01:43.895
<v Speaker 2>lot of hats. That's what it boils down

00:01:43.895 --> 00:01:44.375
<v Speaker 2>to.

00:01:45.175 --> 00:01:47.815
<v Speaker 2>I kind of engage in different organizations and

00:01:47.815 --> 00:01:50.855
<v Speaker 2>different spaces and try to represent developers.

00:01:51.975 --> 00:01:53.495
<v Speaker 2>And they're they're kind of

00:01:54.070 --> 00:01:57.189
<v Speaker 2>their persona, I think, is the project management

00:01:57.189 --> 00:01:59.270
<v Speaker 2>term. I'm trying to learn the vernacular. What

00:01:59.270 --> 00:01:59.990
<v Speaker 2>can I say?

00:02:01.990 --> 00:02:02.630
<v Speaker 2>And

00:02:03.110 --> 00:02:06.710
<v Speaker 2>right now, I'm focusing on a product called

00:02:06.790 --> 00:02:09.375
<v Speaker 2>Watson X, which is an AI platform

00:02:09.775 --> 00:02:12.255
<v Speaker 2>that's built off of open source. And I

00:02:12.255 --> 00:02:14.175
<v Speaker 2>feel like your audience would be interested in

00:02:14.175 --> 00:02:15.855
<v Speaker 2>hearing a little bit about that space.

00:02:16.415 --> 00:02:19.535
<v Speaker 0>Yeah. Yeah. Kind of an interesting topic to

00:02:19.535 --> 00:02:21.709
<v Speaker 0>really bring up. Now I know that there's

00:02:21.709 --> 00:02:25.950
<v Speaker 0>the Linux Foundation's AI and Data Foundation,

00:02:26.269 --> 00:02:28.349
<v Speaker 0>and then there's obviously the CNCF, which is

00:02:28.349 --> 00:02:29.469
<v Speaker 0>another suborganization

00:02:29.469 --> 00:02:30.670
<v Speaker 0>for the Linux Foundation.

00:02:31.069 --> 00:02:31.630
<v Speaker 0>So

00:02:31.870 --> 00:02:34.430
<v Speaker 0>is Watson AI part of, like, the AID

00:02:34.430 --> 00:02:35.950
<v Speaker 0>one? Is it the cloud native one?

00:02:36.885 --> 00:02:38.725
<v Speaker 2>Because I wanna get, like, dig all the

00:02:38.725 --> 00:02:40.885
<v Speaker 0>way down into this landscape. Let's let's go

00:02:40.885 --> 00:02:43.365
<v Speaker 0>for it. So Yeah. Well, first of all,

00:02:43.365 --> 00:02:44.965
<v Speaker 2>I gotta be because I I am an

00:02:44.965 --> 00:02:46.805
<v Speaker 2>IBMer, and I have to have I have

00:02:46.805 --> 00:02:47.925
<v Speaker 2>to get the branding Yeah.

00:02:49.605 --> 00:02:52.610
<v Speaker 2>It's watsonx.ai

00:02:52.690 --> 00:02:56.210
<v Speaker 2>is the official name of the AI platform

00:02:56.210 --> 00:02:58.370
<v Speaker 2>for watsonx.ai.

00:02:58.850 --> 00:03:01.090
<v Speaker 2>We also have a couple other products coming

00:03:01.090 --> 00:03:03.730
<v Speaker 2>down that do other portions that are required

00:03:03.730 --> 00:03:05.890
<v Speaker 2>for a real AI platform to work.

00:03:06.865 --> 00:03:08.785
<v Speaker 2>I don't think I'm allowed to publicly say

00:03:08.785 --> 00:03:11.185
<v Speaker 2>those words quite yet. I don't actually know.

00:03:11.425 --> 00:03:13.745
<v Speaker 2>But the one that your community

00:03:14.065 --> 00:03:16.225
<v Speaker 2>in general would be focusing on most likely

00:03:16.225 --> 00:03:18.145
<v Speaker 2>is watsonx.ai.

00:03:18.600 --> 00:03:20.280
<v Speaker 2>Now that one

00:03:21.320 --> 00:03:23.480
<v Speaker 2>so when you kinda tie that up to

00:03:24.360 --> 00:03:27.320
<v Speaker 2>the foundations and the organizations that our industry

00:03:27.320 --> 00:03:28.440
<v Speaker 2>uses, like

00:03:29.000 --> 00:03:30.920
<v Speaker 2>LF AI and things like that,

00:03:31.585 --> 00:03:33.985
<v Speaker 2>What we have done as IBM is

00:03:34.465 --> 00:03:37.505
<v Speaker 2>kinda stepped into the LF AI to help

00:03:37.665 --> 00:03:39.425
<v Speaker 2>build the ecosystem

00:03:39.585 --> 00:03:40.385
<v Speaker 2>for

00:03:40.545 --> 00:03:42.465
<v Speaker 2>enterprise grade level

00:03:42.785 --> 00:03:45.745
<v Speaker 2>AI an AI platform that allows enterprise to

00:03:45.745 --> 00:03:48.120
<v Speaker 2>be comfortable with using AI.

00:03:48.840 --> 00:03:50.440
<v Speaker 2>I'm gonna just go into a quick little

00:03:50.440 --> 00:03:53.319
<v Speaker 2>tirade about why because you're probably like, JJ,

00:03:53.319 --> 00:03:54.600
<v Speaker 2>why wouldn't any

00:03:54.840 --> 00:03:57.959
<v Speaker 2>enterprise be happy with this? Right? Like Yeah.

00:03:57.959 --> 00:04:00.040
<v Speaker 0>Tell tell us a bit about why enterprises

00:04:00.040 --> 00:04:01.239
<v Speaker 0>maybe don't like AI.

00:04:01.775 --> 00:04:03.535
<v Speaker 0>That that's an interesting question.

00:04:04.175 --> 00:04:07.135
<v Speaker 2>Yeah. Yeah. So so there's this whole world

00:04:07.135 --> 00:04:09.215
<v Speaker 2>right now of people being like, AI is

00:04:09.215 --> 00:04:11.055
<v Speaker 2>gonna take our jobs. Right?

00:04:11.695 --> 00:04:14.015
<v Speaker 0>You know what it is. Right? Everyone

00:04:14.015 --> 00:04:15.935
<v Speaker 2>knows this. Well, we can make fun of

00:04:15.935 --> 00:04:17.695
<v Speaker 2>it, but the the truth of it is

00:04:19.270 --> 00:04:21.830
<v Speaker 2>as much as people think ChatGPT

00:04:22.310 --> 00:04:23.030
<v Speaker 2>or

00:04:24.230 --> 00:04:26.950
<v Speaker 2>all the other major disruptors out there have

00:04:26.950 --> 00:04:29.430
<v Speaker 2>come into the ecosystem and started giving you

00:04:29.430 --> 00:04:30.790
<v Speaker 2>sonnets about Kubernetes,

00:04:31.455 --> 00:04:33.215
<v Speaker 2>which is actually surprisingly good.

00:04:34.335 --> 00:04:36.415
<v Speaker 2>Yeah. Seriously. Well, ask him for a sonnet

00:04:36.415 --> 00:04:38.415
<v Speaker 0>about Kubernetes. Later. Okay. Yeah. No.

00:04:38.895 --> 00:04:39.615
<v Speaker 0>Anyway.

00:04:39.775 --> 00:04:40.335
<v Speaker 2>And

00:04:40.975 --> 00:04:43.455
<v Speaker 2>as much as people think that's that's entertaining

00:04:43.455 --> 00:04:44.335
<v Speaker 2>and interesting,

00:04:44.670 --> 00:04:47.390
<v Speaker 2>when you start looking at actual how our

00:04:47.390 --> 00:04:48.510
<v Speaker 2>business is done,

00:04:48.830 --> 00:04:50.750
<v Speaker 2>there isn't a very good

00:04:51.310 --> 00:04:53.470
<v Speaker 2>safe environment for this information.

00:04:54.830 --> 00:04:58.030
<v Speaker 2>So everything the ChatGPT gets, it learns off

00:04:58.030 --> 00:05:00.165
<v Speaker 2>of. It it it grabs. So if you

00:05:00.165 --> 00:05:02.245
<v Speaker 2>look at the Samsung, for instance, issue with

00:05:02.245 --> 00:05:03.525
<v Speaker 2>people giving out the

00:05:04.005 --> 00:05:06.165
<v Speaker 2>the proprietary information, and then all of a

00:05:06.165 --> 00:05:09.045
<v Speaker 2>sudden, someone else gets that proprietary information

00:05:09.125 --> 00:05:10.805
<v Speaker 2>about the Samsung thing. Hopefully,

00:05:11.365 --> 00:05:12.885
<v Speaker 2>we can figure find Yeah. The link to

00:05:12.885 --> 00:05:13.044
<v Speaker 2>that.

00:05:13.830 --> 00:05:14.550
<v Speaker 2>That

00:05:15.030 --> 00:05:17.430
<v Speaker 2>is just a microcosm of the problems when

00:05:17.430 --> 00:05:20.870
<v Speaker 2>it comes to business. Because right now, ChatGPT

00:05:20.870 --> 00:05:23.110
<v Speaker 2>is a complete and utter black box.

00:05:23.590 --> 00:05:26.310
<v Speaker 2>There is no way that the owners of

00:05:26.310 --> 00:05:27.190
<v Speaker 2>ChatGPT

00:05:27.190 --> 00:05:28.230
<v Speaker 2>or OpenAI

00:05:28.985 --> 00:05:32.264
<v Speaker 2>or Microsoft who has invested billions into it

00:05:32.264 --> 00:05:34.665
<v Speaker 2>will ever give us the data

00:05:34.905 --> 00:05:36.025
<v Speaker 2>that ChatGPT

00:05:36.025 --> 00:05:37.385
<v Speaker 2>has been built off of.

00:05:37.705 --> 00:05:39.384
<v Speaker 2>And if you start going down that train

00:05:39.384 --> 00:05:41.465
<v Speaker 2>of thought, all of a sudden, you recognize

00:05:42.180 --> 00:05:44.500
<v Speaker 2>you're giving all your proprietary info you can't

00:05:44.500 --> 00:05:46.580
<v Speaker 2>give your proprietary information. Whether it be a

00:05:46.580 --> 00:05:49.140
<v Speaker 2>PDF of how your HR system works to

00:05:49.140 --> 00:05:51.780
<v Speaker 2>the schematics of a f 15,

00:05:52.100 --> 00:05:53.620
<v Speaker 2>you can't give it to you can't use

00:05:53.620 --> 00:05:55.380
<v Speaker 2>ChatGPT with it because you don't know who's

00:05:55.380 --> 00:05:57.475
<v Speaker 2>gonna get it on the other side. This

00:05:58.435 --> 00:06:00.435
<v Speaker 2>is where Watson X comes into play.

00:06:01.155 --> 00:06:04.194
<v Speaker 2>A really great analogy for Watson X verse

00:06:04.194 --> 00:06:06.275
<v Speaker 2>the industry right now is

00:06:06.675 --> 00:06:08.435
<v Speaker 2>ChatGPT is Napster.

00:06:08.835 --> 00:06:10.115
<v Speaker 2>Watson X is iTunes.

00:06:11.039 --> 00:06:12.319
<v Speaker 2>If you think of it and you put

00:06:12.319 --> 00:06:14.159
<v Speaker 2>it in that paradigm, all of a sudden,

00:06:14.159 --> 00:06:15.680
<v Speaker 2>things start making a lot more sense on

00:06:15.680 --> 00:06:16.960
<v Speaker 2>the enterprise level.

00:06:17.360 --> 00:06:19.360
<v Speaker 2>Because now you can go to banks and

00:06:19.360 --> 00:06:21.199
<v Speaker 2>say, hey. We can give you a foundational

00:06:21.199 --> 00:06:22.800
<v Speaker 2>model that we can give you all the

00:06:22.800 --> 00:06:25.695
<v Speaker 2>data that was built off of this. Obviously,

00:06:25.695 --> 00:06:26.495
<v Speaker 2>with some money,

00:06:26.895 --> 00:06:29.615
<v Speaker 2>business y stuff happening. Right? We're not gonna

00:06:29.615 --> 00:06:31.135
<v Speaker 2>just, like, give it to you, of course,

00:06:31.135 --> 00:06:32.095
<v Speaker 2>build

00:06:32.095 --> 00:06:34.895
<v Speaker 2>the relationship, etcetera. So but we can give

00:06:34.895 --> 00:06:36.574
<v Speaker 2>you that give you the training on top

00:06:36.574 --> 00:06:37.935
<v Speaker 2>of it, and then you can put your

00:06:37.935 --> 00:06:40.610
<v Speaker 2>proprietary stuff on top of that so you

00:06:40.610 --> 00:06:43.090
<v Speaker 2>can have the chatbot that gives you that

00:06:43.090 --> 00:06:45.169
<v Speaker 2>turns out that Aetna is your insurance and

00:06:45.169 --> 00:06:46.530
<v Speaker 2>give you all the information you need about

00:06:46.530 --> 00:06:47.889
<v Speaker 2>Aetna or or whatever.

00:06:48.129 --> 00:06:50.850
<v Speaker 2>Right? Which is, believe it or not,

00:06:51.810 --> 00:06:53.810
<v Speaker 2>something that's insanely powerful.

00:06:54.210 --> 00:06:56.705
<v Speaker 2>Right? So, anyway, that that that when when

00:06:56.705 --> 00:06:58.544
<v Speaker 2>you start going down that track, you start

00:06:58.544 --> 00:07:00.145
<v Speaker 2>seeing more things in that space.

00:07:01.185 --> 00:07:01.905
<v Speaker 0>Good.

00:07:03.905 --> 00:07:05.425
<v Speaker 0>Ego, David. I mean, I get I get

00:07:05.425 --> 00:07:07.185
<v Speaker 0>asked more questions about, like, open source sitting

00:07:07.185 --> 00:07:08.064
<v Speaker 0>high, but you go next.

00:07:08.840 --> 00:07:10.200
<v Speaker 1>Yeah. I just wanna understand

00:07:10.520 --> 00:07:13.160
<v Speaker 1>make sure I understand the the proposition correctly.

00:07:13.160 --> 00:07:15.639
<v Speaker 1>Right? So you've done this this

00:07:15.639 --> 00:07:18.520
<v Speaker 1>comparison. Right? You're saying Napster versus iTunes.

00:07:20.040 --> 00:07:23.320
<v Speaker 1>People use, say, ChatGPT and OpenAI to go

00:07:23.320 --> 00:07:24.200
<v Speaker 1>and ask

00:07:24.655 --> 00:07:27.695
<v Speaker 1>any question because it's a model that is

00:07:27.695 --> 00:07:30.575
<v Speaker 1>trained on multiple billion of parameters. Right? And

00:07:30.575 --> 00:07:31.855
<v Speaker 1>all that information, like you said, is a

00:07:31.855 --> 00:07:34.095
<v Speaker 1>black box. We don't understand it. It doesn't

00:07:34.095 --> 00:07:35.535
<v Speaker 1>really allow you to,

00:07:36.015 --> 00:07:37.855
<v Speaker 1>I don't know, dig into sort of niche

00:07:37.855 --> 00:07:39.375
<v Speaker 1>subjects with high

00:07:40.030 --> 00:07:42.670
<v Speaker 1>cardinality. Often. Yeah. Yeah. Exactly.

00:07:43.470 --> 00:07:44.590
<v Speaker 1>Watson x,

00:07:44.910 --> 00:07:47.630
<v Speaker 1>I get to decide what that data is

00:07:47.630 --> 00:07:50.430
<v Speaker 1>and I get to then query it through

00:07:50.430 --> 00:07:52.750
<v Speaker 1>a similar style interface. Is that the is

00:07:52.750 --> 00:07:54.430
<v Speaker 1>that what Watson x would offer me as

00:07:54.430 --> 00:07:56.555
<v Speaker 1>a developer, like a model where I say,

00:07:56.555 --> 00:07:58.475
<v Speaker 1>here's all of my data. Have have my

00:07:58.475 --> 00:08:00.475
<v Speaker 1>life. Right? And then I can ask it

00:08:00.475 --> 00:08:01.275
<v Speaker 1>questions

00:08:01.435 --> 00:08:02.875
<v Speaker 1>and it's gonna give me answers.

00:08:03.354 --> 00:08:05.195
<v Speaker 1>But that if that's true, which is cool.

00:08:05.195 --> 00:08:07.035
<v Speaker 1>Right? It does I have this little question

00:08:07.035 --> 00:08:07.835
<v Speaker 1>where I'm like, okay.

00:08:09.490 --> 00:08:11.409
<v Speaker 1>Is there a bias where it's only gonna

00:08:11.409 --> 00:08:13.169
<v Speaker 1>confirm the stuff that I've given it in

00:08:13.169 --> 00:08:14.050
<v Speaker 1>my data?

00:08:14.210 --> 00:08:15.970
<v Speaker 1>Like, is it gonna be able to be

00:08:15.970 --> 00:08:17.650
<v Speaker 1>is it gonna be able to be slightly

00:08:17.650 --> 00:08:20.770
<v Speaker 1>more objective? Can you feed outside properties to

00:08:20.770 --> 00:08:23.169
<v Speaker 1>augment and enrich? Like, how does that all

00:08:23.169 --> 00:08:23.409
<v Speaker 1>work?

00:08:24.294 --> 00:08:24.694
<v Speaker 2>That's

00:08:25.095 --> 00:08:26.935
<v Speaker 2>that is a very great question.

00:08:27.815 --> 00:08:30.135
<v Speaker 2>And that's a natural progression. Right?

00:08:30.375 --> 00:08:32.934
<v Speaker 2>So let's go a little bit deeper than

00:08:32.934 --> 00:08:35.895
<v Speaker 2>the CIO, CTO level, and let's go down

00:08:35.895 --> 00:08:38.455
<v Speaker 2>to the senior executive and start thinking about

00:08:38.455 --> 00:08:41.490
<v Speaker 2>this. Yeah. Yeah. Right. So you like you

00:08:41.490 --> 00:08:42.690
<v Speaker 2>like that? That was a good was that

00:08:42.690 --> 00:08:44.049
<v Speaker 2>was good. So

00:08:45.569 --> 00:08:47.730
<v Speaker 2>so, yes, it's a very valid question.

00:08:48.209 --> 00:08:50.370
<v Speaker 2>What what is AI? A good friend of

00:08:50.370 --> 00:08:52.690
<v Speaker 2>mine, Carl, actually mentioned this the other day.

00:08:53.285 --> 00:08:55.445
<v Speaker 2>What is AI? It's just a yes man.

00:08:55.685 --> 00:08:57.205
<v Speaker 2>Right? When you actually look at it, what

00:08:57.205 --> 00:09:00.005
<v Speaker 2>is AI? It's it takes percentages of the

00:09:00.005 --> 00:09:02.805
<v Speaker 2>possible questions you're asking it, finds the highest

00:09:02.805 --> 00:09:04.405
<v Speaker 2>percentage of what you were looking for, and

00:09:04.405 --> 00:09:06.165
<v Speaker 2>gives you that answer. That is what AI

00:09:06.165 --> 00:09:07.605
<v Speaker 2>is in a nutshell. It's a yes man.

00:09:07.605 --> 00:09:09.230
<v Speaker 2>It's a crony. Right?

00:09:09.470 --> 00:09:10.110
<v Speaker 2>So

00:09:10.350 --> 00:09:12.270
<v Speaker 2>what we need to do is give accuracy

00:09:12.270 --> 00:09:14.190
<v Speaker 2>to the crony to get the answers you

00:09:14.190 --> 00:09:15.870
<v Speaker 2>were looking for very specifically.

00:09:16.750 --> 00:09:19.950
<v Speaker 2>Now we have something called foundational models,

00:09:20.190 --> 00:09:22.110
<v Speaker 2>which is I think there's four of them.

00:09:22.110 --> 00:09:23.745
<v Speaker 2>I don't know exactly what I'm supposed to

00:09:23.745 --> 00:09:25.584
<v Speaker 2>say publicly right now, so I'm just gonna

00:09:25.584 --> 00:09:27.345
<v Speaker 2>say there's four foundational models.

00:09:27.985 --> 00:09:30.065
<v Speaker 2>And they're all built off of

00:09:31.504 --> 00:09:33.824
<v Speaker 2>data that we have got we have agreed

00:09:33.824 --> 00:09:34.464
<v Speaker 2>upon

00:09:34.625 --> 00:09:35.665
<v Speaker 2>that are

00:09:36.225 --> 00:09:39.360
<v Speaker 2>safe to use. Now let's pivot quickly and

00:09:39.360 --> 00:09:41.839
<v Speaker 2>talk about models in the open source ecosystem.

00:09:42.720 --> 00:09:44.880
<v Speaker 2>Models in the open source ecosystem, something called

00:09:44.880 --> 00:09:47.600
<v Speaker 2>Hugging Face. If your audience doesn't know about

00:09:47.600 --> 00:09:50.000
<v Speaker 2>it, the easiest thing to describe Hugging Face

00:09:50.000 --> 00:09:52.480
<v Speaker 2>as is the GitHub of AI.

00:09:54.015 --> 00:09:56.175
<v Speaker 2>It allows places to put models up there

00:09:56.175 --> 00:09:58.575
<v Speaker 2>that you can build off of previous models.

00:09:58.575 --> 00:10:00.175
<v Speaker 2>They also give some really nice

00:10:00.735 --> 00:10:02.335
<v Speaker 2>I think the term is shims on

00:10:02.895 --> 00:10:05.055
<v Speaker 2>top of AI

00:10:05.055 --> 00:10:07.089
<v Speaker 2>development work. So you don't have to do

00:10:07.089 --> 00:10:09.089
<v Speaker 2>all the stuff around it. You can write

00:10:09.089 --> 00:10:10.610
<v Speaker 2>some very simple Python to be able to

00:10:10.610 --> 00:10:14.050
<v Speaker 2>leverage stuff from what from from Hugging Face.

00:10:14.529 --> 00:10:15.810
<v Speaker 2>Now 95

00:10:15.970 --> 00:10:17.329
<v Speaker 2>this is not hyperbole.

00:10:17.490 --> 00:10:18.449
<v Speaker 2>95%

00:10:18.449 --> 00:10:20.714
<v Speaker 2>of those open source models are built off

00:10:20.714 --> 00:10:22.235
<v Speaker 2>of something called literally

00:10:22.475 --> 00:10:23.515
<v Speaker 2>the pile.

00:10:24.154 --> 00:10:27.275
<v Speaker 2>If you Google the pile AI,

00:10:27.435 --> 00:10:30.235
<v Speaker 2>it is a massive dataset out there that

00:10:30.235 --> 00:10:33.115
<v Speaker 2>is just, like, I think, 800 gigs last

00:10:33.115 --> 00:10:35.355
<v Speaker 2>time I looked at it that people Prep.

00:10:36.149 --> 00:10:39.110
<v Speaker 2>Yeah. That is all just text and information

00:10:39.110 --> 00:10:41.430
<v Speaker 2>of on the web that people just shove

00:10:41.430 --> 00:10:42.149
<v Speaker 2>together.

00:10:42.709 --> 00:10:43.750
<v Speaker 2>So LLMs

00:10:43.750 --> 00:10:44.230
<v Speaker 2>or

00:10:45.190 --> 00:10:46.870
<v Speaker 2>linguist

00:10:47.110 --> 00:10:48.550
<v Speaker 2>language models. Language

00:10:48.870 --> 00:10:49.670
<v Speaker 0>learning models.

00:10:50.195 --> 00:10:52.435
<v Speaker 0>Larg language models. Yeah.

00:10:52.995 --> 00:10:55.635
<v Speaker 1>Largest language model. Yeah. Largest Largest

00:10:55.635 --> 00:10:57.795
<v Speaker 2>language models. None of us actually know.

00:10:57.955 --> 00:11:00.274
<v Speaker 0>None of us know what this means. Anyway.

00:11:00.274 --> 00:11:01.315
<v Speaker 2>Exactly. So

00:11:02.115 --> 00:11:04.755
<v Speaker 2>the majority the 95% of these LLMs out

00:11:04.755 --> 00:11:04.995
<v Speaker 2>there

00:11:05.560 --> 00:11:09.240
<v Speaker 2>are actually trained on the pile. And trust

00:11:09.240 --> 00:11:10.839
<v Speaker 2>me, Google it. It comes up in a

00:11:10.839 --> 00:11:13.000
<v Speaker 2>a Wikipedia article. It's this whole big dataset

00:11:13.000 --> 00:11:14.839
<v Speaker 2>problem that they have. Well, you've you've said

00:11:14.839 --> 00:11:17.399
<v Speaker 1>something right there. Right? Like, we have this

00:11:17.885 --> 00:11:22.045
<v Speaker 1>open source governed centralized knowledge of Wikipedia.

00:11:22.045 --> 00:11:24.045
<v Speaker 1>Like, why is that not the base for

00:11:24.045 --> 00:11:24.605
<v Speaker 1>these

00:11:25.325 --> 00:11:27.965
<v Speaker 1>models? So so I I I great great

00:11:27.965 --> 00:11:29.405
<v Speaker 2>great question. Before we get get on that

00:11:29.405 --> 00:11:31.680
<v Speaker 2>that that branch, let's let let me finish

00:11:31.680 --> 00:11:32.720
<v Speaker 2>what I was trying to talk about Yes,

00:11:32.720 --> 00:11:35.279
<v Speaker 1>sir. The the pile problem, which is

00:11:36.399 --> 00:11:38.480
<v Speaker 2>the pile isn't clean.

00:11:39.120 --> 00:11:41.920
<v Speaker 2>There is a lot of proprietary,

00:11:43.199 --> 00:11:43.760
<v Speaker 2>pirated

00:11:44.535 --> 00:11:45.335
<v Speaker 2>issues

00:11:45.335 --> 00:11:46.775
<v Speaker 2>inside of

00:11:46.855 --> 00:11:48.615
<v Speaker 2>of inside of

00:11:48.855 --> 00:11:49.495
<v Speaker 2>the pile

00:11:49.975 --> 00:11:51.815
<v Speaker 2>data inside of that. There are books that

00:11:51.815 --> 00:11:53.655
<v Speaker 2>are completely ripped off.

00:11:53.895 --> 00:11:56.695
<v Speaker 2>So when you start looking into copyright law,

00:11:57.180 --> 00:11:59.100
<v Speaker 2>right, and all of a sudden finding out

00:11:59.100 --> 00:12:01.500
<v Speaker 2>people have written and their books are in

00:12:01.500 --> 00:12:04.460
<v Speaker 2>the pile and the LLM is trained on

00:12:04.460 --> 00:12:06.140
<v Speaker 2>their book without permission

00:12:06.220 --> 00:12:08.460
<v Speaker 2>and they create something off of that,

00:12:08.780 --> 00:12:11.500
<v Speaker 2>that brings into some really interesting conversations about

00:12:11.500 --> 00:12:13.295
<v Speaker 2>how our copyright law works

00:12:13.615 --> 00:12:16.015
<v Speaker 2>and then how with that too

00:12:16.255 --> 00:12:18.095
<v Speaker 2>patent law works

00:12:18.175 --> 00:12:20.415
<v Speaker 2>and then with that licensing.

00:12:20.575 --> 00:12:21.935
<v Speaker 2>And all of a sudden, it it the

00:12:21.935 --> 00:12:22.735
<v Speaker 2>the the

00:12:23.695 --> 00:12:24.575
<v Speaker 2>so what I'm looking for?

00:12:25.589 --> 00:12:27.350
<v Speaker 0>The Pyramid scheme?

00:12:27.910 --> 00:12:30.069
<v Speaker 2>Oh, not only pyramid scheme, but the the

00:12:30.069 --> 00:12:32.950
<v Speaker 2>house of cards that is our industry,

00:12:33.350 --> 00:12:36.070
<v Speaker 2>all of a sudden starts starts falling apart.

00:12:36.389 --> 00:12:36.630
<v Speaker 2>Because,

00:12:37.535 --> 00:12:40.495
<v Speaker 2>frankly speaking, if we if we allow, which

00:12:40.495 --> 00:12:42.815
<v Speaker 2>right now, at this exact moment in time

00:12:42.815 --> 00:12:43.855
<v Speaker 2>when we're recording,

00:12:44.495 --> 00:12:46.255
<v Speaker 2>a federal judge in at least the US

00:12:46.255 --> 00:12:47.775
<v Speaker 2>government has said

00:12:48.255 --> 00:12:49.135
<v Speaker 2>anything

00:12:49.135 --> 00:12:51.899
<v Speaker 2>from a AI that is created cannot be

00:12:51.899 --> 00:12:52.779
<v Speaker 2>copyrighted.

00:12:53.100 --> 00:12:53.740
<v Speaker 2>Right?

00:12:54.220 --> 00:12:55.980
<v Speaker 2>That is that is this exact moment in

00:12:55.980 --> 00:12:57.740
<v Speaker 2>time. I think that was literally yesterday. Yeah.

00:12:57.740 --> 00:12:58.779
<v Speaker 2>It was. Yeah.

00:12:59.100 --> 00:13:01.339
<v Speaker 2>But, again, that's only here in The US.

00:13:01.500 --> 00:13:01.820
<v Speaker 2>Right?

00:13:03.625 --> 00:13:06.585
<v Speaker 2>And being a US centric company at IBM,

00:13:06.585 --> 00:13:08.665
<v Speaker 2>we obviously are paying very close attention to

00:13:08.665 --> 00:13:09.225
<v Speaker 2>this.

00:13:10.265 --> 00:13:12.665
<v Speaker 2>This all brings up a whole conversation of,

00:13:13.305 --> 00:13:13.865
<v Speaker 2>like,

00:13:14.345 --> 00:13:16.025
<v Speaker 2>how do we keep going down this path

00:13:16.025 --> 00:13:17.625
<v Speaker 2>without without

00:13:17.980 --> 00:13:20.060
<v Speaker 2>possibly destroying our industry?

00:13:20.620 --> 00:13:23.340
<v Speaker 2>Right? People joke about how we're barreling forward

00:13:23.340 --> 00:13:24.940
<v Speaker 2>to to the end times

00:13:25.420 --> 00:13:28.940
<v Speaker 2>leveraging leveraging AI and all that jazz. But

00:13:28.940 --> 00:13:31.820
<v Speaker 2>when you actually look at how we protect

00:13:31.820 --> 00:13:32.780
<v Speaker 2>and can use

00:13:33.355 --> 00:13:36.635
<v Speaker 2>the entities that is governments to enforce,

00:13:36.875 --> 00:13:38.795
<v Speaker 2>yes, I have not stolen your idea, and

00:13:38.795 --> 00:13:40.395
<v Speaker 2>it is my right to sell this to

00:13:40.395 --> 00:13:41.675
<v Speaker 2>people for

00:13:41.915 --> 00:13:43.195
<v Speaker 2>goods and services.

00:13:43.515 --> 00:13:45.435
<v Speaker 2>If all of a sudden AI can create

00:13:45.435 --> 00:13:47.355
<v Speaker 2>all that stuff and they create something very

00:13:47.355 --> 00:13:49.640
<v Speaker 2>close to that and they can start selling

00:13:49.640 --> 00:13:51.720
<v Speaker 2>that as their proprietary thing,

00:13:52.040 --> 00:13:54.839
<v Speaker 2>all of a sudden, businesses don't don't function.

00:13:55.000 --> 00:13:57.399
<v Speaker 2>Right? As soon as we get a real

00:13:57.720 --> 00:14:00.120
<v Speaker 2>AI that creates a the next Taylor Swift

00:14:00.120 --> 00:14:00.440
<v Speaker 2>song,

00:14:01.605 --> 00:14:02.485
<v Speaker 2>I mean,

00:14:02.805 --> 00:14:06.005
<v Speaker 2>if if we call it Taylor Swift bot,

00:14:06.005 --> 00:14:07.445
<v Speaker 2>I don't I don't know. What right? Like,

00:14:07.445 --> 00:14:09.045
<v Speaker 2>you get what I'm saying. You you see

00:14:09.045 --> 00:14:10.725
<v Speaker 2>all of a sudden the the onion gets

00:14:10.725 --> 00:14:12.965
<v Speaker 2>scarier and scarier and scarier.

00:14:13.125 --> 00:14:13.685
<v Speaker 2>So

00:14:14.085 --> 00:14:16.165
<v Speaker 2>we we have to build this wall

00:14:16.580 --> 00:14:18.820
<v Speaker 2>to make sure that the world we we

00:14:18.820 --> 00:14:20.260
<v Speaker 2>we know right now,

00:14:21.220 --> 00:14:23.380
<v Speaker 2>which is going to change because we're gonna

00:14:23.380 --> 00:14:25.380
<v Speaker 2>try to create avenues to be successful.

00:14:27.060 --> 00:14:27.860
<v Speaker 2>But if we're not,

00:14:30.275 --> 00:14:33.475
<v Speaker 2>it's scary, people. It's scary. Yeah.

00:14:34.355 --> 00:14:35.955
<v Speaker 0>So why not Wikipedia?

00:14:36.515 --> 00:14:38.995
<v Speaker 2>Ah, yes. Great question. Yeah. I'm I'm I'm

00:14:38.995 --> 00:14:40.035
<v Speaker 0>gonna bring it back to a little bit

00:14:40.035 --> 00:14:41.635
<v Speaker 0>of levity because otherwise, we're just gonna go

00:14:41.635 --> 00:14:44.720
<v Speaker 0>screaming down the black hole. But We've already

00:14:44.720 --> 00:14:46.480
<v Speaker 2>passed the VP level. We've already passed the

00:14:46.480 --> 00:14:49.200
<v Speaker 2>director. We're already, like, into the thought leadership,

00:14:49.200 --> 00:14:52.080
<v Speaker 2>like Alright. Engineering people. Right? Thought leaders.

00:14:52.560 --> 00:14:54.160
<v Speaker 2>Yeah. There you go.

00:14:55.520 --> 00:14:56.720
<v Speaker 2>I do wanna say,

00:14:57.200 --> 00:14:57.520
<v Speaker 2>well,

00:14:58.025 --> 00:14:59.865
<v Speaker 2>at least from my understanding

00:15:00.025 --> 00:15:01.465
<v Speaker 2>of how

00:15:02.105 --> 00:15:03.465
<v Speaker 2>Wikipedia works

00:15:03.785 --> 00:15:06.905
<v Speaker 2>and when if you parallelize it parallelize it,

00:15:06.905 --> 00:15:08.425
<v Speaker 2>sure, with

00:15:08.425 --> 00:15:09.385
<v Speaker 2>the pile,

00:15:10.025 --> 00:15:12.825
<v Speaker 2>the amount of errors inside of Wikipedia

00:15:13.830 --> 00:15:15.830
<v Speaker 2>and conflicting information

00:15:16.150 --> 00:15:17.270
<v Speaker 2>on top of

00:15:17.990 --> 00:15:19.830
<v Speaker 2>the ability to actually get the information out

00:15:19.830 --> 00:15:22.470
<v Speaker 2>of Wikipedia and train it usefully, there really

00:15:22.470 --> 00:15:24.150
<v Speaker 2>isn't actually as much

00:15:24.470 --> 00:15:26.710
<v Speaker 2>to get useful information. It goes back to

00:15:26.710 --> 00:15:28.635
<v Speaker 2>the my simple example of a bunch of

00:15:28.635 --> 00:15:31.515
<v Speaker 2>PDFs to look for suggested areas to find

00:15:31.515 --> 00:15:32.155
<v Speaker 2>stuff.

00:15:33.835 --> 00:15:36.155
<v Speaker 2>In my mind, that is the

00:15:36.555 --> 00:15:39.115
<v Speaker 2>easiest path for most people to grasp when

00:15:39.115 --> 00:15:39.835
<v Speaker 2>it comes to

00:15:40.730 --> 00:15:42.810
<v Speaker 2>understanding the power of AI.

00:15:44.010 --> 00:15:44.810
<v Speaker 2>Right now,

00:15:45.130 --> 00:15:48.010
<v Speaker 2>we've passed the whole if then statements of

00:15:48.010 --> 00:15:51.450
<v Speaker 2>of AI. We've passed the whole ecosystem

00:15:51.450 --> 00:15:52.650
<v Speaker 2>of

00:15:53.955 --> 00:15:54.755
<v Speaker 2>give me

00:15:55.475 --> 00:15:57.075
<v Speaker 2>avocado chairs

00:15:57.075 --> 00:15:58.995
<v Speaker 2>or whatever from Dolly.

00:15:59.635 --> 00:16:01.154
<v Speaker 2>Right now, we need to look at how

00:16:01.154 --> 00:16:02.515
<v Speaker 2>we can make our lives a little bit

00:16:02.515 --> 00:16:03.075
<v Speaker 2>better.

00:16:03.955 --> 00:16:07.730
<v Speaker 2>There are there are LLMs or not even

00:16:07.730 --> 00:16:10.370
<v Speaker 2>LLMs. I think they're considered classification. But I

00:16:10.450 --> 00:16:12.290
<v Speaker 2>point to me is that you can shove

00:16:12.290 --> 00:16:13.730
<v Speaker 2>a bunch of PDFs

00:16:13.730 --> 00:16:16.610
<v Speaker 2>into a model and then say, hey. I'm

00:16:16.610 --> 00:16:19.570
<v Speaker 2>looking for information about our

00:16:20.529 --> 00:16:21.889
<v Speaker 2>growth over the last two quarters.

00:16:22.615 --> 00:16:24.855
<v Speaker 2>And maybe that's only in a chart,

00:16:25.255 --> 00:16:27.335
<v Speaker 2>right, inside of the PDFs of of your

00:16:27.335 --> 00:16:28.375
<v Speaker 2>business logic.

00:16:28.615 --> 00:16:30.695
<v Speaker 2>There are elements that can read all those

00:16:30.695 --> 00:16:33.335
<v Speaker 2>charts and figure out, hey. Turns out growth

00:16:33.335 --> 00:16:35.255
<v Speaker 2>over the last quarter is is in this

00:16:35.255 --> 00:16:36.775
<v Speaker 2>graph and it's over the last four years.

00:16:37.529 --> 00:16:40.410
<v Speaker 2>This is the fourth spot right here. Okay.

00:16:40.410 --> 00:16:42.170
<v Speaker 2>It looks like it was 50%. It can

00:16:42.170 --> 00:16:43.690
<v Speaker 2>respond back. It looks like your growth was

00:16:43.690 --> 00:16:47.290
<v Speaker 2>over about 50% referenced in this diagram in

00:16:47.290 --> 00:16:48.329
<v Speaker 2>this PDF.

00:16:48.649 --> 00:16:50.490
<v Speaker 2>Right? So it becomes like a really, really

00:16:50.490 --> 00:16:51.449
<v Speaker 2>great librarian

00:16:51.845 --> 00:16:53.285
<v Speaker 2>if you think of it that way.

00:16:53.605 --> 00:16:55.445
<v Speaker 2>And then if you can tell that story

00:16:55.445 --> 00:16:56.485
<v Speaker 2>to an enterprise,

00:16:56.805 --> 00:16:59.765
<v Speaker 2>they have p every enterprise, right, has massive

00:16:59.765 --> 00:17:02.085
<v Speaker 2>amounts of PDFs of all their policies, procedures,

00:17:02.085 --> 00:17:03.205
<v Speaker 2>and everything like that.

00:17:03.685 --> 00:17:05.365
<v Speaker 2>If you can create a librarian,

00:17:06.440 --> 00:17:08.680
<v Speaker 2>think of whatever dystopian sci fi that has

00:17:08.680 --> 00:17:10.760
<v Speaker 2>a librarian in it, which is always always

00:17:10.760 --> 00:17:12.359
<v Speaker 2>a thing. Right?

00:17:12.599 --> 00:17:14.200
<v Speaker 2>If you can give that to an enterprise

00:17:14.200 --> 00:17:15.560
<v Speaker 2>and say, I can give you a way

00:17:15.560 --> 00:17:17.880
<v Speaker 2>to do that with your trusted data

00:17:18.325 --> 00:17:20.885
<v Speaker 2>that I know will not leave our borders

00:17:20.885 --> 00:17:23.285
<v Speaker 2>because that is core to our business.

00:17:23.925 --> 00:17:25.525
<v Speaker 2>Maybe you should look into that.

00:17:26.405 --> 00:17:28.485
<v Speaker 0>I mean, but that also assumes that the

00:17:28.485 --> 00:17:30.085
<v Speaker 0>librarian is not

00:17:30.820 --> 00:17:32.419
<v Speaker 0>making up data. I don't know if you

00:17:32.419 --> 00:17:33.459
<v Speaker 0>heard about the

00:17:33.700 --> 00:17:36.259
<v Speaker 0>the lawyer that asked for somebody to go

00:17:36.259 --> 00:17:38.419
<v Speaker 0>through all of the legal history

00:17:38.419 --> 00:17:40.419
<v Speaker 0>for some something and it turned out they

00:17:40.419 --> 00:17:42.899
<v Speaker 0>made up, like, LLM or the

00:17:43.345 --> 00:17:46.304
<v Speaker 0>the the AI made up a fake court

00:17:46.304 --> 00:17:48.065
<v Speaker 0>filing to prove something.

00:17:48.465 --> 00:17:50.784
<v Speaker 2>Yeah. And judge caught it, and it was

00:17:50.784 --> 00:17:51.664
<v Speaker 0>a big deal.

00:17:52.144 --> 00:17:54.544
<v Speaker 0>That's in The US again. But still, like,

00:17:54.544 --> 00:17:56.864
<v Speaker 0>I'm a little worried about that librarian

00:17:57.184 --> 00:17:58.465
<v Speaker 0>not quite

00:17:59.620 --> 00:18:00.659
<v Speaker 0>being correct.

00:18:01.220 --> 00:18:04.260
<v Speaker 1>But does this come down to a misunderstanding

00:18:04.260 --> 00:18:07.059
<v Speaker 1>of where people think AI is today? Like,

00:18:07.059 --> 00:18:08.580
<v Speaker 1>I think, you know, when I speak to

00:18:08.580 --> 00:18:10.420
<v Speaker 1>my wife, I speak to my family, I

00:18:10.420 --> 00:18:12.514
<v Speaker 1>speak to friends, and they're talking about chat

00:18:12.514 --> 00:18:14.434
<v Speaker 1>GPT. Right? I mean, they're all trying it.

00:18:14.434 --> 00:18:15.794
<v Speaker 1>They're all playing with it because it's getting

00:18:15.794 --> 00:18:16.754
<v Speaker 1>so much coverage.

00:18:17.154 --> 00:18:19.875
<v Speaker 1>They don't understand the generative part of it.

00:18:19.875 --> 00:18:22.114
<v Speaker 1>They think it's giving them knowledge.

00:18:22.355 --> 00:18:24.115
<v Speaker 1>They they don't know that it's just all

00:18:24.115 --> 00:18:25.770
<v Speaker 1>made up. Now we do because we are

00:18:25.770 --> 00:18:27.690
<v Speaker 1>in this industry. We follow the news. We

00:18:27.690 --> 00:18:30.330
<v Speaker 1>read these stories. But to most people, it's

00:18:30.330 --> 00:18:33.210
<v Speaker 1>a fact machine that's artificially intelligent and is

00:18:33.210 --> 00:18:35.610
<v Speaker 1>gonna give you the correct answer. So, I

00:18:35.610 --> 00:18:38.170
<v Speaker 1>mean, was the lawyer liable that they know

00:18:38.170 --> 00:18:39.690
<v Speaker 1>it was fake, that they did not verify

00:18:39.690 --> 00:18:41.845
<v Speaker 1>it? Sure. But what did they expect from

00:18:41.845 --> 00:18:42.725
<v Speaker 1>the AI? Like

00:18:44.164 --> 00:18:45.924
<v Speaker 2>And that's one reason why we have to

00:18:45.924 --> 00:18:48.245
<v Speaker 2>get AI to cite the stuff. Right? That's

00:18:48.245 --> 00:18:49.845
<v Speaker 2>not too far away to be able to,

00:18:49.845 --> 00:18:51.765
<v Speaker 2>like, where did you get this information from?

00:18:51.765 --> 00:18:54.164
<v Speaker 2>You can the the the natural progression is

00:18:54.164 --> 00:18:57.379
<v Speaker 2>citation and, you know, trust but verify.

00:18:57.780 --> 00:18:59.940
<v Speaker 2>But, again, that also requires on the model

00:18:59.940 --> 00:19:02.259
<v Speaker 2>that you've created and which model you've chosen,

00:19:02.500 --> 00:19:04.740
<v Speaker 2>which black box you've decided to do, where

00:19:04.740 --> 00:19:06.660
<v Speaker 2>the data has been trained like, where the

00:19:06.660 --> 00:19:09.175
<v Speaker 2>data is actually trained off of. Again, this

00:19:09.175 --> 00:19:11.015
<v Speaker 2>this all goes back to the other problem

00:19:11.015 --> 00:19:13.095
<v Speaker 2>that we don't talk about as an industry.

00:19:13.335 --> 00:19:14.775
<v Speaker 2>And as soon as you start playing more

00:19:14.775 --> 00:19:16.775
<v Speaker 2>in this space, recognize

00:19:16.935 --> 00:19:20.135
<v Speaker 2>that it's not the LLM or the model.

00:19:20.135 --> 00:19:21.975
<v Speaker 2>I I we I know we're intertwining change

00:19:22.135 --> 00:19:23.735
<v Speaker 2>interchanging LLM and model.

00:19:24.670 --> 00:19:26.510
<v Speaker 2>Wanna I wanna acknowledge, first of all, that

00:19:26.510 --> 00:19:27.390
<v Speaker 2>that's wrong.

00:19:27.630 --> 00:19:29.790
<v Speaker 2>Right? But most people have exposure

00:19:29.790 --> 00:19:31.149
<v Speaker 2>to LLMs.

00:19:31.230 --> 00:19:32.830
<v Speaker 2>Where so that's why it's becoming, like, the

00:19:32.830 --> 00:19:34.190
<v Speaker 2>Kleenex conversation.

00:19:34.990 --> 00:19:36.830
<v Speaker 2>Again, I want to acknowledge that that's wrong.

00:19:37.465 --> 00:19:39.545
<v Speaker 2>There is very specific terms here, but just

00:19:39.545 --> 00:19:40.745
<v Speaker 2>to get in there for the conversation, I

00:19:40.745 --> 00:19:42.585
<v Speaker 2>wanna make sure that that's that's clear. I

00:19:42.585 --> 00:19:43.945
<v Speaker 2>I know what I'm talking about, but I

00:19:44.025 --> 00:19:45.705
<v Speaker 2>I'm even making this mistake.

00:19:46.985 --> 00:19:49.305
<v Speaker 2>The core value, the core problem of this

00:19:49.305 --> 00:19:50.505
<v Speaker 2>whole narrative

00:19:50.585 --> 00:19:51.385
<v Speaker 2>is that

00:19:51.990 --> 00:19:54.070
<v Speaker 2>we're talking about the compute. We're talking about

00:19:54.070 --> 00:19:54.950
<v Speaker 2>the compiler.

00:19:55.350 --> 00:19:57.270
<v Speaker 2>We're not talking about the source code, which

00:19:57.270 --> 00:19:58.230
<v Speaker 2>is the data.

00:19:58.630 --> 00:20:01.590
<v Speaker 2>Right? And the data is what actually gives

00:20:01.590 --> 00:20:02.789
<v Speaker 2>the compiler,

00:20:03.270 --> 00:20:04.870
<v Speaker 2>which gives you the answers of the entire

00:20:04.950 --> 00:20:05.830
<v Speaker 2>of the the AI,

00:20:06.634 --> 00:20:07.594
<v Speaker 2>the information.

00:20:08.075 --> 00:20:09.355
<v Speaker 2>We need to figure out a way that

00:20:09.355 --> 00:20:10.874
<v Speaker 2>we can have trusted data

00:20:11.115 --> 00:20:14.154
<v Speaker 2>that exists in the space that we know

00:20:14.154 --> 00:20:17.195
<v Speaker 2>that won't create the court filing or has

00:20:17.195 --> 00:20:19.860
<v Speaker 2>the ability to create a court filing. Right?

00:20:19.860 --> 00:20:21.540
<v Speaker 2>And and we have that ability to have

00:20:21.540 --> 00:20:23.940
<v Speaker 2>that conversation and trust that the AI does

00:20:23.940 --> 00:20:25.780
<v Speaker 2>it. But, again, it goes back to the

00:20:25.780 --> 00:20:26.500
<v Speaker 2>pile,

00:20:26.660 --> 00:20:29.300
<v Speaker 2>right, which is a bunch of untrusted data

00:20:29.300 --> 00:20:31.940
<v Speaker 2>where, again, I'm gonna be an IBM shrill

00:20:31.940 --> 00:20:34.235
<v Speaker 2>for a second and say, we have engineers

00:20:34.235 --> 00:20:37.195
<v Speaker 2>who are dedicated to cleaning the pile.

00:20:37.355 --> 00:20:39.674
<v Speaker 2>We actually have humans that are going through

00:20:39.674 --> 00:20:41.914
<v Speaker 2>the pile, making sure that it is trusted

00:20:41.914 --> 00:20:44.875
<v Speaker 2>data to build build our foundational models off

00:20:44.875 --> 00:20:45.034
<v Speaker 2>of.

00:20:45.690 --> 00:20:47.690
<v Speaker 2>So we pull from the open source ecosystem.

00:20:47.850 --> 00:20:49.690
<v Speaker 2>So we actually do have like, that's the

00:20:49.690 --> 00:20:52.409
<v Speaker 2>other part of IBM's whole model. And we

00:20:52.490 --> 00:20:54.330
<v Speaker 2>why we have such a good relationship, you

00:20:54.330 --> 00:20:55.690
<v Speaker 2>look into it, with Hugging

00:20:55.850 --> 00:20:56.649
<v Speaker 2>specifically.

00:20:56.889 --> 00:21:00.164
<v Speaker 2>We've built partnerships directly with Hugging Face to

00:21:00.164 --> 00:21:02.725
<v Speaker 2>pull from the open source ecosystem because we,

00:21:02.725 --> 00:21:05.605
<v Speaker 2>as IBMers, know that the open source ecosystem

00:21:05.605 --> 00:21:07.845
<v Speaker 2>is gonna drive the winds of the sail,

00:21:07.845 --> 00:21:10.404
<v Speaker 2>the ship, or whatever you wanna whatever enterprise

00:21:10.404 --> 00:21:12.485
<v Speaker 2>y statement you wanna make there of the

00:21:12.485 --> 00:21:12.884
<v Speaker 2>industry.

00:21:13.650 --> 00:21:14.210
<v Speaker 2>And

00:21:15.250 --> 00:21:17.330
<v Speaker 2>we acknowledge that. So we are we are

00:21:17.330 --> 00:21:19.090
<v Speaker 2>spending a lot of time and effort in

00:21:19.090 --> 00:21:21.650
<v Speaker 2>building and building bridges to Hugging Face and

00:21:21.650 --> 00:21:24.450
<v Speaker 2>taking the open source stuff from Hugging Face

00:21:24.690 --> 00:21:26.530
<v Speaker 2>and finding a way to make it

00:21:27.250 --> 00:21:30.105
<v Speaker 2>safe for enterprises to work. We have whole

00:21:30.105 --> 00:21:32.665
<v Speaker 2>teams of people inside of our research org

00:21:32.665 --> 00:21:33.705
<v Speaker 2>at IBM

00:21:34.025 --> 00:21:36.025
<v Speaker 2>that that's all they do is they clean

00:21:36.025 --> 00:21:36.664
<v Speaker 2>data.

00:21:36.985 --> 00:21:39.225
<v Speaker 2>I don't envy them at all because that's

00:21:39.225 --> 00:21:40.825
<v Speaker 2>gotta be boring, but

00:21:41.625 --> 00:21:43.785
<v Speaker 2>we do have these people who do it.

00:21:43.785 --> 00:21:44.905
<v Speaker 2>And I have

00:21:45.340 --> 00:21:47.100
<v Speaker 2>I I was at a training relatively recently

00:21:47.100 --> 00:21:49.100
<v Speaker 2>about this, and it was it was they

00:21:49.100 --> 00:21:51.100
<v Speaker 2>they were proud because they were able to

00:21:51.100 --> 00:21:53.899
<v Speaker 2>say that this sliver of this knowledge inside

00:21:53.899 --> 00:21:55.899
<v Speaker 2>of this open source dataset,

00:21:56.139 --> 00:21:56.779
<v Speaker 2>we can now

00:21:57.255 --> 00:22:00.375
<v Speaker 2>talk to our downstream clients to say, no.

00:22:00.375 --> 00:22:03.895
<v Speaker 2>We can provide this level of confidence

00:22:03.895 --> 00:22:06.375
<v Speaker 2>with this level of information, and you put

00:22:06.375 --> 00:22:06.934
<v Speaker 2>your

00:22:07.175 --> 00:22:08.775
<v Speaker 2>level on top of it and you build

00:22:08.775 --> 00:22:11.414
<v Speaker 2>some prompt prompt engineering on top of that.

00:22:11.575 --> 00:22:12.615
<v Speaker 2>And then all of a sudden,

00:22:13.460 --> 00:22:15.380
<v Speaker 2>you're getting the answers you're looking for.

00:22:16.260 --> 00:22:17.940
<v Speaker 1>Is that sanitized

00:22:18.180 --> 00:22:21.060
<v Speaker 1>data that is now verified pushed back to

00:22:21.060 --> 00:22:22.580
<v Speaker 1>Hugging Face for other people

00:22:23.060 --> 00:22:26.100
<v Speaker 1>to use? That's that's well, because we're investing

00:22:26.100 --> 00:22:27.780
<v Speaker 2>so much money at this exact moment in

00:22:27.780 --> 00:22:28.260
<v Speaker 2>time, no.

00:22:29.245 --> 00:22:30.765
<v Speaker 2>We are we that is gonna be one

00:22:30.765 --> 00:22:33.005
<v Speaker 2>of our selling points is that we can

00:22:33.005 --> 00:22:35.004
<v Speaker 2>give you the core data.

00:22:35.325 --> 00:22:37.485
<v Speaker 2>We can give you the dataset

00:22:37.565 --> 00:22:39.405
<v Speaker 2>that we have built our model. And, of

00:22:39.405 --> 00:22:41.405
<v Speaker 2>course, the model that we're gonna be offering

00:22:41.405 --> 00:22:43.750
<v Speaker 2>to you through the Watson X platform will

00:22:43.750 --> 00:22:45.509
<v Speaker 2>be so large and so

00:22:46.390 --> 00:22:48.230
<v Speaker 2>processor intensive. We need to run it on

00:22:48.230 --> 00:22:50.470
<v Speaker 2>our cloud. I mean, what does IBM have?

00:22:50.549 --> 00:22:53.270
<v Speaker 2>It's literally business machines. Right? So we have

00:22:53.270 --> 00:22:54.710
<v Speaker 2>a lot of compute power to do this

00:22:54.710 --> 00:22:56.390
<v Speaker 2>stuff. So we can run that model that

00:22:56.390 --> 00:22:57.830
<v Speaker 2>we but we can give you the actual

00:22:57.830 --> 00:23:01.534
<v Speaker 2>dataset. And we can say with legal penalties

00:23:01.615 --> 00:23:04.015
<v Speaker 2>that this is the dataset that builds this

00:23:04.015 --> 00:23:04.575
<v Speaker 2>model

00:23:04.975 --> 00:23:06.815
<v Speaker 2>so you can actually push this out to

00:23:06.975 --> 00:23:08.815
<v Speaker 2>and put your level of information on top

00:23:08.815 --> 00:23:10.895
<v Speaker 2>of it to get the information you're looking

00:23:10.895 --> 00:23:11.054
<v Speaker 2>for.

00:23:12.220 --> 00:23:12.940
<v Speaker 1>K.

00:23:13.100 --> 00:23:15.580
<v Speaker 1>Can we I wanna make sure so I

00:23:15.820 --> 00:23:16.459
<v Speaker 1>like,

00:23:17.100 --> 00:23:18.700
<v Speaker 1>let's let's take all the things we've covered.

00:23:18.700 --> 00:23:21.419
<v Speaker 1>Right? What's next? We've got clean, trusted data.

00:23:21.580 --> 00:23:23.500
<v Speaker 1>People people can come along and say, okay.

00:23:23.500 --> 00:23:25.259
<v Speaker 1>I'm gonna use this for my organization because

00:23:25.259 --> 00:23:26.825
<v Speaker 1>I have this level of trust. Right? Now

00:23:26.825 --> 00:23:28.664
<v Speaker 1>I'd like to understand, like, a real

00:23:29.705 --> 00:23:32.345
<v Speaker 1>real use case that any developer listening to

00:23:32.345 --> 00:23:34.105
<v Speaker 1>this episode would be like, yeah. That sounds

00:23:34.105 --> 00:23:36.905
<v Speaker 1>really, really sweet. Right? And I'm

00:23:36.905 --> 00:23:40.440
<v Speaker 1>wondering, like, could we take Kubernetes events? Could

00:23:40.440 --> 00:23:42.279
<v Speaker 1>we take metric servers? Could we push this

00:23:42.279 --> 00:23:45.080
<v Speaker 1>all into Watson x? Can we derive

00:23:45.080 --> 00:23:48.360
<v Speaker 1>insights and predictability into our workloads on our

00:23:48.360 --> 00:23:50.519
<v Speaker 1>Kubernetes cluster as a as an example. Right?

00:23:50.519 --> 00:23:52.519
<v Speaker 1>Is is that something you're seeing people do?

00:23:53.399 --> 00:23:54.279
<v Speaker 1>Well,

00:23:56.485 --> 00:23:57.924
<v Speaker 2>the answer is yes

00:23:59.045 --> 00:24:01.284
<v Speaker 2>with a lot of asterisks behind.

00:24:03.684 --> 00:24:06.085
<v Speaker 2>Or I'm sorry. I'm a c yeah. I'm

00:24:06.085 --> 00:24:07.924
<v Speaker 2>sorry. I'm a senior engineer. It depends.

00:24:09.125 --> 00:24:10.090
<v Speaker 2>But the

00:24:10.649 --> 00:24:12.490
<v Speaker 2>so so as as much as the as

00:24:12.490 --> 00:24:14.090
<v Speaker 2>much as the conversation we had around Watson

00:24:14.090 --> 00:24:14.889
<v Speaker 2>X is,

00:24:15.610 --> 00:24:18.249
<v Speaker 2>the best part about of this whole narrative

00:24:18.409 --> 00:24:20.169
<v Speaker 2>is that I use this term in a

00:24:20.169 --> 00:24:21.850
<v Speaker 2>lot of conversations I have, but it is

00:24:21.850 --> 00:24:24.365
<v Speaker 2>valid in in this one. We want to

00:24:24.365 --> 00:24:26.524
<v Speaker 2>be the red solo cups of AI.

00:24:27.565 --> 00:24:28.924
<v Speaker 2>If you don't know what red solo cup

00:24:28.924 --> 00:24:30.524
<v Speaker 2>is, watch any

00:24:30.764 --> 00:24:31.404
<v Speaker 2>US

00:24:31.804 --> 00:24:34.445
<v Speaker 2>college movie. Right? The beer pong cups. Right?

00:24:34.445 --> 00:24:35.084
<v Speaker 2>Beer

00:24:35.485 --> 00:24:36.284
<v Speaker 2>pong cups. Yes.

00:24:36.605 --> 00:24:39.640
<v Speaker 0>That's that's correct. Yes. Exactly. I'm not in

00:24:39.640 --> 00:24:40.920
<v Speaker 1>The US, but that's all I know them

00:24:40.920 --> 00:24:42.440
<v Speaker 1>as. That's it. Yeah.

00:24:43.160 --> 00:24:45.400
<v Speaker 2>But Solo makes an insane amount of money

00:24:45.400 --> 00:24:47.640
<v Speaker 2>on those things because they're everywhere. Right? They're

00:24:47.640 --> 00:24:49.320
<v Speaker 2>just the way you get you you do

00:24:49.320 --> 00:24:49.720
<v Speaker 2>that stuff.

00:24:51.545 --> 00:24:54.025
<v Speaker 2>IBM wants to be that of AI. We

00:24:54.025 --> 00:24:55.945
<v Speaker 2>do not want you to interface directly

00:24:56.185 --> 00:24:58.505
<v Speaker 2>with Watson x. We don't wanna be to

00:24:58.505 --> 00:25:00.904
<v Speaker 2>be sorry. We do not want to be

00:25:00.905 --> 00:25:03.465
<v Speaker 2>b to c. We want to be b

00:25:03.465 --> 00:25:03.865
<v Speaker 2>to b.

00:25:04.440 --> 00:25:06.360
<v Speaker 2>We want to give you an API with

00:25:06.360 --> 00:25:07.960
<v Speaker 2>a trusted environment to be able to work

00:25:07.960 --> 00:25:09.559
<v Speaker 2>off of that. Now the reason why I'm

00:25:09.559 --> 00:25:10.919
<v Speaker 2>saying this is because

00:25:11.240 --> 00:25:13.080
<v Speaker 2>the developers are listening to your

00:25:13.320 --> 00:25:15.080
<v Speaker 2>your your podcast right now.

00:25:15.559 --> 00:25:17.559
<v Speaker 2>What I am offering to you is a

00:25:17.559 --> 00:25:18.119
<v Speaker 2>simple,

00:25:18.360 --> 00:25:19.159
<v Speaker 2>non crazy,

00:25:19.785 --> 00:25:20.425
<v Speaker 2>right,

00:25:20.585 --> 00:25:23.305
<v Speaker 2>REST API with Watson X that you can

00:25:23.305 --> 00:25:25.865
<v Speaker 2>trust that you can just literally use requests

00:25:25.865 --> 00:25:26.905
<v Speaker 2>from Python,

00:25:27.145 --> 00:25:29.065
<v Speaker 2>right, to do a simple post

00:25:29.385 --> 00:25:32.185
<v Speaker 2>to the back end with a certain API

00:25:32.185 --> 00:25:34.105
<v Speaker 2>key and a couple of requests of, like,

00:25:34.105 --> 00:25:35.680
<v Speaker 2>which model you wanna push it through. It's

00:25:35.680 --> 00:25:37.039
<v Speaker 2>a really simple JSON

00:25:37.760 --> 00:25:41.200
<v Speaker 2>or yeah. JSON. And then it comes back

00:25:41.280 --> 00:25:43.440
<v Speaker 2>with what you you want. So you do

00:25:43.440 --> 00:25:46.080
<v Speaker 2>all the heavy lifting inside of Watson X.

00:25:46.080 --> 00:25:47.280
<v Speaker 2>But for your application,

00:25:48.034 --> 00:25:50.034
<v Speaker 2>you can literally just leverage it a little

00:25:50.034 --> 00:25:52.115
<v Speaker 2>bit higher. Right? You just add one little

00:25:52.115 --> 00:25:53.635
<v Speaker 2>request, and it comes back with a nice

00:25:53.635 --> 00:25:54.994
<v Speaker 2>little blob of information.

00:25:55.315 --> 00:25:57.474
<v Speaker 2>So the practicality of it is,

00:25:58.115 --> 00:25:59.474
<v Speaker 2>again, it depends.

00:25:59.635 --> 00:26:02.034
<v Speaker 2>Right? But the idea is we we're trying

00:26:02.034 --> 00:26:03.980
<v Speaker 2>to build the railroad for you here so

00:26:03.980 --> 00:26:06.140
<v Speaker 2>you can get the information you're looking for

00:26:06.140 --> 00:26:08.299
<v Speaker 2>and be able to pull intelligently back.

00:26:08.460 --> 00:26:09.740
<v Speaker 2>Did that answer your question?

00:26:10.220 --> 00:26:10.860
<v Speaker 1>So

00:26:11.340 --> 00:26:13.820
<v Speaker 1>It did. Yes. Thank you. I was gonna

00:26:13.820 --> 00:26:15.580
<v Speaker 0>say, so, basically, what you're trying to do

00:26:15.580 --> 00:26:17.260
<v Speaker 0>is you're trying to say, okay. See all

00:26:17.260 --> 00:26:19.020
<v Speaker 0>these booths at KubeCon.

00:26:19.635 --> 00:26:20.515
<v Speaker 0>Imagine

00:26:20.595 --> 00:26:22.675
<v Speaker 0>a number of these companies actually running on

00:26:22.675 --> 00:26:24.595
<v Speaker 0>top of what's next underneath, but you just

00:26:24.595 --> 00:26:25.554
<v Speaker 0>don't know it yet.

00:26:26.115 --> 00:26:28.674
<v Speaker 0>You as a consumer wandering around KubeCon.

00:26:29.235 --> 00:26:30.355
<v Speaker 0>Yeah. Okay.

00:26:30.435 --> 00:26:32.870
<v Speaker 0>Okay. We're we're building phone lines, building the

00:26:32.870 --> 00:26:34.630
<v Speaker 2>railroad, whatever you wanna call it about. Okay.

00:26:34.630 --> 00:26:37.750
<v Speaker 2>Whatever major infrastructure change you're thinking of, in

00:26:37.750 --> 00:26:39.590
<v Speaker 2>essence, we're trying to do that for AI.

00:26:39.990 --> 00:26:42.230
<v Speaker 0>And You're trying to be Bell Labs is

00:26:42.230 --> 00:26:43.910
<v Speaker 0>basically what you're telling me. You're trying to

00:26:43.910 --> 00:26:46.225
<v Speaker 0>be the old school Bell Labs that's building

00:26:46.225 --> 00:26:47.504
<v Speaker 0>out the original

00:26:48.145 --> 00:26:50.624
<v Speaker 0>stuff that eventually everybody builds on top of.

00:26:51.184 --> 00:26:53.505
<v Speaker 2>Yep. And then on top of that too,

00:26:53.505 --> 00:26:55.745
<v Speaker 2>like, haven't even mentioned quantum inside of this

00:26:55.745 --> 00:26:57.905
<v Speaker 2>space. Right? There's a there's a spur that

00:26:57.905 --> 00:26:59.585
<v Speaker 0>goes into that ecosystem buzzwords.

00:27:00.280 --> 00:27:03.640
<v Speaker 0>Don't just throw buzzwords, JJ. I I have

00:27:03.640 --> 00:27:05.000
<v Speaker 2>a bunch of PhDs.

00:27:05.000 --> 00:27:07.480
<v Speaker 2>All they do every PhD physicist all day

00:27:07.480 --> 00:27:09.080
<v Speaker 2>every day. All they do is look at

00:27:09.080 --> 00:27:10.360
<v Speaker 2>this weird ass

00:27:11.080 --> 00:27:12.920
<v Speaker 2>computer that's gonna take all our lives or

00:27:12.920 --> 00:27:16.435
<v Speaker 2>something. Yeah. Exactly. That makes sense. I I

00:27:16.435 --> 00:27:19.475
<v Speaker 0>guess my my question always comes down to

00:27:19.555 --> 00:27:22.115
<v Speaker 0>there's a people problem under this to me.

00:27:22.115 --> 00:27:24.675
<v Speaker 0>Right? Under AI in general. No matter where

00:27:24.675 --> 00:27:27.029
<v Speaker 0>where it ends up, if it gets integrated

00:27:27.029 --> 00:27:28.710
<v Speaker 0>into the cloud native ecosystem or if it

00:27:28.710 --> 00:27:31.190
<v Speaker 0>stays in the, like, AI ecosystem,

00:27:31.190 --> 00:27:32.789
<v Speaker 0>I guess, for lack of a better term.

00:27:33.269 --> 00:27:34.389
<v Speaker 0>But,

00:27:35.590 --> 00:27:38.230
<v Speaker 0>you know, we can get the models to

00:27:38.470 --> 00:27:40.184
<v Speaker 0>we can get the AI to start saying,

00:27:40.184 --> 00:27:42.105
<v Speaker 0>here's the citations. We can get people to

00:27:42.105 --> 00:27:44.904
<v Speaker 0>try to verify everything, but we have that

00:27:44.904 --> 00:27:47.065
<v Speaker 0>trust but verify. But how many people sit

00:27:47.065 --> 00:27:48.984
<v Speaker 0>there and actually look at the citation?

00:27:49.705 --> 00:27:52.025
<v Speaker 0>Right? Like, that always was a thing in

00:27:52.025 --> 00:27:54.500
<v Speaker 0>science, remember, was people don't always check the

00:27:54.500 --> 00:27:56.580
<v Speaker 0>citations. You have to learn to be very

00:27:56.580 --> 00:27:59.140
<v Speaker 0>good about checking those. It doesn't come to

00:27:59.140 --> 00:28:00.820
<v Speaker 0>you naturally most of the time.

00:28:01.140 --> 00:28:03.460
<v Speaker 0>So how do we, like if I think

00:28:03.460 --> 00:28:04.820
<v Speaker 0>about, let's say, that

00:28:05.635 --> 00:28:08.515
<v Speaker 0>some company decides, okay, I'm gonna build a

00:28:08.515 --> 00:28:10.515
<v Speaker 0>monitoring tool that goes through all of my

00:28:10.515 --> 00:28:11.475
<v Speaker 0>live metrics

00:28:11.715 --> 00:28:12.995
<v Speaker 0>and analyzes

00:28:12.995 --> 00:28:15.235
<v Speaker 0>everything using AI and comes out with an

00:28:15.235 --> 00:28:18.195
<v Speaker 0>idea of, okay, this system probably is gonna

00:28:18.195 --> 00:28:20.195
<v Speaker 0>fail, has like a twenty percent chance of

00:28:20.195 --> 00:28:23.030
<v Speaker 0>failing in the next twenty four hours. Right?

00:28:23.030 --> 00:28:24.950
<v Speaker 0>And let's say that's an eventual

00:28:25.030 --> 00:28:27.350
<v Speaker 0>thing because that's an evaluation a person might

00:28:27.350 --> 00:28:27.910
<v Speaker 0>do.

00:28:28.390 --> 00:28:30.550
<v Speaker 0>But it gives you all the citations. Who's

00:28:30.550 --> 00:28:32.390
<v Speaker 0>gonna go back and look at their logs

00:28:32.390 --> 00:28:35.350
<v Speaker 0>to verify it right now? Right? If I'm

00:28:35.350 --> 00:28:36.310
<v Speaker 0>told, oh, it works.

00:28:37.265 --> 00:28:40.304
<v Speaker 0>My reaction is, okay, it works. Right? Yeah.

00:28:40.304 --> 00:28:41.825
<v Speaker 0>So how do you how do you deal

00:28:41.825 --> 00:28:43.664
<v Speaker 0>with that people problem when it comes to

00:28:43.664 --> 00:28:45.585
<v Speaker 0>this AI question of,

00:28:45.904 --> 00:28:47.984
<v Speaker 0>well, we do have to verify it somewhere.

00:28:48.144 --> 00:28:50.065
<v Speaker 0>Otherwise, we get pages that wake us up

00:28:50.065 --> 00:28:51.345
<v Speaker 0>at 3AM in the morning

00:28:51.950 --> 00:28:52.590
<v Speaker 0>and

00:28:53.470 --> 00:28:56.110
<v Speaker 0>for nothing because nothing actually went wrong because

00:28:56.110 --> 00:28:58.830
<v Speaker 0>the AI got it wrong because they analyzed

00:28:58.830 --> 00:29:01.630
<v Speaker 0>some other incident from somebody else that just

00:29:01.630 --> 00:29:03.470
<v Speaker 0>happened to have a correlation here or something.

00:29:03.470 --> 00:29:04.565
<v Speaker 0>Right? Like,

00:29:05.445 --> 00:29:07.365
<v Speaker 0>how do we get there? I don't know

00:29:07.365 --> 00:29:09.525
<v Speaker 0>if that makes any sense. But How do

00:29:09.525 --> 00:29:11.205
<v Speaker 2>we fix the chicken little problem?

00:29:12.165 --> 00:29:13.925
<v Speaker 1>Can I try and broaden that question? Because

00:29:13.925 --> 00:29:16.165
<v Speaker 1>I think they're both thinking some something very

00:29:16.165 --> 00:29:17.605
<v Speaker 1>similar. Right? And I don't know if I

00:29:17.845 --> 00:29:19.720
<v Speaker 0>trying to get there. Yeah. David, you might

00:29:19.720 --> 00:29:20.840
<v Speaker 0>be able to say this better than I

00:29:20.840 --> 00:29:22.200
<v Speaker 0>can, so go for it. I I I

00:29:22.200 --> 00:29:24.200
<v Speaker 1>don't think so. But I had that question,

00:29:24.200 --> 00:29:25.720
<v Speaker 1>and then you asked something really similar. And

00:29:25.720 --> 00:29:26.919
<v Speaker 1>I'm like, oh, if we just expect, like

00:29:27.000 --> 00:29:28.119
<v Speaker 1>so let's try it. Right?

00:29:28.760 --> 00:29:30.360
<v Speaker 1>We we've got all we've got a world

00:29:30.360 --> 00:29:32.945
<v Speaker 1>of AIs. Every developer is out there talking

00:29:32.945 --> 00:29:35.184
<v Speaker 1>to a different AI. I'm just gonna call

00:29:35.184 --> 00:29:37.105
<v Speaker 1>them singularities and AIs. Right? I I know

00:29:37.105 --> 00:29:38.864
<v Speaker 1>this deeper whatever.

00:29:39.585 --> 00:29:41.424
<v Speaker 1>Now we're all asking these questions. We're all

00:29:41.424 --> 00:29:42.544
<v Speaker 1>getting answers,

00:29:42.705 --> 00:29:45.424
<v Speaker 1>and we mentioned licensing. We mentioned copyright. Derivative

00:29:45.424 --> 00:29:47.380
<v Speaker 1>works are obviously a huge challenge when it

00:29:47.380 --> 00:29:49.460
<v Speaker 1>comes to people when we're using these answers

00:29:49.460 --> 00:29:51.940
<v Speaker 1>to then put something out into the world.

00:29:52.340 --> 00:29:54.580
<v Speaker 1>So there's this ethical dilemma

00:29:54.580 --> 00:29:56.260
<v Speaker 1>as well, which I think ties into what

00:29:56.260 --> 00:29:58.019
<v Speaker 1>Laura was asking, is that if

00:29:58.820 --> 00:30:01.035
<v Speaker 1>10 people go and ask AI how to

00:30:01.035 --> 00:30:03.035
<v Speaker 1>write a good song or a catchy song

00:30:03.035 --> 00:30:04.635
<v Speaker 1>or a number one hit and then they

00:30:04.635 --> 00:30:06.315
<v Speaker 1>all go start to use this,

00:30:06.795 --> 00:30:09.435
<v Speaker 1>that's 10 different AIs that then somehow

00:30:09.595 --> 00:30:11.515
<v Speaker 1>need to be answered the question of

00:30:12.760 --> 00:30:13.639
<v Speaker 1>backtrack.

00:30:13.960 --> 00:30:15.880
<v Speaker 1>Do we need another AI to answer the

00:30:15.880 --> 00:30:17.720
<v Speaker 1>question of was this written by an AI?

00:30:17.880 --> 00:30:19.720
<v Speaker 1>Do these AI companies need to work together

00:30:19.720 --> 00:30:22.200
<v Speaker 1>to provide a transparency log to inputs and

00:30:22.200 --> 00:30:24.760
<v Speaker 1>outputs to prove that something was an artifact

00:30:24.760 --> 00:30:25.399
<v Speaker 1>from

00:30:25.960 --> 00:30:26.200
<v Speaker 1>their

00:30:26.794 --> 00:30:27.754
<v Speaker 1>algorithm.

00:30:28.475 --> 00:30:30.554
<v Speaker 1>How do what's the future look like? The

00:30:30.554 --> 00:30:32.715
<v Speaker 1>the this is clearly a tough question or

00:30:32.715 --> 00:30:35.515
<v Speaker 1>a tough predicament. So what how does IBM

00:30:35.515 --> 00:30:37.115
<v Speaker 1>tackle that? How does what's next tackle that?

00:30:37.115 --> 00:30:38.794
<v Speaker 1>Are there conversations with other companies?

00:30:40.160 --> 00:30:42.960
<v Speaker 2>This was actually a a wonderful

00:30:43.360 --> 00:30:45.679
<v Speaker 2>working session at Fosse.

00:30:46.400 --> 00:30:48.080
<v Speaker 2>We had this whole and I I can

00:30:48.080 --> 00:30:50.159
<v Speaker 2>I can link to the Etherpad

00:30:50.160 --> 00:30:50.879
<v Speaker 2>that

00:30:51.520 --> 00:30:52.799
<v Speaker 2>captured all the notes for this?

00:30:53.795 --> 00:30:56.915
<v Speaker 2>And it was basically the open source if

00:30:56.915 --> 00:30:58.355
<v Speaker 2>if you don't know what Fosse is, it's

00:30:58.355 --> 00:31:00.274
<v Speaker 2>what OSCON used to be.

00:31:00.995 --> 00:31:03.315
<v Speaker 2>But Fosse has broken off from it's the

00:31:03.315 --> 00:31:05.635
<v Speaker 2>same group of people, but not under the

00:31:05.635 --> 00:31:06.835
<v Speaker 2>O'Reilly banner.

00:31:07.580 --> 00:31:10.700
<v Speaker 2>It's it's completely it's under the software conservancy.

00:31:11.340 --> 00:31:13.020
<v Speaker 2>They're the ones who go after the GPL

00:31:13.020 --> 00:31:13.580
<v Speaker 2>people.

00:31:13.820 --> 00:31:16.140
<v Speaker 2>Right? They they they have the GPL lawyers,

00:31:16.380 --> 00:31:19.100
<v Speaker 2>and they they are they're an interesting breed.

00:31:19.100 --> 00:31:20.940
<v Speaker 2>Let's just put it that way. Lot of

00:31:20.940 --> 00:31:22.300
<v Speaker 2>stallmen. Lot of stallmen. Anyway,

00:31:24.435 --> 00:31:27.155
<v Speaker 2>the the the interesting conversation happened was how

00:31:27.155 --> 00:31:27.555
<v Speaker 2>do you

00:31:28.275 --> 00:31:29.794
<v Speaker 2>how can you verify

00:31:29.955 --> 00:31:32.995
<v Speaker 2>and and say that this data is is

00:31:32.995 --> 00:31:33.955
<v Speaker 2>not stolen?

00:31:34.515 --> 00:31:36.115
<v Speaker 2>That was what it boiled down to, which

00:31:36.115 --> 00:31:37.795
<v Speaker 2>is, I think, kind of where you're going

00:31:37.795 --> 00:31:39.389
<v Speaker 2>there. K. And

00:31:41.070 --> 00:31:43.070
<v Speaker 2>it always went back to the it always

00:31:43.230 --> 00:31:46.829
<v Speaker 2>this this end point this end entity,

00:31:46.830 --> 00:31:49.389
<v Speaker 2>this tarball. Sure. It was called a tarball.

00:31:50.029 --> 00:31:51.949
<v Speaker 2>This make sure that this tarball,

00:31:52.110 --> 00:31:53.629
<v Speaker 2>which most likely is a binary,

00:31:54.295 --> 00:31:55.495
<v Speaker 2>isn't stolen.

00:31:56.375 --> 00:31:58.375
<v Speaker 2>It always went back to the data. It

00:31:58.375 --> 00:32:00.375
<v Speaker 2>always kept going back to if I can

00:32:00.375 --> 00:32:03.495
<v Speaker 2>prove to you that this tarball came out

00:32:03.495 --> 00:32:05.335
<v Speaker 2>of this data block

00:32:06.135 --> 00:32:08.490
<v Speaker 2>through the model that I've created and it

00:32:08.490 --> 00:32:10.570
<v Speaker 2>is this data is verifiable,

00:32:10.890 --> 00:32:12.490
<v Speaker 2>that is the only way that you can

00:32:12.490 --> 00:32:13.690
<v Speaker 2>backtrack through.

00:32:14.490 --> 00:32:16.170
<v Speaker 2>Problem is models aren't

00:32:16.410 --> 00:32:18.970
<v Speaker 2>models don't have, like, a SHA. Right? They

00:32:18.970 --> 00:32:20.650
<v Speaker 2>don't have some way to say that this

00:32:20.650 --> 00:32:22.010
<v Speaker 2>was the model that I used to do

00:32:22.010 --> 00:32:23.690
<v Speaker 2>off because the technology just doesn't exist.

00:32:24.465 --> 00:32:26.465
<v Speaker 2>Right? And as much as we want to

00:32:26.465 --> 00:32:28.625
<v Speaker 2>add all that stuff onto this stuff to

00:32:28.625 --> 00:32:29.985
<v Speaker 2>be able to say that, yes, this is

00:32:30.145 --> 00:32:30.945
<v Speaker 2>this exists,

00:32:31.905 --> 00:32:34.865
<v Speaker 2>the ecosystem is already so large

00:32:35.345 --> 00:32:38.465
<v Speaker 2>and has moved far so so so rapidly

00:32:38.465 --> 00:32:40.630
<v Speaker 2>ahead of us where you can get an

00:32:40.630 --> 00:32:43.590
<v Speaker 2>older version of ChatGPT on Hugging Face. You

00:32:43.590 --> 00:32:46.070
<v Speaker 2>didn't know. If they're they've open sourced the

00:32:46.230 --> 00:32:48.550
<v Speaker 2>those models. The models, not the data. The

00:32:48.550 --> 00:32:50.070
<v Speaker 2>models on the Hugging Face so you can

00:32:50.070 --> 00:32:53.025
<v Speaker 2>play with ChatGPT on your local laptop. They're

00:32:53.025 --> 00:32:55.185
<v Speaker 2>shit, but they do exist.

00:32:56.305 --> 00:32:58.385
<v Speaker 2>The challenge is we have no answer for

00:32:58.385 --> 00:33:00.305
<v Speaker 2>that in the space. And the way that

00:33:00.305 --> 00:33:02.785
<v Speaker 2>IBM was answering it is through

00:33:04.305 --> 00:33:05.425
<v Speaker 2>doing the pile

00:33:05.809 --> 00:33:06.690
<v Speaker 2>cleaning,

00:33:06.690 --> 00:33:08.610
<v Speaker 2>being able to show you the models, and

00:33:08.610 --> 00:33:10.450
<v Speaker 2>and with legal penalties,

00:33:10.530 --> 00:33:12.770
<v Speaker 2>say and with literally pen to paper, say,

00:33:12.770 --> 00:33:15.090
<v Speaker 2>yes. This is we can show you exactly

00:33:15.090 --> 00:33:17.649
<v Speaker 2>what's going on. But ChatGPT,

00:33:17.809 --> 00:33:18.690
<v Speaker 2>Microsoft,

00:33:18.850 --> 00:33:20.610
<v Speaker 2>I can tell you they will never do

00:33:20.610 --> 00:33:23.285
<v Speaker 2>that. Right? As as much as as much

00:33:23.285 --> 00:33:25.445
<v Speaker 2>as we want to say that when

00:33:26.645 --> 00:33:29.525
<v Speaker 2>you go into the the disruptors

00:33:29.765 --> 00:33:31.445
<v Speaker 2>I'm just using that as a term to

00:33:31.445 --> 00:33:32.885
<v Speaker 2>cover the non

00:33:33.365 --> 00:33:34.005
<v Speaker 2>the the the

00:33:35.750 --> 00:33:38.230
<v Speaker 2>I say this with love. The normies,

00:33:38.230 --> 00:33:39.989
<v Speaker 2>when they think about ChatGPT,

00:33:39.990 --> 00:33:41.349
<v Speaker 2>right, they

00:33:41.990 --> 00:33:43.909
<v Speaker 2>those are the disruptors, the ones that they're

00:33:43.909 --> 00:33:45.429
<v Speaker 2>gonna be using it to write the paper

00:33:45.429 --> 00:33:47.990
<v Speaker 2>for their history class or whatever. Right?

00:33:49.085 --> 00:33:50.925
<v Speaker 2>The the models and the data that they

00:33:51.005 --> 00:33:52.445
<v Speaker 2>that exists in that space,

00:33:53.485 --> 00:33:55.565
<v Speaker 2>the companies behind it, like OpenAI,

00:33:55.725 --> 00:33:58.044
<v Speaker 2>will never give us those datasets.

00:33:58.285 --> 00:34:00.365
<v Speaker 2>They will never do that. And then with

00:34:00.365 --> 00:34:02.684
<v Speaker 2>Microsoft investing their time and effort with, like,

00:34:03.645 --> 00:34:04.365
<v Speaker 2>a Copilot.

00:34:04.620 --> 00:34:06.300
<v Speaker 2>Right? That's the one I I I kind

00:34:06.300 --> 00:34:07.900
<v Speaker 2>of just skipped over, but I fall in

00:34:07.900 --> 00:34:09.500
<v Speaker 2>the same space as ChatGPT.

00:34:10.940 --> 00:34:13.260
<v Speaker 2>As much as they claim that they didn't

00:34:13.260 --> 00:34:15.340
<v Speaker 2>take code from GitHub,

00:34:15.580 --> 00:34:16.540
<v Speaker 2>as much as they claim

00:34:17.184 --> 00:34:17.904
<v Speaker 2>they they

00:34:18.304 --> 00:34:20.625
<v Speaker 2>you can have only Copilot only look in

00:34:20.625 --> 00:34:21.505
<v Speaker 2>your org.

00:34:22.145 --> 00:34:23.665
<v Speaker 2>We're computer professionals.

00:34:24.224 --> 00:34:25.665
<v Speaker 2>We know that's never true.

00:34:26.145 --> 00:34:27.984
<v Speaker 2>Right? And as soon as you get that

00:34:27.984 --> 00:34:30.704
<v Speaker 2>data swerved into the model, you have no

00:34:30.704 --> 00:34:31.664
<v Speaker 2>way to pull that data out.

00:34:32.469 --> 00:34:35.590
<v Speaker 2>People forget that. Like, models are added There's

00:34:35.590 --> 00:34:38.870
<v Speaker 0>no Like, there's no there's no reverting.

00:34:39.030 --> 00:34:41.030
<v Speaker 0>Once you once you've trained something, there's no

00:34:41.030 --> 00:34:42.550
<v Speaker 0>way to revert it and, like,

00:34:43.430 --> 00:34:46.165
<v Speaker 0>pull it out. Not easily. Not no. You

00:34:46.165 --> 00:34:48.165
<v Speaker 2>could destroy the model and recreate the model

00:34:48.165 --> 00:34:50.324
<v Speaker 2>with removing the data. But then again, if

00:34:50.324 --> 00:34:52.165
<v Speaker 2>that model's already out there, it just again,

00:34:52.165 --> 00:34:53.924
<v Speaker 2>it's it's a compiler. You've gotta think of

00:34:53.924 --> 00:34:55.284
<v Speaker 2>it as a compiler, and all of sudden,

00:34:55.284 --> 00:34:56.484
<v Speaker 2>a lot more things start making sense.

00:34:57.620 --> 00:35:00.740
<v Speaker 2>So Microsoft will never give us a legal

00:35:00.740 --> 00:35:02.980
<v Speaker 2>affidavit saying that, no. I did not take

00:35:02.980 --> 00:35:05.620
<v Speaker 2>any private repos from GitHub.

00:35:05.860 --> 00:35:06.740
<v Speaker 2>And, no,

00:35:08.180 --> 00:35:09.935
<v Speaker 2>I used I looked through through all of

00:35:09.935 --> 00:35:12.335
<v Speaker 2>GitHub looking for all the license files to

00:35:12.335 --> 00:35:14.735
<v Speaker 2>make sure the attribution is correct on all

00:35:14.735 --> 00:35:16.895
<v Speaker 2>of the open source projects I did. That

00:35:16.895 --> 00:35:20.255
<v Speaker 2>alone is a Herculean effort. Like, what did

00:35:20.255 --> 00:35:22.175
<v Speaker 2>you get trained off of this this code?

00:35:22.175 --> 00:35:23.935
<v Speaker 2>Did somebody just put up on GitHub and

00:35:23.935 --> 00:35:25.135
<v Speaker 2>there's no default license?

00:35:25.820 --> 00:35:27.180
<v Speaker 2>Legally speaking,

00:35:27.260 --> 00:35:30.700
<v Speaker 2>Software Conservancy couldn't represent them in court going

00:35:30.700 --> 00:35:33.500
<v Speaker 2>after Microsoft because of that. Right? And it

00:35:33.660 --> 00:35:35.020
<v Speaker 2>again, it goes back to what we were

00:35:35.020 --> 00:35:36.700
<v Speaker 2>starting at the very beginning in this conversation

00:35:36.700 --> 00:35:37.980
<v Speaker 2>with is

00:35:38.855 --> 00:35:41.895
<v Speaker 2>we have no visibility into this space because

00:35:41.895 --> 00:35:44.454
<v Speaker 2>the technology moves so quickly without checks and

00:35:44.454 --> 00:35:45.255
<v Speaker 2>balances

00:35:45.255 --> 00:35:47.175
<v Speaker 2>that we are now at a point where

00:35:48.135 --> 00:35:48.615
<v Speaker 2>okay.

00:35:49.655 --> 00:35:51.575
<v Speaker 2>Bad analogy, and it just kinda hit me

00:35:51.575 --> 00:35:54.200
<v Speaker 2>right now. Stick with me for a second.

00:35:54.440 --> 00:35:56.200
<v Speaker 0>You know I'm the queen of bad analogies,

00:35:56.200 --> 00:35:57.320
<v Speaker 0>so go for it.

00:35:58.520 --> 00:36:01.000
<v Speaker 2>In essence, we've created a bunch of of

00:36:01.000 --> 00:36:01.800
<v Speaker 2>of

00:36:02.040 --> 00:36:03.320
<v Speaker 2>of printing

00:36:03.320 --> 00:36:04.120
<v Speaker 2>presses.

00:36:04.359 --> 00:36:05.800
<v Speaker 2>Right? All of a sudden,

00:36:06.545 --> 00:36:08.865
<v Speaker 2>we created printing presses all over the world

00:36:08.865 --> 00:36:11.025
<v Speaker 2>that you can create that you don't have

00:36:11.025 --> 00:36:12.305
<v Speaker 2>to organize anymore.

00:36:12.465 --> 00:36:15.025
<v Speaker 2>They can just start printing out information for

00:36:15.025 --> 00:36:17.905
<v Speaker 2>you. And now what's stopping you from selling

00:36:17.905 --> 00:36:18.385
<v Speaker 2>those books?

00:36:19.560 --> 00:36:21.800
<v Speaker 2>Right? Because there's no validation that as long

00:36:21.800 --> 00:36:24.120
<v Speaker 2>as you got that initial printing press with

00:36:24.120 --> 00:36:24.760
<v Speaker 2>the

00:36:25.400 --> 00:36:27.400
<v Speaker 2>the the plates in it to give you

00:36:27.400 --> 00:36:29.960
<v Speaker 2>the books to to to to shove out,

00:36:30.440 --> 00:36:32.040
<v Speaker 2>that is what AI has done. It's given

00:36:32.040 --> 00:36:33.720
<v Speaker 2>the ability to send out that information very

00:36:33.720 --> 00:36:36.055
<v Speaker 2>quickly. At least when we had the Internet

00:36:36.135 --> 00:36:38.535
<v Speaker 2>when it first hit and people were worried

00:36:38.535 --> 00:36:40.535
<v Speaker 2>about pirated books and stuff like that,

00:36:41.575 --> 00:36:43.655
<v Speaker 2>the ecosystem created

00:36:43.975 --> 00:36:47.335
<v Speaker 2>blobs of ways of security and pathways to

00:36:47.335 --> 00:36:49.920
<v Speaker 2>getting these things. Right? And and and validate

00:36:49.920 --> 00:36:52.560
<v Speaker 2>the iTunes of the world, if you will.

00:36:54.160 --> 00:36:55.040
<v Speaker 2>But here,

00:36:56.480 --> 00:36:58.800
<v Speaker 2>that horse is already out of the barn

00:36:58.800 --> 00:37:00.720
<v Speaker 2>and already to the next town.

00:37:01.119 --> 00:37:01.840
<v Speaker 2>Right?

00:37:01.920 --> 00:37:03.760
<v Speaker 2>We've got a lot of catching up to

00:37:03.760 --> 00:37:06.935
<v Speaker 2>do, and the only way and problem also

00:37:06.935 --> 00:37:08.455
<v Speaker 2>is that this is global.

00:37:08.855 --> 00:37:11.735
<v Speaker 2>Right? Like, this is not just our friends

00:37:11.735 --> 00:37:12.455
<v Speaker 2>in China,

00:37:12.855 --> 00:37:15.095
<v Speaker 2>they could create LMs that do all the

00:37:15.175 --> 00:37:17.335
<v Speaker 2>that create patents for them. And

00:37:18.375 --> 00:37:21.950
<v Speaker 2>US law means nothing, right, over there. They

00:37:21.950 --> 00:37:23.710
<v Speaker 2>have their own set of laws and their

00:37:23.710 --> 00:37:25.230
<v Speaker 2>own ways of doing technology,

00:37:25.230 --> 00:37:27.470
<v Speaker 2>and they have a lot of computing power

00:37:27.470 --> 00:37:30.670
<v Speaker 2>over there. Right? So so it just like,

00:37:30.670 --> 00:37:32.109
<v Speaker 2>I'm not trying to be doom and gloom.

00:37:32.715 --> 00:37:35.035
<v Speaker 2>I'm just trying to express this to our

00:37:35.035 --> 00:37:37.355
<v Speaker 2>audience that we're talking to here saying that

00:37:37.355 --> 00:37:39.915
<v Speaker 2>as much as you think, hey. The joke

00:37:39.915 --> 00:37:40.475
<v Speaker 2>of,

00:37:40.875 --> 00:37:41.355
<v Speaker 2>hey.

00:37:41.675 --> 00:37:42.635
<v Speaker 2>ChatGPT,

00:37:42.635 --> 00:37:43.835
<v Speaker 2>give me an application,

00:37:43.915 --> 00:37:45.355
<v Speaker 2>and it gives you an application in five

00:37:45.355 --> 00:37:46.635
<v Speaker 2>minutes. It does blah.

00:37:46.875 --> 00:37:48.830
<v Speaker 2>You're you're gonna be spending twenty four hours

00:37:48.910 --> 00:37:50.990
<v Speaker 2>debugging what the hell that application does because

00:37:50.990 --> 00:37:52.030
<v Speaker 2>you can't trust it.

00:37:52.430 --> 00:37:54.750
<v Speaker 2>And there's this whole ecosystem of around that

00:37:54.750 --> 00:37:57.070
<v Speaker 2>that people don't recognize that it actually spurs

00:37:57.070 --> 00:37:58.910
<v Speaker 2>out to a lot of other stuff. Anyway,

00:37:59.055 --> 00:38:01.214
<v Speaker 2>sorry. I I got on on a roll

00:38:01.214 --> 00:38:02.495
<v Speaker 0>there. It's okay.

00:38:02.974 --> 00:38:05.454
<v Speaker 1>Alright. So the TLDR is we're all doomed.

00:38:05.454 --> 00:38:07.775
<v Speaker 1>It's all fucked. Go hug your loved ones.

00:38:08.575 --> 00:38:10.335
<v Speaker 0>Hug your thing? Turn off just turn off

00:38:10.335 --> 00:38:11.775
<v Speaker 1>the computer. Yeah.

00:38:11.775 --> 00:38:14.175
<v Speaker 2>Hugging face. Hugging hugging the face of the

00:38:14.335 --> 00:38:16.360
<v Speaker 0>face and Turn it off, and the the

00:38:16.360 --> 00:38:18.280
<v Speaker 0>next podcast will be coming to you generated

00:38:18.280 --> 00:38:19.320
<v Speaker 0>by AI.

00:38:19.800 --> 00:38:21.800
<v Speaker 0>Our our faces will be moving,

00:38:22.440 --> 00:38:23.800
<v Speaker 0>but we will not be the ones set.

00:38:23.800 --> 00:38:24.440
<v Speaker 0>And I'm just gonna

00:38:25.720 --> 00:38:27.000
<v Speaker 0>yeah. I guess I guess we're at the

00:38:27.000 --> 00:38:28.120
<v Speaker 0>end of this. I mean, like, there's a

00:38:28.120 --> 00:38:29.240
<v Speaker 0>ton more to dig into.

00:38:30.085 --> 00:38:30.725
<v Speaker 0>And

00:38:31.125 --> 00:38:33.205
<v Speaker 0>who knows? I mean, if if

00:38:33.365 --> 00:38:35.205
<v Speaker 0>there's more you wanna hear on this topic,

00:38:35.205 --> 00:38:36.885
<v Speaker 0>by the way, there is a Discord

00:38:37.125 --> 00:38:40.005
<v Speaker 0>that you can join. It's called Rawkode Academy,

00:38:40.005 --> 00:38:41.125
<v Speaker 0>and there's a channel

00:38:41.660 --> 00:38:43.820
<v Speaker 0>specifically for Cloud Data Compass. And I'm calling

00:38:43.820 --> 00:38:45.100
<v Speaker 0>it out right now because I haven't opened

00:38:45.100 --> 00:38:46.220
<v Speaker 0>in another tab.

00:38:46.860 --> 00:38:49.340
<v Speaker 0>But if you wanna ask more questions, maybe

00:38:49.340 --> 00:38:51.500
<v Speaker 0>we can do another episode on AI

00:38:51.820 --> 00:38:53.180
<v Speaker 0>someday in the future.

00:38:53.660 --> 00:38:56.140
<v Speaker 0>You can see. But yeah. Yeah. Let let

00:38:56.140 --> 00:38:58.220
<v Speaker 1>let me try a

00:38:57.695 --> 00:38:59.055
<v Speaker 1>positive side. Right? Like

00:38:59.855 --> 00:39:01.535
<v Speaker 0>Or positive. Either one.

00:39:02.895 --> 00:39:04.015
<v Speaker 1>Alright. I don't know if there's a bit

00:39:04.015 --> 00:39:05.695
<v Speaker 1>of lack there. No. I was just thinking,

00:39:05.695 --> 00:39:07.775
<v Speaker 1>like, you know, this is like, for me,

00:39:07.775 --> 00:39:09.295
<v Speaker 1>right, I'm not in the ML space. I'm

00:39:09.295 --> 00:39:11.215
<v Speaker 1>not in the AI space. This was all

00:39:11.215 --> 00:39:14.470
<v Speaker 1>new to me as chat GPT and OpenAI

00:39:14.470 --> 00:39:16.150
<v Speaker 1>came out and Google Bard and that sort

00:39:16.150 --> 00:39:16.950
<v Speaker 1>of stuff. Right?

00:39:17.510 --> 00:39:19.269
<v Speaker 1>To me, those were the only options, but

00:39:19.269 --> 00:39:20.869
<v Speaker 1>I think we're in a really fortunate position.

00:39:20.869 --> 00:39:23.029
<v Speaker 1>But there are other options. There's a lot

00:39:23.029 --> 00:39:24.710
<v Speaker 1>of movement. There's a lot of,

00:39:25.109 --> 00:39:26.630
<v Speaker 1>you know, not volatility,

00:39:26.630 --> 00:39:28.694
<v Speaker 1>but it's just there's new ideas coming all

00:39:28.694 --> 00:39:29.894
<v Speaker 1>the time and there's a lot more open

00:39:29.894 --> 00:39:32.055
<v Speaker 1>source happening too, and there's a lot more

00:39:32.055 --> 00:39:34.295
<v Speaker 1>trusted execution environments like we're hearing about with

00:39:34.295 --> 00:39:36.055
<v Speaker 1>Watson X. I think there's a lot of

00:39:36.055 --> 00:39:38.535
<v Speaker 1>positive six twenty come from AI even though

00:39:38.535 --> 00:39:41.450
<v Speaker 1>it's easy to point out the scary bits,

00:39:41.450 --> 00:39:44.010
<v Speaker 1>the negative bits. Right? But I'm I'm still

00:39:44.010 --> 00:39:46.730
<v Speaker 1>extremely hopeful based on everything that I've seen

00:39:46.730 --> 00:39:48.089
<v Speaker 1>in the past and I've heard today from

00:39:48.089 --> 00:39:48.810
<v Speaker 1>JJ

00:39:48.890 --> 00:39:51.290
<v Speaker 1>that it's gonna have a net positive impact

00:39:51.290 --> 00:39:53.609
<v Speaker 1>on my life and hopefully other people's lives.

00:39:53.609 --> 00:39:55.770
<v Speaker 1>Like, I'm I'm excited for the people working

00:39:55.770 --> 00:39:56.089
<v Speaker 1>on this.

00:39:56.945 --> 00:39:57.585
<v Speaker 0>Okay.

00:39:57.905 --> 00:40:00.225
<v Speaker 0>I'm excited. And at the same time, like,

00:40:00.225 --> 00:40:02.945
<v Speaker 0>I have I have more history in, like,

00:40:03.025 --> 00:40:05.905
<v Speaker 0>ML and things like that from Python, but

00:40:05.905 --> 00:40:08.625
<v Speaker 0>also from science. Like, just thinking through that.

00:40:08.625 --> 00:40:11.000
<v Speaker 0>Like, to me, I'll be honest, all of

00:40:11.000 --> 00:40:12.760
<v Speaker 0>this is stats all the way down and

00:40:12.760 --> 00:40:14.920
<v Speaker 0>stats that I have a hate relationship for

00:40:14.920 --> 00:40:15.960
<v Speaker 0>the rest of my life.

00:40:17.480 --> 00:40:19.240
<v Speaker 1>Yeah. I just act her in. I'm just,

00:40:19.240 --> 00:40:21.000
<v Speaker 1>like, do my homework. Stat it. Stand lies

00:40:21.000 --> 00:40:23.195
<v Speaker 0>and statistics. I mean, come on. Like, that's

00:40:23.195 --> 00:40:24.475
<v Speaker 0>exactly how it works.

00:40:24.955 --> 00:40:26.555
<v Speaker 0>I think Rawkode is a little fiesta in

00:40:26.555 --> 00:40:27.275
<v Speaker 0>that sense.

00:40:27.595 --> 00:40:28.315
<v Speaker 0>Anyway

00:40:28.955 --> 00:40:31.275
<v Speaker 0>but who knows? Maybe we'll get really lucky,

00:40:31.275 --> 00:40:32.235
<v Speaker 0>and it works out.

00:40:32.795 --> 00:40:35.515
<v Speaker 0>But, JJ, is there any last, like, thoughts,

00:40:35.515 --> 00:40:35.915
<v Speaker 0>comments,

00:40:36.540 --> 00:40:37.740
<v Speaker 0>whatever?

00:40:37.740 --> 00:40:39.980
<v Speaker 0>Because we're already longer than we normally are,

00:40:39.980 --> 00:40:41.980
<v Speaker 0>but this is a really interesting conversation.

00:40:41.980 --> 00:40:42.540
<v Speaker 0>And

00:40:43.100 --> 00:40:45.420
<v Speaker 0>maybe you have any last things, last plugs,

00:40:45.420 --> 00:40:46.220
<v Speaker 0>last whatever.

00:40:46.380 --> 00:40:48.540
<v Speaker 0>Where can we find you online? All that

00:40:48.540 --> 00:40:50.140
<v Speaker 0>fun stuff. Yeah. Well,

00:40:50.975 --> 00:40:52.815
<v Speaker 2>first of all, I'm pretty easy to find

00:40:52.815 --> 00:40:53.455
<v Speaker 2>online.

00:40:54.415 --> 00:40:55.935
<v Speaker 2>JJ Asgar most places.

00:40:56.335 --> 00:40:58.815
<v Speaker 2>If you are interested in Watson x, I

00:40:58.815 --> 00:41:00.895
<v Speaker 2>do need to plug the URL.

00:41:00.895 --> 00:41:05.300
<v Speaker 2>Ibm.biz/dev-watsonx.

00:41:05.300 --> 00:41:08.100
<v Speaker 2>The dash is the actual dash, not dash,

00:41:08.100 --> 00:41:11.700
<v Speaker 2>but the actual dash. Dev - Watson x

00:41:12.100 --> 00:41:13.300
<v Speaker 2>ibm Biz.

00:41:14.660 --> 00:41:16.580
<v Speaker 2>I do want to acknowledge that

00:41:16.980 --> 00:41:17.780
<v Speaker 2>it is hard.

00:41:18.585 --> 00:41:20.425
<v Speaker 2>You think Kubernetes is hard.

00:41:21.065 --> 00:41:23.785
<v Speaker 2>You think our cloud native ecosystem is hard,

00:41:23.785 --> 00:41:24.665
<v Speaker 2>and it is.

00:41:25.625 --> 00:41:27.145
<v Speaker 2>When you start playing in the what the

00:41:27.145 --> 00:41:28.985
<v Speaker 2>the the AI space,

00:41:29.785 --> 00:41:31.785
<v Speaker 2>be prepared to be confused.

00:41:33.490 --> 00:41:34.210
<v Speaker 2>Question

00:41:34.690 --> 00:41:36.290
<v Speaker 2>your ethics and morality,

00:41:36.769 --> 00:41:39.250
<v Speaker 2>and never ask what a developer advocate is

00:41:39.250 --> 00:41:41.170
<v Speaker 2>because I did once.

00:41:41.650 --> 00:41:42.130
<v Speaker 2>And

00:41:42.450 --> 00:41:44.769
<v Speaker 2>it told me I was, in essence, a

00:41:44.769 --> 00:41:46.289
<v Speaker 2>CIA agent killing people.

00:41:46.925 --> 00:41:48.365
<v Speaker 2>Was a little weird. What?

00:41:48.765 --> 00:41:49.805
<v Speaker 0>Okay. Yeah.

00:41:50.205 --> 00:41:52.605
<v Speaker 2>Yeah. A reason I had not created a

00:41:52.605 --> 00:41:54.045
<v Speaker 0>chat GPT account

00:41:54.685 --> 00:41:56.205
<v Speaker 2>Yeah. Or any AI. No. This was this

00:41:56.205 --> 00:41:57.725
<v Speaker 2>was just an open source LLM.

00:41:57.805 --> 00:41:59.085
<v Speaker 2>It was even better. It was just like

00:41:59.085 --> 00:42:01.720
<v Speaker 2>some generic open source LLM that basically describe

00:42:01.720 --> 00:42:04.920
<v Speaker 2>describe developer advocates going sent out by Obama

00:42:05.000 --> 00:42:07.160
<v Speaker 2>to to kill Russian

00:42:07.880 --> 00:42:09.560
<v Speaker 2>entities or something like that. It was really

00:42:09.560 --> 00:42:11.320
<v Speaker 2>weird. It was really weird.

00:42:11.800 --> 00:42:13.800
<v Speaker 0>Even more exciting lives than I realized.

00:42:15.605 --> 00:42:16.805
<v Speaker 0>On that note,

00:42:17.205 --> 00:42:17.925
<v Speaker 0>I

00:42:18.405 --> 00:42:19.925
<v Speaker 0>thanks for coming out, JJ.

00:42:20.085 --> 00:42:21.445
<v Speaker 0>I hope it was fun. Having me. I

00:42:21.445 --> 00:42:22.405
<v Speaker 0>hope you all enjoyed.

00:42:23.845 --> 00:42:26.244
<v Speaker 0>David, last thoughts. I know you're lagging all

00:42:26.244 --> 00:42:26.724
<v Speaker 0>of a sudden.

00:42:27.450 --> 00:42:28.090
<v Speaker 0>Meh.

00:42:29.530 --> 00:42:30.090
<v Speaker 0>Meh.

00:42:30.330 --> 00:42:31.850
<v Speaker 0>I guess that's the answer.

00:42:32.330 --> 00:42:33.210
<v Speaker 0>AI.

00:42:33.690 --> 00:42:34.170
<v Speaker 0>Meh.

00:42:34.490 --> 00:42:35.290
<v Speaker 0>No.

00:42:37.609 --> 00:42:39.450
<v Speaker 0>It's probably good. I'll put on that note.

00:42:39.450 --> 00:42:40.650
<v Speaker 0>Thanks to y'all for listening.

00:42:41.130 --> 00:42:43.175
<v Speaker 0>Thanks for joining us. If you wanna keep

00:42:43.175 --> 00:42:45.895
<v Speaker 1>up with us, consider subscribing to the podcast

00:42:45.895 --> 00:42:48.295
<v Speaker 1>on your favorite podcasting app or even go

00:42:48.295 --> 00:42:50.535
<v Speaker 1>to cloudnativecompass.fm.

00:42:50.695 --> 00:42:52.055
<v Speaker 0>And if you want us to talk with

00:42:52.055 --> 00:42:54.935
<v Speaker 0>someone specific or cover a specific topic,

00:42:55.175 --> 00:42:57.255
<v Speaker 0>reach out to us on any social media

00:42:57.255 --> 00:43:00.140
<v Speaker 0>platform. Until next time when exploring the cloud

00:43:00.140 --> 00:43:03.980
<v Speaker 1>native landscape on 3. On 3. 1,

00:43:04.220 --> 00:43:04.780
<v Speaker 1>2,

00:43:05.020 --> 00:43:05.660
<v Speaker 1>3.

00:43:06.059 --> 00:43:07.900
<v Speaker 1>Don't forget your Don't forget your compass.
